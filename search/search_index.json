{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Erg\u00e4nzungen zum Buch \"Statistik Schritt f\u00fcr Schritt\"","text":"<p>Schneller Navigieren</p> <p>P oder , : Zur vorherigen Seite gehen (Previous)</p> <p>N oder . : Zur n\u00e4chsten Seite gehen (Next)</p> <p>Unittest</p> <p>Bei Unittests, sollte man die folgenden F\u00e4lle beachten:</p> <ul> <li>leere Argumente (z.B. leere Liste, <code>0</code>, usw.)</li> <li>fast leere/minimale Argumente (z.B. Listen mit nur einem Element, <code>1</code>, usw.)</li> <li>Normalf\u00e4lle (z.B. <code>mode([1,3,2,1])</code>, alle Eintr\u00e4ge negativ, positive und negative Eintr\u00e4ge, usw.)</li> <li>Extremf\u00e4lle (z.B. alles gleich)</li> <li>Unsch\u00f6nste F\u00e4lle (z.B. mit maximaler Laufzeit <code>sort([5,4,3,2,1])</code>)</li> <li>Fehlerf\u00e4lle, wenn die Fehler auch im Code geworfen werden k\u00f6nnen und sollten.</li> <li>Performance-Tests (sollten aber nicht zu lange dauern. Unittest sollten weiterhin in wenigen ms fertig sein. Z.B.: <code>[1] * 10**6 + [2] * 10**6</code>)</li> <li>Alle Zweige im Code sollten getestet werden. (nicht zwingend erforderlich ist es, alle Variationen durchzugehen, aber man muss jede Codezeile in jedem Test mindestens einmal durchgef\u00fchrt haben)</li> </ul>"},{"location":"content/B_Grundlagen_statistischer_Untersuchungen/B_1_Gesamtheiten/","title":"Gesamtheiten und ihre Beschreibung durch Merkmale","text":"<p>Die interessante Eigenschaft ist das Merkmal.</p> <p>Die konkreten Werte der interessanten Eigenschaft sind die Merkmalsauspr\u00e4gung.</p> <p>Das Ding mit der interessanten Eigenschaft ist der Merkmalstr\u00e4ger.</p> Merkmalsauspr\u00e4gungen finden <p>Denken Sie sich zu jedem der hier aufgelisteten Merkmale sinnvolle Merkmalsauspr\u00e4gungen aus.</p> <p>a) Wetter</p> <p>b) Getr\u00e4nk</p> <p>c) Fortbewegungsarten bei Google Maps</p> <p>d) Gehalt</p> <p>e) Temperatur in Celsius (wenn m\u00f6glich als Intervall angeben)</p> L\u00f6sung <p>a) Sonnig, Bew\u00f6lkt, Regnerisch, Schneefall, Nebelig, Windig, St\u00fcrmisch, Heiter, Hagel, Schw\u00fcl, Trocken</p> <p>b) Wasser, Saft, Limonade, Tee, Kaffee</p> <p>c) Auto, \u00d6ffentliche Verkehrsmittel, Zu Fu\u00df, Rad/Roller, Flugzeug</p> <p>d) Alle Zahlen gr\u00f6\u00dfer oder gleich \\(0\\) mit h\u00f6chstens zwei Nachkommastellen: 1000\u20ac, 20,52\u20ac, ...</p> <p>e) \\([-273, 15; \\infty)\\)</p>"},{"location":"content/B_Grundlagen_statistischer_Untersuchungen/B_2_Skalenniveaus/","title":"Skalenniveaus","text":""},{"location":"content/B_Grundlagen_statistischer_Untersuchungen/B_2_Skalenniveaus/#zusammenfassung-zu-skalen","title":"Zusammenfassung zu Skalen","text":"<p>Eine Skala ist eine Zusammenstellung aller m\u00f6glichen Auspr\u00e4gungen eines Merkmals und dient als Ma\u00dfstab f\u00fcr die Datenerhebung. Je nach Skalenniveau (Nominal-, Ordinal-, Intervall- oder Verh\u00e4ltnisskala) unterscheiden sich die zul\u00e4ssigen mathematischen Operationen, wobei h\u00f6here Skalenniveaus mehr mathematische M\u00f6glichkeiten bieten.</p> <p>\u00dcbersicht der Skalenniveaus</p> Skalenniveau Zul\u00e4ssige mathematische Operationen Beispiele Nominalskala = / \u2260 - Geschlecht: - Name: - Farbe: Ordinalskala = / \u2260; &lt; / &gt; (z. B. \"besser/schlechter\") - Zufriedenheit: - Schulnoten: - Dienstgrad: Metrisch (kardinal) Intervallskala = / \u2260; &lt; / &gt;; + / \u2212 - Temperatur: {\u2026, -1, 0, 1, \u2026} \u00b0C - Geburtsjahr: Verh\u00e4ltnisskala = / \u2260; &lt; / &gt;; + / \u2212; \u00d7 / \u00f7 - Monatseinkommen: [0; \u221e) EUR - Alter: [0; \u221e) Jahre - Gewicht: [0; \u221e) g - Kinderzahl: <p>Tabelle 1: \u00dcberblick \u00fcber die Skalenniveaus</p> <p>Aufgabe Gruppierer:</p> <p>Erstelle eine Funktion <code>group(values)</code>, dass eine liste in ein dictionary verwandelt und z\u00e4hlt, wie oft jedes Element in der Liste auftaucht. z.b.  <pre><code>group(['mo', 'di', 'mo']) # {'mo': 2, 'di': 1}\n</code></pre></p> Tests <pre><code>from unittest import TestCase, main\nfrom parameterized import parameterized\n\nclass TestGroup(TestCase):\n\n    @parameterized.expand([\n        (['C', 'H', 'O', 'C', 'Cl', 'N', 'C', 'H'], {'C': 3, 'H': 2, 'O': 1, 'Cl': 1, 'N': 1}),\n        (['C', 'H', 'C', 'C', 'H', 'O', 'C', 'H'], {'C': 4, 'H': 3, 'O': 1})\n    ])\n    def test_group(self, input_list, expected):\n        self.assertDictEqual(group(input_list), expected)\n</code></pre> L\u00f6sung mit for-Schleifen <pre><code>def groupby(values):\n    grouped_dict = {}\n    for value in values:\n        if value in grouped_dict:\n            grouped_dict[value] += 1\n        else:\n            grouped_dict[value] = 1\n    return grouped_dict\n</code></pre> L\u00f6sung mit Dictionary Comprehension <pre><code>def group(values):\n    return {value: values.count(value) for value in set(values)}\n</code></pre> L\u00f6ung mit Doctests <pre><code>def group(values:list)-&gt;dict:\n    \"\"\"    \n    &gt;&gt;&gt; group(['mo', 'di', 'mo'])\n    {'mo': 2, 'di': 1}\n\n    &gt;&gt;&gt; group([1, 2, 2, 3, 1, 1])\n    {1: 3, 2: 2, 3: 1}\n\n    &gt;&gt;&gt; group([])\n    {}\n\n    &gt;&gt;&gt; group(['a', 'a', 'a', 'b', 'c', 'c'])\n    {'a': 3, 'b': 1, 'c': 2}\n    \"\"\"\n    return {ele:values.count(ele) for ele in set(values)}\n</code></pre>"},{"location":"content/C_Univariate_Analyse/C_2_Eindimensionale_H%C3%A4ufigkeitsverteilung/","title":"C.2.1 Eindimensionale H\u00e4ufigkeitsverteilung","text":"<p>Absolute empirische H\u00e4ufigkeitsfunktion</p> <p>Es sei \\(X = (x_1, x_2, \\cdots, x_n)\\) ein Mermal. Um die absolute empirische H\u00e4ufigkeitsfunktion zu bestimmen, kann folgende Formel genutzt werden:</p> \\[ f_h(x) = \\begin{cases}  n_k &amp; \\text{f\u00fcr } x = x_k, \\\\  0 &amp; \\text{sonst.} \\end{cases} \\] <p>\\(n_k\\) steht f\u00fcr die Anzahl des Auftretens von \\(x_k\\) in \\(X\\).</p> <p>Beispiel</p> <p>Sei \\(X=(1, 2, 1, 2, 3, 2, 1, 1)\\).  Dann sind z.B. \\(x_1 = 1\\), \\(x_2 = 2\\), \\(x_8 = 1\\) und \\(x_9\\) gibt es nicht!</p> <p>\\(n_1 = 4\\), weil der Wert \\(x_1 = 1\\) in \\(X\\) insgesamt \\(4\\) mal auftaucht.</p> <p>\\(n_2 = 3\\) weil der Wert \\(x_2 = 2\\) in \\(X\\) insgesamt \\(3\\) mal auftaucht.</p> <p>\\(n_3 = 4\\) weil der Wert \\(x_3 = 1\\) in \\(X\\) insgesamt \\(4\\) mal auftaucht.</p> <p>\\(n_4 = 3\\) weil der Wert \\(x_4 = 2\\) in  \\(X\\) insgesamt \\(3\\) mal auftaucht.</p> <p>\\(n_5 = 1\\) weil der Wert \\(x_5 = 3\\) in \\(X\\) insgesamt \\(1\\) mal auftaucht.</p> <p>...</p> <p>\\(n_8 = 4\\) weil der Wert \\(x_8 = 1\\) in \\(X\\) insgesamt \\(4\\) mal auftaucht.</p> <p>Nutzen wir nun die absolute empirische H\u00e4ufigkeitsfunktion:</p> <p>Es ist \\(f_h(2) = 3\\). Denn \\(x=2\\) und damit ist z.B. \\(x = x_2\\). Also wird \\(n_2 = 3\\) zur\u00fcckgegeben.</p> <p>Es ist \\(f_h(7) = 0\\). Denn \\(x=7\\) und es gibt kein \\(k\\) mit \\(x_k=x=7\\). Also wird der \"sonst\"-Fall ausgegeben.</p> <p>Implementierung in Python:</p> <p>In Python lassen sich die absoluten H\u00e4ufigkeiten \\(n_k\\) mit der Methode <code>count</code> ermitteln:</p> <pre><code>X = (1, 2, 1, 2, 3, 2, 1, 1) \n\ndef f_h(x, data):\n    return data.count(x)\n\nprint(f_h(1, data=X)) # 4 \n</code></pre> <p>Relative empirische H\u00e4ufigkeitsfunktion</p> <p>Es sei \\(X = (x_1, x_2, \\cdots, x_n)\\) ein Mermal. Um die relative empirische H\u00e4ufigkeitsfunktion zu bestimmen, kann folgende Formel genutzt werden:</p> \\[ f_h^*(x) = \\begin{cases}  \\frac{n_k}{n} = n_k^* &amp; \\text{f\u00fcr } x = x_k, \\\\  0 &amp; \\text{sonst.} \\end{cases} \\] <p>\\(n_k\\) steht f\u00fcr die Anzahl des Auftretens von \\(x_k\\) in \\(X\\).</p> <p>Implementierung in Python:</p> <pre><code>X = (1, 2, 1, 2, 3, 2, 1, 1) \n\ndef f_h_rel(x, data):\n    return data.count(x) / len(data)\n\nprint(f_h_rel(1, data=X)) # 0.5 \nprint(f_h_rel(7, data=X)) # 0.0\n</code></pre> Relative H\u00e4ufigkeiten berechnen <p>Erstelle eine funktion <code>relative_counts</code>, die eine Liste mit absoluten H\u00e4ufigkeiten erh\u00e4lt und aus dieser die Liste mit relativen H\u00e4ufigkeiten berechnet.  <pre><code>relative_counts([4, 5, 15, 17, 10]) # [0.08, 0.1, 0.29, 0.33, 0.2]\n</code></pre></p> L\u00f6sung <pre><code>def relative_counts(absolute_values):\n    if not absolute_values:\n        raise ValueError(\"Bitte keine leere Liste \u00fcbergeben\")\n    n = sum(absolute_values)\n    return [number / n for number in absolute_values]\n\n\nclass Test_Relative_Counts(unittest.TestCase):\n    @parameterized.expand([\n        ([4, 5, 15, 17, 10], [0.08, 0.1, 0.29, 0.33, 0.2]),\n        ([1],[1]),\n        ([10, 20, 35], [0.15, 0.31, 0.54]),\n        ([22, 11, 37, 70, 2, 7], [0.15, 0.07, 0.25, 0.47, 0.01, 0.05]),\n        ([1, 2, 3, 4], [0.1, 0.2, 0.3, 0.4])\n    ])  \n\n\n    def test_parametrized(self,values,expected):\n        result = relative_counts(values)\n        rounded_result = [round(value, 2) for value in result]\n        self.assertEqual(rounded_result, expected)\n\n    def test_empty_list(self):\n        with self.assertRaises(ValueError):\n            relative_counts([])\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n</code></pre> Relative und absolute H\u00e4ufigkeiten in pandas berechnen <p>Erstelle in Pandas eine neue Methode <code>Series.get_counts(self, bins, **cut_kwargs)</code>. Diese soll eine Series mit Hilfe von <code>cut</code> klassieren und dann die absolute und relative H\u00e4ufigkeit der Klassen angeben.</p> L\u00f6sung <pre><code>def get_counts(self, bins, **cut_kwargs):\n    ser_bins = pd.cut(self, bins, **cut_kwargs)\n    return pd.DataFrame({\n        \"relativen H\u00e4ufigkeiten\": ser_bins.value_counts(normalize=True),\n        \"absoluten H\u00e4ufigkeiten\": ser_bins.value_counts(normalize=False),\n    })\n\npd.Series.get_counts = get_counts\n\nif __name__ == '__main__':\n    daten = pd.Series([1, 2, 2, 3, 4, 4, 4, 5, 6, 6, 6, 6, 7, 8, 9, 9, 9, 9, 9, 10])\n    print(daten.get_counts(bins=[0, 3, 6, 9, 12], right=True))\n</code></pre>"},{"location":"content/C_Univariate_Analyse/C_3_0_Ma%C3%9Fzahlen/","title":"C.3.0 Mittelwerte","text":"Modus, Median und Arithmetisches Mittel in Python <p>Implementierung Sie die Funktionen <code>mode</code>, <code>median</code> und <code>mean</code>, um den Modus, den Median und das Arithmetische Mittel einer Liste zu bestimmen.</p> L\u00f6sung <p>L\u00f6sung von Marina:</p> <pre><code>    import numpy as np\n    import unittest\n    from parameterized import parameterized\n\n\n    def median(liste):\n        if liste:\n            liste = sorted(liste)\n            mid = int(len(liste) // 2)\n\n            if len(liste) % 2 == 1:\n                return liste[mid]\n\n            return (liste[mid - 1] + liste[mid]) / 2\n        return np.nan\n\n\n    def mode(liste):\n        if not liste:\n            return np.nan\n\n        return _mode_impl(liste)\n\n\n    def _mode_impl(liste):\n        frequency = {value: liste.count(value) for value in set(liste)}\n        return max(frequency, key=frequency.get)\n\n\n    def mean(liste):\n        return sum(liste) / len(liste)\n\n\n    class TestStatistics(unittest.TestCase):\n        @parameterized.expand([\n            ([1, 2, 3, 4, 5], 3),\n            ([10, 20], 15),\n            ([-1, 0, 1], 0),\n        ])\n        def test_mean(self, data, expected):\n            self.assertAlmostEqual(mean(data), expected)\n\n        def test_mean_empty(self):\n            self.assertTrue(np.isnan(mean([])))\n\n        @parameterized.expand([\n            ([1, 2, 3, 4, 5], 3),\n            ([1, 2, 3, 4], 2.5),\n            ([7], 7),\n        ])\n        def test_median(self, data, expected):\n            self.assertEqual(median(data), expected)\n\n        def test_median_empty(self):\n            self.assertTrue(np.isnan(median([])))\n\n        @parameterized.expand([\n            ([1, 2, 2, 3, 4], 2),\n            ([4, 4, 4, 1, 2], 4),\n            ([1, 2, 3, 4, 5, 5, 6, 7], 5),\n        ])\n        def test_mode(self, data, expected):\n            self.assertEqual(mode(data), expected)\n\n\n    if __name__ == '__main__':\n        unittest.main()\n</code></pre> Varianz und Standardabweichung in Python <p>Implementierung Sie die Funktionen <code>var</code>, <code>std</code> um die Varianz und Standardabweichung einer Liste zu bestimmen.</p> L\u00f6sung <p>L\u00f6sung von Andreas: <pre><code>    import unittest\n    from parameterized import parameterized\n\n    def variance(lst: list) -&gt; int | float:\n        if not lst:\n            raise ValueError('give not a empty list u can')\n\n        mean = (sum(lst) / len(lst))\n        return sum((num - mean) ** 2 for num in lst) / len(lst)\n\n\n    def standard_deviation(lst: list) -&gt; int | float:\n        return variance(lst) ** 0.5\n\n\n    class TestUnit(unittest.TestCase):\n\n        @parameterized.expand([\n            ([23, 16, 18, 17, 22, 28, 26, 22, 20, 8], 29.0),\n            ([27, 22, 21, 26, 27, 35, 31, 24, 22, 15], 28.0),\n            ([27], 0.0),\n            ([27, 27, 27], 0.0),\n        ])\n        def test_variance_values(self, get, expected):\n            self.assertAlmostEqual(variance(get), expected, places=2)\n\n        @parameterized.expand([\n            ([23, 16, 18, 17, 22, 28, 26, 22, 20, 8], 5.39),\n            ([27, 22, 21, 26, 27, 35, 31, 24, 22, 15], 5.29),\n            ([27], 0.0),\n            ([27, 27, 27], 0.0),\n        ])\n        def test_standard_devation(self, get, expected):\n            self.assertAlmostEqual(standard_deviation(get), expected, places=2)\n\n\n    if __name__ == '__main__':\n        unittest.main(argv=[''], verbosity=2, exit=False)\n</code></pre></p> <p>Arithmetisches Mittel</p> <p>Sei \\(X = (x_1, \\cdots , x_n)\\) ein Datensatz. Dann ist das  arithmetische Mittel definiert als:</p> \\[ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i \\] <p>Varianz</p> <p>Sei \\(X = (x_1, \\cdots , x_n)\\) ein Datensatz. Dann ist das  Varianz definiert als:</p> \\[ \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2 \\] <p>Die Standardabweichung \\(\\sigma\\) ist definiert als die Wurzel der Varianz:</p> \\[ \\sigma = \\sqrt{\\sigma^2} \\] <p>\ud83d\udc41\u200d\ud83d\udde8Visualisierung der Varianz und Standardabweichung</p> <p></p> <p>Die Varianz ist das arithmetische Mittel, des quadratischen Abstandes von den Datenpunkten zum arithmetischen Mittel (bzw. des quadrierten Fehlers).</p> \\[ \\sigma^2=\\overline{(X - \\overline{X})^2} \\] <p>Beispiel</p> <p>Es sei \\(X = (1,2,3,1.5)\\). Dann ist \\(\\bar{X} = 1.875\\)</p> <p>Daraus ergibt sich \\((X - \\bar{X})^2 = (0.765625, 0.015625, 1.265625, 0.140625)\\)</p> <p>Berechnen wir davon noch mal das Arithmetische Mittel, erhalten wir die Varianz von \\(X\\):</p> \\[ \\sigma^2_X=\\overline{(X - \\overline{X})^2} = 0.546875 \\] <p>Beispiel</p> <p>Mittlere Note: \\(\\bar{x} = 3\\).</p> <p>Standardabweichung: \\(\\sigma = 0.5\\).</p> <p>Wenn die Noten normalverteilt sind, so gilt: \\(\\approx 68\\%\\) der Sch\u00fcler liegen im Bereich \\(2.5\\) bis \\(3.5\\). \\(\\approx 95\\%\\) der Sch\u00fcler liegen im Bereich \\(2\\) bis \\(4\\).</p> <p>\ud83d\udc41\u200d\ud83d\udde8Visualisrung</p>"},{"location":"content/C_Univariate_Analyse/C_3_3_Boxplots/","title":"C.3.3 Boxplots und Quantile","text":""},{"location":"content/C_Univariate_Analyse/C_3_3_Boxplots/#aufgabe-1","title":"Aufgabe 1","text":"<p>L\u00d6SUNG</p> <p></p>"},{"location":"content/C_Univariate_Analyse/C_3_3_Boxplots/#aufgabe-2","title":"Aufgabe 2","text":"<p>L\u00d6SUNG</p> <p></p>"},{"location":"content/C_Univariate_Analyse/C_3_3_Boxplots/#aufgabe-3","title":"Aufgabe 3","text":"<p>L\u00d6SUNG</p> <p></p>"},{"location":"content/C_Univariate_Analyse/C_3_5_Gini_Koeffizient/","title":"C.3.5 Lorenzkurve und Gini-Koeffizient","text":"1) Ums\u00e4tze in 4 M\u00e4rkten <p>Gegeben sind in 4 M\u00e4rkten folgende Verteilungen des Gesamtumsatzes auf die verschiedenen konkurrierenden Firmen:</p> Firma Markt 1 Markt 2 Markt 3 Markt 4 Firma A 40 15 25 20 Firma B 10 30 20 20 Firma C 10 10 15 20 Firma D 0 10 10 20 Firma E 0 0 5 20 <p>Aufgabe:  (a) Erstelle die Lorenzkurve f\u00fcr jeden Markt.</p> <p>(b) Berechne den normierten Gini-Koeffizienten G* f\u00fcr jeden Markt.</p> L\u00f6sung <p>a)</p> <p></p> <p>b) </p> Markt 1 Markt 2 Markt 3 Markt 4 G 0.6 0.4 \\( \\frac{4}{15} = 0.2\\overline{6} \\) 0 G* 0.75 0.5 \\( \\frac{1}{3} = 0.\\overline{3} \\) 0 Gini mit Datensatz <p>Laden Sie diesen Datensatz \u00fcber die Einkommen von Erwachsenen herunter. Hier finden Sie den  orginalen Download und die  Erkl\u00e4rungen zum Datensatz</p> <p>Berechnen Sie den Gini-Koeffizienten (und den normierten Gini-Koeffizienten) und zeichnen Sie die Lorenzkurve zu diesem Datensatz.</p> <p>\u00dcberlegen Sie selbst, welche Merkmale da interessant sind zu untersuchen.</p> <p>Berechnen sie dann dan Gini-Koeffizienten, den normierten Gini-Koeffizienten und zeichnen Sie die Lorenzkurve f\u00fcr Untergruppen (z.B. <code>Male</code> und <code>Female</code>) Wo lassen sich Unterschiede erkennen und wo nicht?</p>"},{"location":"content/Clustering/0_einf%C3%BChrung/","title":"Einf\u00fchrung Clustering","text":"<p>Das Ziel von Clusteringalgorithmen ist es ein neues Merkmal zu erzeugen, das anzeigt, welche Daten zusammengeh\u00f6ren. Es gibt hier eine ganze Reihe verschiedener Algorithmen, die oben Beispielhaft dargestellt werden  (Quelle).</p> <p>Clusteringalgorithmen werden (ungl\u00fccklicherweise) auch unsupervised learning genannt.</p>"},{"location":"content/Clustering/1_KMeans/","title":"KMeans","text":"<p>Die Idee beim Clusteringalgorithmus KMeans ist es Mittelpunkte von \\(k\\) Clustern \u00fcber einen iterativen Prozess zu finden. \\(k\\) ist ein Hyperparameter (wird also im vorhinein vom Nutzer bestimmt).</p> <p>Algorithmus</p> <p>Gegeben Sei ein Datensatz \\(X := \\{x_1, \\cdots, x_n \\}\\) ein Datensatz mit \\(n\\in \\mathbb{N}\\) Eintr\u00e4gen. Wir suchen \\(k\\) Cluster. Dazu werden Clustermittelpunkte (Centroide) \\(C := \\{\\mu_1 , \\cdots , \\mu_k \\}\\) gesucht. Ein Punkt \\(x\\in X\\) geh\u00f6rt zu dem Cluster \\(\\mu \\in C\\), zu dem er den geringsten Abstand hat.</p> <p>Die Qualit\u00e4t eines Clusterings wird mit der Inertia gemessen:</p> \\[ \\sum_{x\\in X}^n \\underset{\\mu \\in C}{min}\\text{ }d^{EDQ}(x, \\mu) \\] <p>Ein Durchlauf des Algorithmus l\u00e4uft wie folgt ab:</p> <ol> <li>Lege \\(k\\) zuf\u00e4llige Centroide fest.</li> <li>Bestimme f\u00fcr jeden Eintrag \\(x\\in X\\) zu welchem Cluster er geh\u00f6rt (d.h. bestimme den Centroid mit dem geringsten Abstand zu \\(x\\))</li> <li>Berechne f\u00fcr jedes Cluster den Mittelwert der zugeh\u00f6rigen Eintr\u00e4ge. Diese Mittelwerte bilden die neuen Clustermittelpunkte.</li> <li>F\u00fchre Schritte 2 und 3 so lange erneut aus, bis keine Eintr\u00e4ge mehr ihre Zugeh\u00f6rigkeit zu einem Cluster \u00e4ndern oder bis die Reduzierung der Inertia sich nur noch minimal ver\u00e4ndert.</li> <li>Merke dir das Ergebnis und die Inertia und beginne nun erneut bei 1, um ggf. bessere Ergebnisse zu erzielen.</li> <li>Nach ausreichend vielen Wiederholungen von 1 bis 5 vergleiche die Inertia der Ergebnisse und w\u00e4hle das Clustering mit der niedrigsten Inertia.</li> </ol> <p></p> <p>Bildsegmentierung mit KMeans</p> <p>\ud83d\udcd9Kagle Notebook zur Bildsegmentierung mit KMeans</p> <p></p> Kmeans implementieren <p>Implementieren Sie das KMeans Model, sodass es sich in die sklearn Modelle einbettet.</p> L\u00f6sung <pre><code>### Henrik\n\nfrom sklearn.base import BaseEstimator, ClusterMixin\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\nclass MyKMeans(BaseEstimator, ClusterMixin):\n    def __init__(self, n_clusters: int, max_iter:int=300, tol:float=0.0001, init='random', random_state:int=None, max_epochs:int=20):\n        super().__init__()\n        self.n_cluster:int = n_clusters\n        self.max_iter:int = max_iter\n        self.tol:float = tol\n        self.init = init\n        self.random_state:int = random_state\n        self.max_epochs:int = max_epochs\n        self.labels_:np.ndarray = None\n        self.cluster_centers_:np.ndarray = None\n        self.inertia_:float = None\n\n\n    def _create_starting_centers(self, X:np.ndarray):\n        if isinstance(self.init, np.ndarray):\n            if self.init.shape == (self.n_cluster, X.shape[1]):\n                return self.init\n            else:\n                raise ValueError(\n                    f\"Error: self.init hat die falsche Form {self.init.shape}, erwartet wurde {(self.n_cluster, X.shape[1])}.\"\n                )\n        elif isinstance(self.init, str):\n            if self.init == 'random':\n                np.random.seed(self.random_state)\n                min_values = np.min(X, axis=0)\n                max_values = np.max(X, axis=0)\n                return np.random.uniform(min_values, max_values, size=(self.n_cluster, X.shape[1])) \n            else:\n                raise ValueError(\n                    f\"Error: self.init kann nur ein string 'random' oder ein ndarray sein\"\n                )\n        else:\n            raise TypeError(\n                f\"Error: self.init muss entweder 'random' oder ein NumPy-Array sein, aber ist {type(self.init)}.\"\n            )\n\n\n    def _calc_dist(self, X, centers):\n        return np.sum((X[:, np.newaxis, :] - centers[np.newaxis, :, :]) ** 2, axis=2)\n\n    def _assign_labels(self, dist): \n        return np.argmin(dist, axis=1)\n\n    def _calc_inertia(self, dist, labels):\n        return np.sum(dist[np.arange(len(labels)), labels])\n\n    def _new_cluster_centers(self, X, labels:np.ndarray, old_centers: np.ndarray):\n        new_centers = []\n        for i in range(self.n_cluster):\n            if np.any(labels == i): \n                new_centers.append(X[labels == i].mean(axis=0))\n            else:\n                new_centers.append(old_centers[i])\n        return np.array(new_centers)\n\n    def fit(self, X, show=False):\n        epoch_labels = []\n        epoch_cluster_centers = []\n        epoch_inertia = []\n        for i in range(self.max_epochs):\n            centers = self._create_starting_centers(X)\n            old_inertia = float(\"inf\")\n            labels = None\n            for n in range(self.max_iter):\n                if labels is not None:\n                    centers = self._new_cluster_centers(X, labels, centers)\n                dist = self._calc_dist(X, centers)\n                labels = self._assign_labels(dist)\n                new_inertia = np.sum(self._calc_inertia(dist, labels))\n                if np.isclose(new_inertia, old_inertia):\n                    epoch_labels.append(labels)\n                    epoch_cluster_centers.append(centers)\n                    epoch_inertia.append(new_inertia)\n                    break\n                old_inertia = new_inertia\n                if show:\n                    self.show_labels(X, labels, centers)\n            else:\n                epoch_labels.append(labels)\n                epoch_cluster_centers.append(centers)\n                epoch_inertia.append(new_inertia)\n        best_index = np.argmin(epoch_inertia)\n        self.labels_ = epoch_labels[best_index]\n        self.cluster_centers_ = epoch_cluster_centers[best_index]\n        self.inertia_ = epoch_inertia[best_index]\n        return self.labels_, self.cluster_centers_, self.inertia_\n\n    def predict(self, X):\n        dist = self._calc_dist(X, self.cluster_centers_)\n        return self._assign_labels(dist)\n\n    def show_labels(self, X, labels, centers):\n        plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolors='k', s=100)\n        plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='x', s=200, label=\"Cluster-Zentren\")\n        plt.show()\n\ndef create_cluster_array(random_seed:int=42) -&gt; np.array:\n    np.random.seed(random_seed)\n    cluster_1 = np.random.randn(50, 2) * 0.5 + [2, 2]\n    cluster_2 = np.random.randn(50, 2) * 2 + [4, 4]\n    cluster_3 = np.random.randn(50, 2) * 0.5 + [2, 6]\n    cluster_4 = np.random.randn(50, 2) * 0.5 + [6, 2]\n    return np.vstack((cluster_1, cluster_2, cluster_3, cluster_4))\n\ndef show_labels(X, labels, centers):\n    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolors='k', s=100)\n    plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='x', s=200, label=\"Cluster-Zentren\")\n    plt.show()\n\nif __name__ == \"__main__\":\n    data = create_cluster_array()\n    model = MyKMeans(n_clusters=4, random_state=None, init=\"random\", max_epochs=50)\n    labels, center, inertia = model.fit(data, show=False)\n    print(model.predict(X=np.array([[0,0]])), center)\n    show_labels(data, labels, center)\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nfrom sklearn.base import BaseEstimator, ClusterMixin\nfrom PIL import Image\n\ndef euclid_squared(x, c):\n    return np.sum((x - c) ** 2)\n\nclass MyKMean(BaseEstimator, ClusterMixin):\n    def __init__(self, n_cluster, max_iter=300, tol=0.0001, init='random', random_state=None, dist=euclid_squared):\n        self.n_cluster = n_cluster\n        self.random_state = random_state\n        self.max_iter = max_iter\n        self.tol = tol\n        self.dist = dist\n        self.init = init\n\n    def fit(self, X, y=None, visualize=False):\n        # Centroids initialisieren\n        if not isinstance(self.init, np.ndarray):\n            X = np.array(X)\n            min_x = np.min(X, axis=0)\n            max_x = np.max(X, axis=0)\n            random.seed(self.random_state)\n            C_start = {tuple(min_x + random.random() * (max_x - min_x)): [] for _ in range(self.n_cluster)}\n        else:\n            C_start = {i: [] for i in self.init}\n\n        # Zuordnen der Daten aus X zu dem jeweils n\u00e4chstem Centroid\n        for x in X:\n            closest_cluster = min(C_start.keys(), key=lambda c: self.dist(x, np.array(c)))\n            C_start[closest_cluster].append(x)\n\n        # Vor dem Schelifenbeginn merker f\u00fcr vorhrerige Toleranz und aktuelle toleranz und Durchlaufz\u00e4hler initialisieren\n        prev_tol = 0\n        act_tol = None\n        i = 0\n\n        if visualize:\n            fig, ax = plt.subplots()\n        # Hauptschleife, die solange durchlaufen wird bis Terminierungsbedingungen erf\u00fcllt sind\n        while (act_tol is None or act_tol &gt; self.tol) and i &lt; self.max_iter:\n\n            i += 1\n            # Neue optimierte Centroids berechnen\n            new_C = {}\n            for c in C_start.keys():\n                if C_start[c]:\n                    new_C[tuple(np.mean(C_start[c], axis=0))] = []\n                else:\n                    new_C[c] = []\n            # Punkte dem neuen Centroid zuordnen\n            for x in X:\n                closest_cluster = min(new_C.keys(), key=lambda c: self.dist(x, np.array(c)))\n                new_C[closest_cluster].append(x)\n\n            # Inertia berechnen \n            inertia = sum(self.dist(x, np.array(c)) for c in new_C.keys() for x in new_C[c])\n            # Inertia speichern und aktuelle Toleranz ermittlen\n            if act_tol is not None:\n                prev_tol = act_tol\n            act_tol = abs(inertia - prev_tol)\n\n            C_start = new_C\n\n            if visualize:\n                ax.clear()\n                colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n                for idx, (cluster, points) in enumerate(C_start.items()):\n                    points = np.array(points)\n                    ax.scatter(points[:, 0], points[:, 1], c=colors[idx % len(colors)], label=f'Cluster {idx}')\n                centers = np.array(list(C_start.keys()))\n                ax.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X', label='Centers')\n                ax.legend()\n                plt.draw()\n                plt.pause(0.5)\n\n        if visualize:\n            plt.show()\n\n        self.cluster_centers_ = list(C_start.keys())\n        self.labels_ = [min(self.cluster_centers_, key=lambda c: self.dist(x, np.array(c))) for x in X]\n\n    def predict(self, X):\n        return [min(self.cluster_centers_, key=lambda c: self.dist(x, np.array(c))) for x in X]\n\ndef image_seg(image_path, n_clusters=3):\n    image_org = Image.open(image_path)\n    image_np = np.array(image_org)\n\n    # Reshape the image to a 2D array of pixels\n    pixels = image_np.reshape(-1, 3)\n\n    # Apply KMeans clustering to segment the image\n    kmeans = MyKMean(n_cluster=n_clusters, random_state=42)\n    kmeans.fit(pixels)\n\n    # Replace each pixel value with its corresponding cluster center value\n    segmented_img_np = np.array([kmeans.cluster_centers_[label] for label in kmeans.labels_])\n    segmented_img_np = segmented_img_np.reshape(image_np.shape).astype(np.uint8)\n\n    # Convert the numpy array back to an image\n    segmented_img = Image.fromarray(segmented_img_np)\n\n    return segmented_img\n\n# Beispiel-Daten\nX = np.random.rand(100, 2)\nkmeans = MyKMean(n_cluster=3, random_state=42)\nkmeans.fit(X, visualize=False)\n\nX = np.random.rand(100, 3)\nkmean_2 = MyKMean(n_cluster=3, random_state=42)\nkmean_2.fit(X)\n# Beispiel-Daten\nX = np.random.rand(100, 2)\nkmeans = MyKMean(n_cluster=3, random_state=42)\nkmeans.fit(X, visualize=True)\n\n-----------Johannes/Andi--------\n\n\ndef euclidean_distances(X, centroids):\n    return np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n\ndef axis_aligned_distances(X, centroids):\n    return np.abs(X[:, np.newaxis] - centroids).sum(axis=2)  # Manhattan-Distanz\n\ndef kmeans_with_plots(X, k, max_iters=10, tol=1e-4, distance_func=euclidean_distances, random_state = 42):\n    np.random.seed(random_state)  # F\u00fcr Reproduzierbarkeit\n    n_samples, n_features = X.shape\n\n    # Zuf\u00e4llige Auswahl der Start-Centroids\n    centroids = X[np.random.choice(n_samples, k, replace=False)]\n\n    # Subplots vorbereiten\n    fig, axes = plt.subplots(2, 5, figsize=(20, 8))  # 2 Zeilen, 5 Spalten\n    axes = axes.flatten()  # 2D -&gt; 1D Array f\u00fcr leichtere Iteration\n\n    for i in range(max_iters):\n        # Schritt 1: Cluster-Zuordnung mit externer Distanzfunktion\n        distances = distance_func(X, centroids)\n        labels = np.argmin(distances, axis=1)\n\n        # Schritt 2: Neue Centroids berechnen\n        new_centroids = np.array([X[labels == j].mean(axis=0) if np.any(labels == j) else centroids[j] for j in range(k)])\n\n        # Plot der aktuellen Iteration\n        ax = axes[i]  # Aktuelles Subplot\n        ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.6)\n        ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, label='Centroids')\n\n        # Linien von Punkten zu den Centroids zeichnen\n        for j in range(n_samples):\n            ax.plot([X[j, 0], centroids[labels[j], 0]], [X[j, 1], centroids[labels[j], 1]], 'k-', alpha=0.2)\n\n        ax.set_title(f\"Iteration {i+1}\")\n\n        # Konvergenzpr\u00fcfung\n        if np.linalg.norm(new_centroids - centroids) &lt; tol:\n            break\n\n        centroids = new_centroids\n\n    plt.tight_layout()\n    plt.show()\n\n\n\nX, _ = make_blobs(n_samples=300, centers=8, cluster_std=2, random_state=42)  # Erh\u00f6hte Streuung\n\n# K-Means mit Visualisierung aufrufen\nkmeans_with_plots(X, k=3, distance_func=axis_aligned_distances)\n\n\n\n\nif __name__ == '__main__':\n    unittest.main(argv=[''], exit=False)\n</code></pre>"},{"location":"content/Clustering/2_dbscan/","title":"DBScan","text":"<p>DBScan ist ein Clusteringalgorithmus, der selbstst\u00e4ndig eine passende Anzahl von Clustern sucht. Elemente, die er nicht einem Cluster zuordnen kann, werden als \"Ausrei\u00dfer\" markiert.</p> <p>Algorithmus</p> <p>Hyperparameter:</p> <ul> <li>\ud835\udc5b\u2208\u2115 : die Anzahl der Nachbarn, die ein Punkt braucht, um als Kernpunkt zu gelten.</li> <li>\ud835\udf00\u2208(0,\u221e): der Abstand, in dem ein Punkt als Nachbar eines anderen Punktes gilt.</li> <li>Eine Abstandsfunktion \ud835\udc51.</li> </ul> <p>Algorithmus:</p> <ol> <li>Berechnen Sie f\u00fcr jeden Datenpunkt die Anzahl der Nachbarn, die n\u00e4her als ein bestimmter Grenzwert \ud835\udf00\u2208(0,\u221e) liegen.</li> <li>Markiere alle Punkte mit mindestens \ud835\udc5b\u2208\u2115 Neiboren als Kernpunkte.</li> <li>W\u00e4hle einen beliebigen Kernpunkt \ud835\udc5d, der nicht Teil eines Clusters ist, und f\u00fcge ihn zu einem neuen Cluster \ud835\udc50 hinzu.</li> <li>F\u00fcge alle Kernpunkte, die einen maximalen Abstand von \ud835\udf00 zu \ud835\udc5d haben, dem Cluster \ud835\udc50 hinzu.</li> <li>Wiederholen Sie Schritt 4 f\u00fcr alle neu hinzugef\u00fcgten Kernpunkte des Clusters.</li> <li>Wenn es in diesem Cluster keine weiteren Kernpunkte mehr hinzuzuf\u00fcgen gibt, wiederholen Sie Schritt 3. Wenn alle Kernpunkte zu Clustern hinzugef\u00fcgt wurden, fahren Sie mit Schritt 7 fort.</li> <li>F\u00fcr jeden Nicht-Kernpunkt \ud835\udc5d^\u2032 wird der n\u00e4chstgelegene Kernpunkt-Nachbar \ud835\udc5d mit einem maximalen Abstand von \ud835\udf00 gesucht und \ud835\udc5d\u2032 zum Cluster von \ud835\udc5d hinzugef\u00fcgt.</li> <li>Alle verbleibenden Nicht-Kernpunkte werden dem Ausrei\u00dfer-Cluster hinzugef\u00fcgt.</li> </ol> <p>Beispiel</p> <p> </p> <p>Beispiel</p> <p>\ud83d\udcd9Clustering der Benzin- und Dieselpreise in Indien</p> <p>Download Datensatz</p> <p></p> <p>Beispiel</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\n\n# 1. K\u00fcnstliche Daten generieren\nX, _ = make_blobs(n_samples=300, centers=[[4, 4], [-2, -1], [1, 1], [10, 4]],\n                  cluster_std=0.9, random_state=0)\n\n# 2. DBSCAN-Algorithmus anwenden\n# Parameter:\n# - eps: maximaler Abstand f\u00fcr Nachbarschaft\n# - min_samples: Mindestanzahl von Punkten in der Nachbarschaft f\u00fcr einen Kernpunkt\ndbscan = DBSCAN(eps=0.8, min_samples=10)\ndbscan.fit(X)\nlabels = dbscan.labels_\n\n# 3. Anzahl der Cluster ermitteln (Label -1 steht f\u00fcr Rauschen)\nn_clusters = len(set(labels)) - (1 if -1 in labels else 0)\nprint(\"Anzahl Cluster:\", n_clusters)\n\n# 4. Ergebnisse visualisieren\nplt.figure(figsize=(8, 6))\nunique_labels = set(labels)\ncolors = [plt.cm.Spectral(each)\n          for each in np.linspace(0, 1, len(unique_labels))]\n\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Rauschpunkte (Outlier) in schwarz darstellen\n        col = [0, 0, 0, 1]\n\n    class_member_mask = (labels == k)\n    xy = X[class_member_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=6)\n\nplt.title('DBSCAN Clustering: {} Cluster gefunden'.format(n_clusters))\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.show()\n</code></pre> <p></p> DBScan anwenden <p>Nutzen Sie DBScan und KMeans, um die Cluster im Indiendatensatz vom obigen Beispiel zu erkennen.</p> <p>Welcher Algorithmus eignet sich hier besser? Inwiefern kann man den Algorithmus wiederholt anwenden?</p> DBScan implementieren <p>Implementieren Sie das DBScan Model.</p> <p>Implementieren Sie dazu eine Klasse <code>MyDBScan</code>, die von <code>BaseEstimator</code> und <code>ClusterMixin</code> ableitet.</p> <p>Die Methode <code>__init__</code> enth\u00e4lt die Parameter:</p> <ul> <li><code>eps</code>: Abstandsschwelle, ab der zwei Punkte als Nachbarn gelten.</li> <li><code>min_samples</code>: Mindestanzahl an Punkten in der eps-Umgebung, um einen Kernpunkt zu definieren.</li> </ul> <p>Implementiere die Methoden <code>predict</code> und <code>fit</code>. Orientiere dich dabei an  sklearns DBScan.</p> L\u00f6sung <pre><code>from sklearn.base import BaseEstimator, ClusterMixin\nfrom scipy.spatial import distance_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef merge_intersecting_sets(dict_of_sets):\n    \"\"\"\n    Diese Funktion vereinigt Mengen aus einem Dictionary, falls sich zwei Mengen \u00fcberschneiden.\n    Dabei wird iterativ gepr\u00fcft, bis keine weiteren Vereinigungen mehr m\u00f6glich sind.\n    \"\"\"\n    # Extrahiere alle Mengen als Liste\n    sets_list = list(dict_of_sets.values())\n    changed = True\n    while changed:\n        changed = False\n        new_sets = []\n        # Nimm solange Mengen aus der Liste, bis diese leer ist\n        while sets_list:\n            current_set = sets_list.pop(0)\n            i = 0\n            # Vergleiche current_set mit jeder weiteren Menge\n            while i &lt; len(sets_list):\n                # Falls ein Schnitt vorhanden ist, vereinige die Mengen\n                if current_set.intersection(sets_list[i]):\n                    current_set |= sets_list.pop(i)\n                    changed = True  # Es gab eine \u00c4nderung, erneute Pr\u00fcfung n\u00f6tig\n                else:\n                    i += 1\n            new_sets.append(current_set)\n        sets_list = new_sets\n    return sets_list\n\n\nclass MyDBScan(BaseEstimator, ClusterMixin):\n    \"\"\"\n    Eigene Implementierung des DBSCAN-Algorithmus.\n\n    Parameter:\n      - eps: Abstandsschwelle, ab der zwei Punkte als Nachbarn gelten.\n      - min_samples: Mindestanzahl an Punkten in der eps-Umgebung, um einen Kernpunkt zu definieren.\n    \"\"\"\n\n    def __init__(self, eps=0.5, min_samples=5):\n        self.eps = eps\n        self.min_samples = min_samples\n\n    def fit(self, X, y=None):\n        \"\"\"\n        F\u00fchrt das Clustering auf den Daten X durch.\n        \"\"\"\n        n_samples = X.shape[0]\n        # Berechne die paarweise Distanzmatrix\n        dist_matrix = distance_matrix(X, X)\n\n        # Bestimme f\u00fcr jeden Punkt p die Menge der Nachbarn (Indices), die innerhalb von eps liegen\n        neighbors = {\n            p: {i for i in range(n_samples) if self.is_neighbor(i, p, dist_matrix)}\n            for p in range(n_samples)\n        }\n\n        # Z\u00e4hle die Anzahl der Nachbarn f\u00fcr jeden Punkt\n        count_neighbors = {p: len(neighbors[p]) for p in neighbors.keys()}\n\n        # Bestimme die Kernpunkte: Punkte mit mindestens min_samples Nachbarn\n        core_point_indices = {p for p in neighbors.keys() if count_neighbors[p] &gt;= self.min_samples}\n\n        # F\u00fcr jeden Kernpunkt: Behalte nur die Nachbarn, die ebenfalls Kernpunkte sind\n        cp_neighbors_cp = {\n            p: core_point_indices.intersection(neighbors[p])\n            for p in core_point_indices\n        }\n\n        # Vereinige alle Kernpunkte, die direkt miteinander verbunden sind\n        cp_clusters = merge_intersecting_sets(cp_neighbors_cp)\n\n        # Erweitere jeden Cluster: F\u00fcge auch alle Punkte hinzu, die in der eps-Umgebung\n        # eines Kernpunkts liegen (auch wenn sie selbst keine Kernpunkte sind)\n        for cluster in cp_clusters:\n            for cp in list(cluster):\n                cluster.update(neighbors[cp])\n\n        # Alle in Clustern enthaltenen Punkte\n        all_clustered_elements = {x for c in cp_clusters for x in c}\n        # Punkte, die in keinem Cluster landen, werden als Rauschen betrachtet (Label -1)\n        not_clustered = set(range(n_samples)) - all_clustered_elements\n\n        # Erstelle ein Dictionary, in dem jeder Cluster (mit einem eindeutigen Index) zugeordnet wird\n        self.clusters = {i: c for i, c in enumerate(cp_clusters)}\n        self.clusters[-1] = not_clustered\n\n        # Erzeuge f\u00fcr jeden Punkt das zugeh\u00f6rige Label anhand des Clusters, in dem er enthalten ist\n        self.labels_ = [self.get_cluster(i) for i in range(n_samples)]\n        return self\n\n    def get_cluster(self, i):\n        \"\"\"\n        Gibt das Cluster-Label des Punktes i zur\u00fcck. Falls i in keinem Cluster ist, wird -1 zur\u00fcckgegeben.\n        \"\"\"\n        for cluster_label, cluster in self.clusters.items():\n            if i in cluster:\n                return cluster_label\n        return -1\n\n    def is_neighbor(self, i, j, dist_matrix):\n        \"\"\"\n        Pr\u00fcft, ob Punkt i und Punkt j Nachbarn sind, also ob ihr Abstand kleiner als eps ist.\n        \"\"\"\n        return dist_matrix[i, j] &lt; self.eps\n\n    def predict(self, X):\n        \"\"\"\n        Da DBSCAN ein \"fit-and-predict\"-Ansatz ist, werden hier bereits berechnete Labels zur\u00fcckgegeben.\n        (F\u00fcr neue Daten m\u00fcsste man weitere Berechnungen durchf\u00fchren.)\n        \"\"\"\n        return self.labels_\n\n\n# -------------------------------\n# Beispiel-Visualisierung der Cluster\n# -------------------------------\nif __name__ == '__main__':\n    # F\u00fcr reproduzierbare Ergebnisse setzen wir einen Seed\n    np.random.seed(42)\n\n    # Anzahl der Punkte pro Cluster\n    n_points = 100\n\n    # Erstellen von drei Clustern im 2D-Raum mit unterschiedlichen Zentren\n    cluster_1 = np.random.randn(n_points, 2) + np.array([0, 0])\n    cluster_2 = np.random.randn(n_points, 2) + np.array([5, 5])\n    cluster_3 = np.random.randn(n_points, 2) + np.array([0, 5])\n\n    # Kombiniere alle Cluster zu einem Array\n    X = np.vstack((cluster_1, cluster_2, cluster_3))\n\n    # Erzeuge das DBSCAN-Modell (Parameter ggf. anpassen)\n    model = MyDBScan(eps=0.75, min_samples=5)\n    model.fit(X)\n\n    # Ausgabe der Labels f\u00fcr jeden Datenpunkt\n    print(\"Cluster-Labels:\", model.labels_)\n\n    # Visualisierung: Farbgebung anhand der Clusterzugeh\u00f6rigkeit (Cluster -1 = Rauschen)\n    # plt.figure(figsize=(8, 6))\n    # plt.scatter(X[:, 0], X[:, 1], c=model.labels_, cmap='viridis', s=30)\n    # plt.title(\"DBSCAN Clustering\")\n    # plt.xlabel(\"X-Achse\")\n    # plt.ylabel(\"Y-Achse\")\n    # plt.grid(True)\n    # plt.show()\n\n\n# -------------------------------\n# Beispieltests f\u00fcr MyDBScan\n# -------------------------------\ndef test_dbscan_zwei_cluster_mit_rauschen():\n    \"\"\"\n    Test: Zwei gut getrennte Cluster und ein einzelner Rauschpunkt.\n    Erwartung:\n      - Punkte aus den Clustern erhalten konsistente (unterschiedliche) Labels (nicht -1).\n      - Der Rauschpunkt erh\u00e4lt Label -1.\n    \"\"\"\n    np.random.seed(0)\n    # Erzeuge zwei Cluster\n    cluster1 = np.random.randn(20, 2) + np.array([0, 0])\n    cluster2 = np.random.randn(20, 2) + np.array([10, 10])\n    # Ein einzelner Rauschpunkt, der weder nahe zu Cluster 1 noch Cluster 2 liegt\n    noise = np.array([[5, 5]])\n    X_test = np.vstack([cluster1, cluster2, noise])\n\n    # W\u00e4hle eps so, dass die beiden Cluster erkannt werden, der Noise aber nicht dazugeh\u00f6rt\n    model = MyDBScan(eps=2.5, min_samples=3)\n    model.fit(X_test)\n    labels = np.array(model.labels_)\n\n    # Cluster 1: Indizes 0 bis 19\n    label_cluster1 = labels[0]\n    # Cluster 2: Indizes 20 bis 39\n    label_cluster2 = labels[20]\n\n    # Pr\u00fcfe, dass beide Cluster nicht als Rauschen (Label -1) markiert wurden\n    assert label_cluster1 != -1, \"Cluster 1 sollte kein Rauschen sein.\"\n    assert label_cluster2 != -1, \"Cluster 2 sollte kein Rauschen sein.\"\n\n    # Alle Punkte in Cluster 1 sollten dasselbe Label haben\n    for i in range(20):\n        assert labels[i] == label_cluster1, \"Nicht alle Punkte in Cluster 1 haben dasselbe Label.\"\n    # Alle Punkte in Cluster 2 sollten dasselbe Label haben\n    for i in range(20, 40):\n        assert labels[i] == label_cluster2, \"Nicht alle Punkte in Cluster 2 haben dasselbe Label.\"\n\n    # Der Rauschpunkt (Index 40) sollte als Rauschen markiert sein (-1)\n    assert labels[40] == -1, \"Der Rauschpunkt sollte das Label -1 erhalten.\"\n\n    print(\"Test 'test_dbscan_zwei_cluster_mit_rauschen' erfolgreich bestanden.\")\n\n\ndef test_dbscan_alle_punkte_cluster():\n    \"\"\"\n    Test: Alle Punkte liegen in einem dichten Cluster.\n    Erwartung:\n      - Alle Punkte erhalten dasselbe Cluster-Label.\n    \"\"\"\n    np.random.seed(1)\n    X_dense = np.random.randn(50, 2)  # dichter Cluster\n    model = MyDBScan(eps=2.0, min_samples=3)\n    model.fit(X_dense)\n    labels = np.array(model.labels_)\n\n    # Alle Labels sollten gleich sein und nicht -1 (kein Rauschen)\n    unique_labels = np.unique(labels)\n    assert len(unique_labels) == 1 and unique_labels[0] != -1, \"Alle Punkte sollten in einem Cluster sein.\"\n\n    print(\"Test 'test_dbscan_alle_punkte_cluster' erfolgreich bestanden.\")\n\n\nif __name__ == '__main__':\n    # F\u00fchre die Tests aus\n    test_dbscan_zwei_cluster_mit_rauschen()\n    test_dbscan_alle_punkte_cluster()\n</code></pre>"},{"location":"content/Clustering/3_distances/","title":"Abst\u00e4nde","text":"<p>Wann immer wir von \u201eNachbarn\u201c, \u201eweit\u201c, \u201enah\u201c usw. sprechen, beziehen wir uns implizit auf Entfernungen. Es gibt jedoch eine Vielzahl g\u00fcltiger Definitionen f\u00fcr Entfernungen.</p> <p>Beispiel</p> <p>Wie kann man die Entfernung zwischen zwei Steckdosen berechnen?</p> <p></p> <p>Beispiel</p> <p>F\u00fcr eine Schachfigur kann man die Entfernung zu jedem anderen Feld berechnen, indem man die minimale Anzahl von Z\u00fcgen z\u00e4hlt, die sie braucht, um dorthin zu gelangen.</p> <p></p> <p>Definition Distanz</p> <p>Wenn du eine Menge \\(M\\) von Elementen hast, f\u00fcr die du eine paarweise Distanz definieren m\u00f6chtest, kannst du das wie folgt tun: Sei \\(d: M \\times M \\rightarrow \\mathbb{R}\\) eine Funktion. \\(d\\) definiert genau dann eine Distanz (Metrik) auf \\(M\\), wenn und nur wenn die folgenden Bedingungen erf\u00fcllt sind:</p> Eigenschaft Formel Erkl\u00e4rung Keine Bewegung n\u00f6tig \\(\\forall x \\in M: d(x, x) = 0\\) Du musst keine Strecke zur\u00fccklegen, wenn du dich nicht vom Ausgangspunkt wegbewegst. Richtungsunabh\u00e4ngig \\(\\forall x, y \\in M: d(x, y) = d(y, x)\\) Die Distanz von \\(x\\) nach \\(y\\) ist identisch mit der Distanz von \\(y\\) nach \\(x\\). Verschiedene Punkte, verschiedene Orte \\(\\forall x, y \\in M: d(x, y) = 0 \\implies x = y\\) Zwischen zwei unterschiedlichen Punkten muss immer ein positiver Abstand bestehen. Keine Abk\u00fcrzungen erlaubt \\(\\forall x, y, z \\in M: d(x, y) \\le d(x, z) + d(z, y)\\) \\(d\\) beschreibt die L\u00e4nge des k\u00fcrzesten Weges zwischen zwei Elementen. \\((d(x,z) + d(z,y))\\) steht f\u00fcr eine Route \u00fcber \\(z\\). Die Summe dieser Abst\u00e4nde ist nicht k\u00fcrzer als der direkte Weg. <p>Excel</p> <p>\ud83d\udcd7Download Excelsheet mit verschiedenen Distanzmatrizen</p> <p></p>"},{"location":"content/D_Bivariate_und_multivariate_Analyse/D_1_Darstellung%20Daten/","title":"D.1 Darstellung multivariater Datens\u00e4tze","text":"1) Daten zu Neugeborenen darstellen <p>F\u00fcr 10 neugeborene Babys sind die nach der Geburt gemessenen Daten f\u00fcr die Gr\u00f6\u00dfe (in cm) und das Gewicht (in kg) in dieser Tabelle dargestellt: </p> Gr\u00f6\u00dfe (cm) 52 50 53 50 54 53 52 50 48 49 Gewicht (kg) 3,7 3,2 3,5 3,4 4 4,2 3,1 2,9 2,7 3,5 <p>Stelle diese Daten ...</p> <p>(i) ... in einem Streudiagramm dar. (D.1.1)</p> <p>(ii) ... in einer Kontingenztabelle dar. (D.1.2)</p> <p>(iii) ... in einem Streifendiagramm dar. (D.1.3)</p> Tipp <ul> <li>F\u00fcr (ii) &amp; (iii) ist die Einteilung in Klassen sinnvoll (notwendig). Bestimme dazu geeignete Klassen.</li> <li>F\u00fcr die L\u00f6sung wurden folgende Klasseneinteilungen vorgenommen</li> <li>Gewicht (a) unter 3 kg, (b) ab 3 kg, (c) ab 3,5 kg, (d) ab 4 kg.</li> <li>Gr\u00f6\u00dfe (a) unter 50 cm, (b) unter 52 cm, (a) ab 52 cm.</li> </ul> L\u00f6sung <p>(i)</p> <p></p> <p>(ii)</p> Gewicht (kg) / Gr\u00f6\u00dfe (cm) bis 2,9 kg 3 - 3,4 kg 3,5 - 3,9 kg ab 4,0 kg bis 49 cm 1 0 1 0 50 und 51 cm 1 2 0 0 ab 52 cm 0 1 2 2 <p>(iii)</p> <p></p>"},{"location":"content/D_Bivariate_und_multivariate_Analyse/D_2_Korrelationsanalyse/","title":"D.2 Korrelationsanalyse","text":""},{"location":"content/D_Bivariate_und_multivariate_Analyse/D_2_Korrelationsanalyse/#d21-metrischer-korrelationskoeffizient-nach-bravais-pearson","title":"D.2.1 Metrischer Korrelationskoeffizient nach Bravais-Pearson","text":"1) Korrelationskoeffizienten zuordnen <p>Ordne den folgenden Streudiagrammen jeweils einen Korrelationskoeffizienten zu.</p> 0.90 0.71 0.31 -0.01 -0.61 -0.97 <p></p> L\u00f6sung 0.90 0.71 0.31 -0.01 -0.61 -0.97 C D F B E A <p></p> <p>Metrischer Korrelationskoeffizient r</p> \\[ r = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x}) \\cdot (y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}  = \\frac{s_{xy}}{s_x \\cdot s_y} \\] \\[ r = \\frac{ \\left( \\sum_{i=1}^{n} x_i \\cdot y_i \\right) - n \\cdot \\bar{x} \\cdot \\bar{y}}{\\sqrt{\\left( \\left( \\sum_{i=1}^{n} x_i^2 \\right) - n \\cdot \\bar{x}^2 \\right) \\cdot \\left( \\left( \\sum_{i=1}^{n} y_i^2 \\right) - n \\cdot \\bar{y}^2 \\right)}} \\] 2) Korrelationskoeffizienten berechnen <p>Berechne den Korrelationskoeffizienten f\u00fcr den folgenden Datensatz:</p> Lernstunden Punktzahl 2 24 4 37 6 62 8 79 <p></p> L\u00f6sung <p>r = 0.993</p> <p>Hinweis: Es gibt einige verschiedene Versionen der Formel. Allgemein ist es aber m\u00f6glich das \"n\" herauszuk\u00fcrzen.</p> <p>Rechenweg:</p> <p></p>"},{"location":"content/D_Bivariate_und_multivariate_Analyse/D_2_Korrelationsanalyse/#d22-rangkorrelationskoeffizient-nach-spearman-pearson","title":"D.2.2 Rangkorrelationskoeffizient nach Spearman-Pearson","text":"<p>Berechnung vom Rangkorrelationskoeffizienten</p> <p>Formel f\u00fcr den Spearman'schen Rangkorrelationskoeffizienten:</p> \\[ r_s = 1 - \\frac{6 \\sum_{i=1}^{n} ( \\text{rg}(x_i) - \\text{rg}(y_i) )^2}{n (n^2 - 1)} \\] <p>Wenn man die Rangdifferenz f\u00fcr jedes Paar \\( i \\) darstellt als</p> \\[ d_i = \\text{rg}(x_i) - \\text{rg}(y_i) \\] <p>kann man diese k\u00fcrzere Darstellung verwenden:</p> \\[ r_s = 1 - \\frac{6 \\sum_{i=1}^{n} d_i^2}{n (n^2 - 1)} \\] <p>Interpretation vom Rangkorrelationskoeffizienten</p> <ul> <li> <p>Wenn die Rangordnung der Werte in beiden Variablen exakt \u00fcbereinstimmt, ist der Wert der Rangkorrelation \\(r_s\\) gleich 1.</p> </li> <li> <p>Wenn die Rangordnung v\u00f6llig entgegengesetzt ist, ist \\(r_s\\) gleich -1.</p> </li> <li> <p>Ein Wert von 0 bedeutet, dass keine monotone Beziehung besteht.</p> </li> </ul> 3) Rangkorrelationskoeffizienten berechnen <p>Von einer Arbeitnehmervereinigung wurde eine anonyme Umfrage durchgef\u00fchrt, wobei der Zusammenhang zwischen K\u00fcndigungsabsicht  und der wahrgenommen Fairness innerhalb des Unternehmens untersucht werden soll.</p> <p>Dabei standen folgende Auswahlm\u00f6glichkeiten zur Verf\u00fcgung:</p> <ul> <li> <p>K\u00fcndigung in Erw\u00e4gung gezogen (X) (in den letzte 3 Monaten); Skala: {A - sehr stark in Erw\u00e4gung gezogen} bis zu {E - \u00fcberhaupt nicht in Erw\u00e4gung gezogen}</p> </li> <li> <p>Unternehmenskultur: Fairness (Y); Skala: Die Bewertung besteht nur aus den 3 Kategorien {gut}, {mittel} und {schlecht}.</p> </li> </ul> <p>Die Umfrage ergab folgende Ergebnisse:</p> Person K\u00fcndigung in Erw\u00e4gung gezogen (X) Unternehmenskultur: Fairness (Y) 1 A schlecht 2 C mittel 3 D gut 4 B mittel 5 E gut 6 B schlecht 7 A gut <p>Berechne den Rangkorrelationskoeffizienten f\u00fcr den Datensatz.</p> L\u00f6sung \\[ r_s = -0.366 \\] <p>Ermittlung der R\u00e4nge:</p> K\u00fcndigung Position Rang Fairness Position Rang E 1 1 schlecht 1, 2 1.5 D 2 2 schlecht 1, 2 1.5 C 3 3 mittel 3, 4 3.5 B 4, 5 4.5 mittel 3, 4 3.5 B 4, 5 4.5 gut 5, 6, 7 6 A 6, 7 6.5 gut 5, 6, 7 6 A 6, 7 6.5 gut 5, 6, 7 6 <p>Tabelle um R\u00e4nge erg\u00e4nzen und Rangdifferenzen berechnen:</p> Person K\u00fcndigung in Erw\u00e4gung gezogen (X) Unternehmenskultur: Fairness (Y) rg(X) rg(Y) di = rg(xi) - rg(yi) di^2 1 A schlecht 6.5 1.5 5 25 2 C mittel 3 3.5 -0.5 0.25 3 D gut 2 6 -4 16 4 B mittel 4.5 3.5 1 1 5 E gut 1 6 -5 25 6 B schlecht 4.5 1.5 3 9 7 A gut 6.5 6 0.5 0.25 \\[ \\sum_{i=1}^{n} d_i^2 = 76.5 \\] <p>Einsetzen in die Formel mit n = 7 ergibt:</p> \\[ r_s = -0.366 \\] <p></p>"},{"location":"content/D_Bivariate_und_multivariate_Analyse/D_3_Regressionsrechnung/","title":"D.3 Regressionsrechnung","text":"<p>Regression h\u00e4ndisch berechnen</p> <p>\u00dcbungsaufgaben zum schriftlichen Berechnen der Regression: 1 2</p> Regression in Python implementieren <p>Implementieren Sie ein Funktion, die bei einer einfachen Regression <code>a</code> und <code>b</code> bestimmen.</p> L\u00f6sung <pre><code>def regression():\n    ...\n</code></pre> Herleitung der Regression verstehen <p>Um die Herleitung der Regression zu verstehen, sollten ein paar Mathematische Inhalte fest sitzen:</p> <ul> <li>Wie geht man mit Summen um?</li> <li>Wie erhalte ich den Hoch- bzw. Tiefpunkt einer Funktion?</li> <li>Wie funktioniert Ableiten?</li> <li>Wie funktioniert partielles Ableiten?</li> <li>Was ist ein Lineares Gleichungssystem (LGS)?</li> <li>Wie l\u00f6st man ein LGS?</li> <li>Wie l\u00f6st man ein LGS mit Hilfe von Matrixinversen?</li> <li>Wie werden Matrizen und Vektoren miteinander multipliziert?</li> <li>Gegeben seien \\(X = (x_1 \\cdots x_n) \\in \\mathbb{R}^{1\\times n}\\) und \\(Y = (y_1 \\cdots y_n) \\in \\mathbb{R}^{1\\times n}\\). Wie kann \\(\\sum_{i=1}^n x_i y_i\\) als Multiplikation der beiden Matrizen / Vektoren notiert werden?</li> <li>Wie kann man auch \\(\\sum_{i=1}^n x_i\\) als das Produkt zweier Matrizen/Vektoren ausdr\u00fccken?</li> </ul> <p>Um das ganze noch gut in Python umzusetzen bleibt noch sicherzustellen, dass man bei Matrizen die folgenden Operationen durchf\u00fchren kann:</p> <ul> <li>Transponieren,</li> <li>Multiplizieren,</li> <li>Inverse bilden.</li> </ul>"},{"location":"content/D_Bivariate_und_multivariate_Analyse/D_3_Regressionsrechnung/#schritweise-herleitung-der-regressionsgeraden","title":"Schritweise Herleitung der Regressionsgeraden","text":""},{"location":"content/D_Bivariate_und_multivariate_Analyse/D_3_Regressionsrechnung/#lineare-funktion","title":"Lineare Funktion","text":"<p>Es solle eine lineare Funktion aufgestellt werden, welche die folgende Form hat:</p> \\[ y = f(x) = a + b\\cdot x \\] <p>\\(a\\) ist der Schnitt mit der \\(y\\)-Achse mit der Geraden und \\(b\\) die Steigung der Gerade.</p>"},{"location":"content/D_Bivariate_und_multivariate_Analyse/D_3_Regressionsrechnung/#datensatz","title":"Datensatz","text":"<p>Es liegt ein Datensatz von \\(X = (x_1, \\cdots , x_n)\\) und \\(Y = (y_1, \\cdots, y_n)\\) Werten vor. \\(X\\) ist die unabh\u00e4ngige Variable und aus dieser soll \\(Y\\) hergeleitet werden, welche deswegen abh\u00e4ngige Variable hei\u00dft.</p>"},{"location":"content/D_Bivariate_und_multivariate_Analyse/D_3_Regressionsrechnung/#regressionsgerade","title":"Regressionsgerade","text":"<p>Da es zwischen \\(X\\) und \\(Y\\), keinen perfekten linearen Zusammenhang gibt, wird jede Funktion, die wir aufstellen, fehlerbehaftet sein.</p> <p>Wenn wir also \\(a\\) und \\(b\\) w\u00e4hlen und damit eine Regressionsgerade \\(f(x) = a + b\\cdot x\\) aufstellen, so berechnet diese nicht direkt die \\(y\\)-Werte, sondern nur (hoffentlich gute) Ann\u00e4herungen.</p> <p>Daher nenen wir das Ergebnis von \\(f(x)\\) auch oft \\(\\hat{y}\\).</p> <p>Wir wollen also versuchen den Unterschied von \\(y\\) und \\(\\hat{y}\\) zu minieren. Nicht nur, dass, wir wollen sogar, das Quadrat dieses Unterschiedes minimieren, also </p> \\[ \\text{minimiere } (y - \\hat{y})^2 \\] <p>Da wir aber in \\(Y =  (y_1, \\cdots, y_n)\\) sehr viele \\(y\\)'s haben, wollen wir die Summe all dieser Unterschiede minimieren:</p> \\[ \\text{minimiere } \\sum_{i=1}^n (y_i - \\hat{y_i})^2 \\]"},{"location":"content/D_Bivariate_und_multivariate_Analyse/D_3_Regressionsrechnung/#fehlerfunktion-aufstellen","title":"Fehlerfunktion aufstellen","text":"<p>Wir nennen diese Summe \\(E\\) und l\u00f6sen Sie im folgenden auf, um die Abh\u00e4ngigkeit von \\(a\\) und \\(b\\) deutlich zu machen:</p> \\[ E = \\sum_{i=1}^n (y_i - \\hat{y_i})^2 = \\sum_{i=1}^n (y_i - f(x_i))^2 = \\sum_{i=1}^n (y_i - (a + b\\cdot x_i))^2 =\\sum_{i=1}^n (y_i - a - b\\cdot x_i)^2   \\] <p>Da \\(a\\) und \\(b\\), als einzige von uns frei w\u00e4hlbar sind und die \\(x\\)- und \\(y\\)-Werte von Vornherein festgelegt sind, k\u00f6nnen wir \\(E\\) als eine Funktion betrachten, die von \\(a\\) und \\(b\\) abh\u00e4ngig ist.</p> \\[ E(a,b) =\\sum_{i=1}^n (y_i - a - b\\cdot x_i)^2   \\]"},{"location":"content/D_Bivariate_und_multivariate_Analyse/D_3_Regressionsrechnung/#fehlerfunktion-minimieren","title":"Fehlerfunktion minimieren","text":"<p>Wir suchen nun die \\(a\\) und \\(b\\), sodas \\(E(a,b)\\) minimal wird.</p> <p>Theoretisch wollen wir die Ableitung bilden und sie gleich \\(0\\) setzen:</p> \\[ E' = 0 \\] <p>Da \\(E(a,b)\\), jedoch von zwei Variablen abh\u00e4ngt, m\u00fcssen wir hier die beiden partiellen Ableitungen bilden:</p> \\[ E'_a = \\frac{\\partial E(a, b)}{\\partial a}  \\text{ und } E'_b = \\frac{\\partial E(a, b)}{\\partial b} \\]"},{"location":"content/D_Bivariate_und_multivariate_Analyse/D_3_Regressionsrechnung/#fehlerfunktion-ableiten","title":"Fehlerfunktion ableiten","text":"\\[ E(a,b) =\\sum_{i=1}^n (y_i - a - b\\cdot x_i)^2   \\] <p>Um die Fehlerfunktion ableiten, betrachten wir erstmal nur die  Summanden:</p> \\[ (y_i - a - b\\cdot x_i)^2 \\] <p>Die \\(x_i\\) und \\(y_i\\) k\u00f6nnen wir als feste Zahlen betrachten.</p> <p>Leiten wir den Ausdruck zun\u00e4chst nach \\(a\\) ab:</p> \\[ \\frac{\\partial }{\\partial a}  (y_i - a - b\\cdot x_i)^2 = 2 (a - y_i +b\\cdot x_i) \\] <p>Dann leiten wir den Summanden auch noch nach \\(b\\) ab:</p> \\[ \\frac{\\partial }{\\partial b}  (y_i - a - b\\cdot x_i)^2 = 2x (x_i b - y_i + a) \\] <p>\\(E(a,b)\\)  ist eine Summe:</p> \\[ E(a,b) =\\sum_{i=1}^n (y_i - a - b\\cdot x_i)^2 = (y_1 - a - b\\cdot x_1)^2 + (y_2 - a - b\\cdot x_2)^2 + \\cdots +  (y_n - a - b\\cdot x_n)^2   \\] <p>Wenn wir eine Summe ableiten, sagen uns die Rechenregeln, dass wir jeden Summanden einzeln ableiten und die Ergebnisse summieren:</p> \\[ \\frac{\\partial E(a, b)}{\\partial a} = \\frac{\\partial }{\\partial a}(y_1 - a - b\\cdot x_1)^2 + \\frac{\\partial }{\\partial a}(y_2 - a - b\\cdot x_2)^2 + \\cdots + \\frac{\\partial }{\\partial a}(y_n - a - b\\cdot x_n)^2 \\] <p>also:</p> \\[ \\frac{\\partial E(a, b)}{\\partial a} = 2(a-y_1 + b \\cdot x_1) + 2(a-y_2 + b \\cdot x_2) + \\cdots + 2(a-y_n + b \\cdot x_n) \\] <p>Wir schreiben das kompakt mit dem Summenzeichen als:</p> \\[ \\frac{\\partial E(a, b)}{\\partial a} = \\sum_{i=1}^n 2(a-y_i + b \\cdot x_i)  \\] <p>Analog, l\u00e4sst sich alles mit \\(b\\) durchf\u00fchren:</p> \\[ \\frac{\\partial E(a, b)}{\\partial b} = \\sum_{i=1}^n 2x_i(a-y_i + b \\cdot x_i)  \\]"},{"location":"content/D_Bivariate_und_multivariate_Analyse/D_3_Regressionsrechnung/#ableitungen-gleich-0-setzen","title":"Ableitungen gleich 0 setzen","text":"<p>Wir setzen nun die Ableitungen gleich \\(0\\)</p> \\[ 0 = \\frac{\\partial E(a, b)}{\\partial a} = \\sum_{i=1}^n 2(a-y_i + b \\cdot x_i) = 2 \\sum_{i=1}^n (a-y_i + b \\cdot x_i) \\] \\[ 0 = \\frac{\\partial E(a, b)}{\\partial b} = \\sum_{i=1}^n 2x_i(a-y_i + b \\cdot x_i) = 2 \\sum_{i=1}^n (ax_i-y_i x_i + b \\cdot x_i^2) \\] <p>Ich kann auf beiden Seiten der Gleichung durch \\(2\\) teilen:</p> \\[ 0 =  \\sum_{i=1}^n (a-y_i + b \\cdot x_i)  \\] \\[ 0 =  \\sum_{i=1}^n (ax_i-y_i x_i + b \\cdot x_i^2) \\] <p>Wir errinnern uns, wie man ein Summenzeichen auf die  einzelnen Summanden aufteilt:</p> \\[ \\begin{align} \\sum_{i=1}^n (x_i + y_i) &amp;= (x_1 + y_1) + (x_2 + y_2) + \\cdots + (x_n + y_n) \\\\ &amp;= (x_1 + x_2 + \\cdots + x_n) + (y_1 + y_2 + \\cdots + y_n) \\\\ &amp;= \\sum_{i=1}^n x_i + \\sum_{i=1}^n y_i \\end{align} \\] <p>Wir verteilen nun die Summenzeichen:</p> \\[ 0 =  \\sum_{i=1}^n (a-y_i + b \\cdot x_i) = \\sum_{i=1}^n a \\cdot 1 - \\sum_{i=1}^n y_i + \\sum_{i=1}^n b\\cdot x_i  = a \\sum_{i=1}^n 1 - \\sum_{i=1}^n y_i + b \\sum_{i=1}^n x_i  \\] \\[ 0 =  \\sum_{i=1}^n (ax_i-y_i x_i + b \\cdot x_i^2) = \\sum_{i=1}^n ax_i - \\sum_{i=1}^n y_i x_i + \\sum_{i=1}^n b\\cdot x_i^2 = a\\sum_{i=1}^n x_i - \\sum_{i=1}^n y_i x_i + b\\sum_{i=1}^n  x_i^2 \\] <p>Es gilt </p> \\[ \\sum_{i=1}^5 1 = 1+ 1+1+1+1 = 5\\cdot 1 = 5 \\] <p>also allgemein: </p> \\[ \\sum_{i=1}^n 1 = (1+\\cdots +1) = n \\cdots 1 = n \\] <p>Also k\u00f6nnen wir auch noch eine Summe k\u00fcrzer notieren als:</p> \\[ a\\sum_{i=1}^n 1 = n\\cdot a \\]"},{"location":"content/D_Bivariate_und_multivariate_Analyse/D_3_Regressionsrechnung/#gleichungen-ordnen","title":"Gleichungen ordnen","text":"<p>Ordnen wir nun die Gleichungen nach den \\(y_i\\), \\(a\\) und \\(b\\):</p> \\[ \\begin{matrix} -\\sum_{i=1}^n y_i &amp;+&amp; an &amp;+&amp; b \\sum_{i=1}^n x_i &amp;=&amp; 0 \\\\ -\\sum_{i=1}^n y_i x_i &amp;+&amp; a \\sum_{i=1}^n x_i &amp;+&amp; b \\sum_{i=1}^n x_i^2 &amp;=&amp; 0 \\end{matrix} \\] <p>Wir verschieben die \\(y\\)-Summanden nach rechts:</p> \\[ \\begin{matrix} an &amp;+&amp; b \\sum_{i=1}^n x_i &amp;=&amp; \\sum_{i=1}^n y_i \\\\ a \\sum_{i=1}^n x_i &amp;+&amp; b \\sum_{i=1}^n x_i^2 &amp;=&amp; \\sum_{i=1}^n y_i x_i \\end{matrix} \\]"},{"location":"content/D_Bivariate_und_multivariate_Analyse/D_3_Regressionsrechnung/#gleichung-im-matrix-verwandeln","title":"Gleichung im Matrix verwandeln","text":"\\[ \\begin{matrix} a R &amp;+&amp; b T &amp;=&amp; U \\\\ a V &amp;+&amp; b W &amp;=&amp; Z \\end{matrix} \\] <p>Wir k\u00f6nnen das als Matrixmulitpliktion wie folgt notieren:</p> \\[ \\begin{pmatrix} Ra + Tb \\\\ Va + Wb \\end{pmatrix} = \\begin{pmatrix} R &amp; T \\\\ V &amp; W \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\begin{pmatrix} U \\\\ Z \\end{pmatrix} \\] <p>Wir k\u00f6nnen nun \\(R, S, T, U, V, W, Z\\) wieder aufl\u00f6sen:</p> \\[ \\begin{pmatrix} n &amp; \\sum_{i=1}^n x_i \\\\ \\sum_{i=1}^n x_i &amp; \\sum_{i=1}^n x_i^2 \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\begin{pmatrix} \\sum_{i=1}^n y_i \\\\ \\sum_{i=1}^n y_i x_i \\end{pmatrix} \\]"},{"location":"content/D_Bivariate_und_multivariate_Analyse/D_3_Regressionsrechnung/#summenzeichen-in-matrixoperationen-verwandeln","title":"Summenzeichen in Matrixoperationen verwandeln","text":"<p>Gegeben sind ja \\(X = (x_1 \\cdots x_n) \\in \\mathbb{R}^{1\\times n}\\) und \\(Y = (y_1 \\cdots y_n) \\in \\mathbb{R}^{1\\times n}\\).</p> <p>Wie kann \\(\\sum_{i=1}^n x_i y_i\\) als Multiplikation der beiden Matrizen / Vektoren notiert werden?</p> \\[  x_1\\cdot y_1 + x_2 \\cdot y_2 + \\cdots + x_n \\cdot y_n = (x_1 \\cdots x_n) \\cdot  \\begin{pmatrix} y_1\\\\ \\cdots\\\\ y_n \\end{pmatrix} = (x_1 \\cdots x_n) \\cdot (y_1 \\cdots y_n)^T  = XY^T  = \\sum_{i=1}^n x_i y_i \\] <p>bzw.:</p> \\[ (y_1 \\cdots y_n) \\cdot  \\begin{pmatrix} x_1\\\\ \\cdots\\\\ x_n \\end{pmatrix} = Y X^T = \\sum_{i=1}^n y_i x_i \\] <p>Wir k\u00f6nnen dies auch f\u00fcr die anderen summen tun:</p> \\[ \\sum_{i=1}^n x_i^2 = \\sum_{i=1}^n x_i \\cdot x_i = XX^T \\] <p>Sei \\(\\mathbb{1} = (1 \\cdots 1) \\in \\mathbb{R}^{1\\times n}\\)</p> \\[ \\sum_{i=1}^n x_i = \\sum_{i=1}^n 1 \\cdot x_i = (1 \\cdots 1) \\cdot  \\begin{pmatrix} x_1\\\\ \\cdots\\\\ x_n \\end{pmatrix} = \\mathbb{1} X^T = X \\mathbb{1}^T \\] \\[ \\sum_{i=1}^n y_i  = \\mathbb{1} Y^T = Y \\mathbb{1}^T \\] \\[ n = \\sum_{i=1}^n 1 = \\sum_{i=1}^n 1 \\cdot 1 = \\mathbb{1} \\cdot \\mathbb{1}^T \\]"},{"location":"content/D_Bivariate_und_multivariate_Analyse/D_3_Regressionsrechnung/#summen-zu-matrixmultiplikationen-umschreiben","title":"Summen zu Matrixmultiplikationen umschreiben","text":"\\[ \\begin{pmatrix} n &amp; \\sum_{i=1}^n x_i \\\\ \\sum_{i=1}^n x_i &amp; \\sum_{i=1}^n x_i^2 \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\begin{pmatrix} \\sum_{i=1}^n y_i \\\\ \\sum_{i=1}^n y_i x_i \\end{pmatrix} \\] \\[ \\begin{pmatrix} \\mathbb{1} \\cdot \\mathbb{1}^T &amp; \\mathbb{1} \\cdot X^T \\\\ X \\cdot \\mathbb{1}^T          &amp; X \\cdot X^T \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\begin{pmatrix} \\mathbb{1} \\cdot Y^T \\\\ X \\cdot Y^T \\end{pmatrix} \\] <p>Es seien \\(r = \\mathbb{1}, s = \\mathbb{1}^T, t = X^T, u = X\\). Dann ist:</p> \\[ \\begin{pmatrix} r \\cdot s &amp; r \\cdot t \\\\ u \\cdot s &amp; u \\cdot t \\end{pmatrix} = \\begin{pmatrix} r\\\\ u \\end{pmatrix} \\begin{pmatrix} s&amp;t \\end{pmatrix} \\] <p>Wir k\u00f6nnen also oben wieder vereinfachen:</p> \\[ \\begin{pmatrix} \\mathbb{1}\\\\ X \\end{pmatrix} \\begin{pmatrix} \\mathbb{1}^T &amp; X^T \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\begin{pmatrix} \\mathbb{1} \\cdot \\mathbb{1}^T &amp; \\mathbb{1} \\cdot X^T \\\\ X \\cdot \\mathbb{1}^T          &amp; X \\cdot X^T \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\begin{pmatrix} \\mathbb{1} \\cdot Y^T \\\\ X \\cdot Y^T \\end{pmatrix} = \\begin{pmatrix} \\mathbb{1}  \\\\ X \\end{pmatrix} \\cdot Y^T \\] <p>Wir k\u00f6nnen die rechte Seite schreiben als:</p> \\[ \\begin{pmatrix} \\mathbb{1}  \\\\ X \\end{pmatrix} \\cdot Y^T = \\begin{pmatrix} 1 &amp; \\cdots &amp; 1  \\\\ x_1 &amp; \\cdots &amp; x_n \\end{pmatrix} \\cdot \\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_n \\end{pmatrix} \\] <p>und die linke Seite wie folgt</p> \\[ \\begin{pmatrix} \\mathbb{1}\\\\ X \\end{pmatrix} \\begin{pmatrix} \\mathbb{1}^T &amp; X^T \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\begin{pmatrix} 1 &amp; \\cdots &amp; 1  \\\\ x_1 &amp; \\cdots &amp; x_n \\end{pmatrix} \\begin{pmatrix} 1  &amp; x_1\\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\begin{pmatrix} \\mathbb{1}\\\\ X \\end{pmatrix} \\begin{pmatrix} \\mathbb{1}\\\\ X \\end{pmatrix}^T \\begin{pmatrix} a\\\\ b \\end{pmatrix} \\] <p>Also insgesamt:</p> \\[ \\begin{pmatrix} \\mathbb{1}\\\\ X \\end{pmatrix} \\begin{pmatrix} \\mathbb{1}\\\\ X \\end{pmatrix}^T \\begin{pmatrix} a\\\\ b \\end{pmatrix} = \\begin{pmatrix} \\mathbb{1}  \\\\ X \\end{pmatrix} \\cdot Y^T \\] \\[ \\begin{pmatrix} a\\\\ b \\end{pmatrix} = \\left( \\begin{pmatrix} \\mathbb{1}\\\\ X \\end{pmatrix} \\begin{pmatrix} \\mathbb{1}\\\\ X \\end{pmatrix}^T \\right)^{-1} \\begin{pmatrix} \\mathbb{1}\\\\ X \\end{pmatrix} \\begin{pmatrix} \\mathbb{1}\\\\ X \\end{pmatrix}^T \\begin{pmatrix} a\\\\ b \\end{pmatrix} = \\left( \\begin{pmatrix} \\mathbb{1}\\\\ X \\end{pmatrix} \\begin{pmatrix} \\mathbb{1}\\\\ X \\end{pmatrix}^T \\right)^{-1} \\begin{pmatrix} \\mathbb{1}  \\\\ X \\end{pmatrix} \\cdot Y^T \\] \\[ \\begin{pmatrix} a\\\\ b \\end{pmatrix} = \\left( \\begin{pmatrix} \\mathbb{1}\\\\ X \\end{pmatrix} \\begin{pmatrix} \\mathbb{1}\\\\ X \\end{pmatrix}^T \\right)^{-1} \\begin{pmatrix} \\mathbb{1}  \\\\ X \\end{pmatrix} \\cdot Y^T \\] Multiple Regression in Python implementieren <p>Der folgende Code zeigt, wie man eindimensionale lineare Regression durchf\u00fchren kann, indem man die Rechnun auf Matrixmultiplikationen zur\u00fcckf\u00fchrt.</p> <pre><code>import numpy as np\nfrom unittest import TestCase, main\nfrom parameterized import parameterized\n\ndef regression(x, y):\n    ones_x = np.vstack((\n        np.ones(len(x)),  # Erste Zeile: Einsen\n        x  # Zweite Zeile: Die unabh\u00e4ngige Variable\n    ))\n\n    return np.linalg.inv(ones_x @ ones_x.T) @ ones_x @ np.array(y).T\n\n\n\nclass TestDeviation(TestCase):\n    @parameterized.expand([\n        ([1, 2, 3], [1, 2, 3], 0, 1),  # no bias\n        ([1, 2, 3], [2, 4, 6], 0, 2),  # no bias\n        ([0, 1, 2], [3, 2, 1], 3, -1),  # negative\n        ([0, 1, 2], [1, 2, 3], 1, 1),  # basic\n        ([1, 2, 3], [3, 5, 7], 1, 2),  # basic\n        ([0, 1, 2], [1, 1, 1], 1, 0),  # constant function\n        ([1, 2, 3], [3, 3, 3], 3, 0),  # contant function\n    ])\n    def test_basic_lin_reg(self, x, y, a, b):\n        result = regression(x, y)\n        self.assertAlmostEqual(float(result[0]), a)\n        self.assertAlmostEqual(float(result[1]), b)\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Diese Code soll nun so erweitert werden, dass eine Linearen Regression mit mehreren unabh\u00e4ngigen Variablen m\u00f6glich ist. Beachte dazu,  dass bisher die \\(X\\)-Werte als eine einfache Liste \u00fcbergeben werden. Dies ist dann nicht mehr m\u00f6glich. Die Werte werden als Matrix \u00fcbergeben. Hier m\u00fcssen dann auch die Tests angepasst werden  (z.B.: aus <code>([1, 2, 3], [2, 4, 6], 0, 2)</code> wird  <code>([[1], [2], [3]], [2, 4, 6], 0, 2)</code>).</p> <p></p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_0_Vortr%C3%A4ge_Verteilungen/","title":"Vortr\u00e4ge zu Verteilungen","text":""},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_0_Vortr%C3%A4ge_Verteilungen/#verteilungen","title":"Verteilungen","text":"Verteilung Team Breakoutroom Start Ende Binomialverteilung Kevin, Maurice, Tom 1 10:30 11:10 Geometrische Verteilung Johannes, Tobias, Daniel 2 11:20 12:00 Hypergeometrische Verteilung Ly, Christoph, Henrik 3 12:50 13:30 Poissonverteilung Elias, Andreas, David 4 13:40 14:20 Exponentialverteilung Florentin, Waldemar, Christian 5 14:30 15:10 Pareto-Verteilung Fabian, Konstantin, Nick, Noah 6 15:20 16:00"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_0_Vortr%C3%A4ge_Verteilungen/#was-wird-erwartet","title":"Was wird erwartet","text":"<ul> <li>.md erstellen, die in die Webseite eingebunden werden kann. Weitere Datein, die damit Verkn\u00fcpft sind (.ppt, .ipynb) sind erlaubt.</li> <li>Was ist das f\u00fcr eine Verteilung und wozu dient sie?</li> <li>Definition nachvollziehbar darstellen (d.h. Herleitung nach M\u00f6glichkeit erkl\u00e4ren)</li> <li>Anwendungsbezogene Beispiele (alltagsbezogen)</li> <li>Rechenaufgaben/MC-Fragen/Freitext-Aufgaben/Wissenfragen erstellen.</li> <li>Ggf. Programmierung (selbst programmieren/Beispiel pr\u00e4sentieren/Bibliotheken vorstellen/...)</li> </ul>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_0_Vortr%C3%A4ge_Verteilungen/#zeitplan","title":"Zeitplan","text":"<p>Tag 1:</p> <p>Ausarbeiten des Themas bis 16:00</p> <p>Tag 2:</p> <p>Finalisierung und Austausch.</p> <p>N\u00e4chste Woche: Test und Korrektur.</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_10%20Standardnormalverteilung/","title":"F.5.10 Standardnormalverteilung","text":""},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_10%20Standardnormalverteilung/#ub-","title":"\u00dcb----------------------------","text":""},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_10%20Standardnormalverteilung/#ub-_1","title":"\u00dcb----------------------------","text":""},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/","title":"Binomialverteilung","text":""},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#ursprung-in-baumdiagrammen","title":"Ursprung in Baumdiagrammen","text":"<p>Die Binomialverteilung findet ihren Ursprung in Entscheidungsb\u00e4umen. Ein Baumdiagramm stellt eine Folge von unabh\u00e4ngigen Versuchen mit jeweils zwei m\u00f6glichen Ergebnissen (Erfolg/Misserfolg) grafisch dar.</p> <p>Baumdiagramme</p> <p>Beispiel f\u00fcr ein Baumdiagramm (zwei W\u00fcrfe mit einer fairen M\u00fcnze)</p> <pre><code>        Start\n        /   \\\n      K      Z  (1. Wurf)\n     / \\    / \\\n    K   Z  K   Z  (2. Wurf)\n</code></pre> <ul> <li>Jede Kante entspricht einer Wahrscheinlichkeit von 50%.</li> <li>Die m\u00f6glichen Ergebnisse nach zwei W\u00fcrfen sind KK, KZ, ZK, ZZ.</li> <li>Die Wahrscheinlichkeit f\u00fcr jedes dieser Ergebnisse betr\u00e4gt \\(0.5 \\cdot 0.5 = 0.25\\).</li> </ul> <p>Baumdiagramme sind besonders anschaulich, weil sie jeden einzelnen Pfad der M\u00f6glichkeiten visualisieren und das Zusammenspiel der Wahrscheinlichkeiten verdeutlichen. Dies legt die Grundlage f\u00fcr die Binomialverteilung.</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#binomialverteilung-einfach-erklart","title":"Binomialverteilung einfach erkl\u00e4rt","text":"<p>Die Binomialverteilung beschreibt die Anzahl der Erfolge in einer festen Anzahl von unabh\u00e4ngigen Versuchen, wobei jeder Versuch nur zwei m\u00f6gliche Ergebnisse hat: Erfolg oder Misserfolg. Wie die Silbe \"Bi\" (lateinisch: Zwei) schon andeutet, dreht sich alles um zwei m\u00f6gliche Resultate: Erfolg oder Misserfolg, ja oder nein, Treffer oder kein Treffer. Solche \"entweder oder\"-Experimente mit nur zwei m\u00f6glichen Ergebnissen nennt man Bernoulli-Experimente.</p> <p>Klassisches Beispiel</p> <p>Ein klassisches Beispiel ist der M\u00fcnzwurf, bei dem es nur zwei m\u00f6gliche Ergebnisse gibt: Kopf (Erfolg) oder Zahl (Misserfolg). Die Binomialverteilung hilft dabei, die Wahrscheinlichkeit f\u00fcr eine bestimmte Anzahl von Erfolgen in einer Serie solcher Versuche zu berechnen.</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#definition-der-binomialverteilung","title":"Definition der Binomialverteilung","text":"<p>Binomialverteilung</p> <p>Ein Zufallsexperiment ist binomialverteilt, wenn:</p> <ol> <li>Es gibt \\(n\\) unabh\u00e4ngige Versuche.</li> <li>Jeder Versuch hat zwei m\u00f6gliche Ergebnisse: Erfolg (mit Wahrscheinlichkeit \\(p\\)) oder Misserfolg (mit Wahrscheinlichkeit \\(1-p\\)).</li> <li>Die Erfolgswahrscheinlichkeit \\(p\\) bleibt bei jedem Versuch gleich.</li> </ol> <p>Die Binomialverteilung ist eine der wichtigsten diskreten Wahrscheinlichkeitsverteilungen. Sie wird oft verwendet, um Prozesse zu modellieren, bei denen es nur zwei m\u00f6gliche Ausg\u00e4nge gibt.</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#binomialverteilung-formel","title":"Binomialverteilung Formel","text":"<p>Binomialverteilung Formel</p> <p></p> <p>Die Wahrscheinlichkeit, dass in \\(n\\) Versuchen genau \\(k\\) Erfolge auftreten, wird durch folgende Formel beschrieben:</p> \\[ P(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k} \\]"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#bestandteile-der-formel","title":"Bestandteile der Formel","text":"<ul> <li>\\(n\\): Anzahl der Versuche.</li> <li>\\(k\\): Anzahl der Erfolge.</li> <li>\\(p\\): Wahrscheinlichkeit eines Erfolgs pro Versuch.</li> <li>\\((1-p)\\): Wahrscheinlichkeit eines Misserfolgs pro Versuch.</li> <li>\\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\): Der Binomialkoeffizient, der angibt, auf wie viele Arten \\(k\\) Erfolge in \\(n\\) Versuchen angeordnet werden k\u00f6nnen.</li> </ul>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#erklarung-der-bestandteile-der-binomialverteilung","title":"Erkl\u00e4rung der Bestandteile der Binomialverteilung","text":""},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#der-binomialkoeffizient","title":"Der Binomialkoeffizient","text":"<p>Der Binomialkoeffizient \\(\\binom{n}{k}\\) beschreibt die Anzahl der M\u00f6glichkeiten, \\(k\\) Erfolge aus \\(n\\) Versuchen auszuw\u00e4hlen:</p> \\[ \\binom{n}{k} = \\frac{n!}{k!(n-k)!} \\] <p>Hierbei steht \\(n!\\) f\u00fcr die Fakult\u00e4t von \\(n\\), also das Produkt aller nat\u00fcrlichen Zahlen von \\(1\\) bis \\(n\\).</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#wahrscheinlichkeit-eines-erfolgs","title":"Wahrscheinlichkeit eines Erfolgs","text":"<p>Der Term \\(p^k\\) beschreibt die Wahrscheinlichkeit, dass genau \\(k\\) Erfolge eintreten. Wenn die Erfolgswahrscheinlichkeit eines einzelnen Versuchs \\(p\\) ist, multiplizieren wir sie \\(k\\)-mal mit sich selbst.</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#wahrscheinlichkeit-eines-misserfolgs","title":"Wahrscheinlichkeit eines Misserfolgs","text":"<p>Der Term \\((1-p)^{n-k}\\) beschreibt die Wahrscheinlichkeit, dass die restlichen \\((n-k)\\) Versuche fehlschlagen.</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#zusammensetzung-der-formel","title":"Zusammensetzung der Formel","text":"<p>Die gesamte Wahrscheinlichkeit ergibt sich aus der Kombination der m\u00f6glichen Anordnungen der Erfolge (\\(\\binom{n}{k}\\)) und den Wahrscheinlichkeiten f\u00fcr Erfolge (\\(p^k\\)) und Misserfolge (\\((1-p)^{n-k}\\)).</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#kumulierte-binomialverteilung","title":"Kumulierte Binomialverteilung","text":"<p>Kumulierte Binomialverteilung</p> <p>Die kumulierte Binomialverteilung gibt die Wahrscheinlichkeit an, dass die Zufallsvariable \\(X\\) h\u00f6chstens einen bestimmten Wert \\(k\\) annimmt. Sie berechnet sich durch die Summation der Wahrscheinlichkeiten bis zu \\(k\\):</p> \\[ F(X \\leq k) = \\sum_{i=0}^k \\binom{n}{i} p^i (1-p)^{n-i} \\] <p>Beispiel</p> <p>Ein Basketballspieler mit \\(p = 0.7\\) wirft \\(n = 10\\) Mal. Wie hoch ist die Wahrscheinlichkeit, dass er h\u00f6chstens \\(k = 3\\) Treffer erzielt?</p> \\[ F(X \\leq 3) = P(X = 0) + P(X = 1) + P(X = 2) + P(X = 3) \\] <p>F\u00fcr jeden Wert von \\(i\\) wird die Binomialformel angewandt, und die Ergebnisse werden addiert.</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#deskriptive-mae-der-binomialverteilung","title":"Deskriptive Ma\u00dfe der Binomialverteilung","text":"<p>Deskriptive Ma\u00dfe</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#erwartungswert-ex","title":"Erwartungswert \\(E(X)\\)","text":"<p>Der Erwartungswert gibt an, wie viele Erfolge man im Durchschnitt erwarten kann:</p> \\[ E(X) = n \\cdot p \\]"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#varianz-textvarx","title":"Varianz \\(\\text{Var}(X)\\)","text":"<p>Die Varianz misst die Streuung der Werte der Zufallsvariablen um den Erwartungswert:</p> \\[ \\text{Var}(X) = n \\cdot p \\cdot (1-p) \\]"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#standardabweichung-sigmax","title":"Standardabweichung \\(\\sigma(X)\\)","text":"<p>Die Standardabweichung ist die Wurzel der Varianz:</p> \\[ \\sigma_X = \\sqrt{n \\cdot p \\cdot (1-p)} \\]"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#exkurs-normalapproximation-der-binomialverteilung","title":"Exkurs: Normalapproximation der Binomialverteilung","text":"<p>Bei sehr gro\u00dfen Werten von \\(n\\) kann die Berechnung der Binomialverteilung zeit- und rechenaufwendig werden, da die Berechnung des Binomialkoeffizienten und die Potenzierungen hohe Zahlen umfassen.</p> <p>Um dies zu vermeiden, wird oft die Normalapproximation genutzt, bei der die Binomialverteilung durch eine Normalverteilung angen\u00e4hert wird. Diese Methode basiert auf der Tatsache, dass die Binomialverteilung f\u00fcr gro\u00dfe \\(n\\) und moderate Werte von \\(p\\) (also \\(n \\cdot p \\geq 5\\) und \\(n \\cdot (1-p) \\geq 5\\)) nahezu symmetrisch wird und der Normalverteilung \u00e4hnelt.</p> <p>Die Approximation reduziert den Rechenaufwand erheblich, da anstelle komplexer Summen und Fakult\u00e4ten nur Wahrscheinlichkeiten einer Normalverteilung berechnet werden m\u00fcssen. Dabei wird die Stetigkeitskorrektur eingef\u00fchrt, um die Diskretheit der Binomialverteilung anzupassen.</p> <p>Das macht die Normalapproximation besonders bei gro\u00dfen Stichproben n\u00fctzlich und ist eine weitverbreitete Methode in Statistik und Wahrscheinlichkeitstheorie.</p> <p>Die Binomialverteilung \\(X \\sim B(n, p)\\) wird durch die Normalverteilung \\(Z \\sim N(\\mu, \\sigma^2)\\) approximiert, wobei:</p> <ul> <li> <p>Mittelwert (Erwartungswert): \\(\\mu = n \\cdot p\\)</p> </li> <li> <p>Standardabweichung: \\(\\sigma = \\sqrt{n \\cdot p \\cdot (1 - p)}\\)</p> </li> <li> <p>Standardisierung Um Wahrscheinlichkeiten mit der Standardnormalverteilung \\(Z \\sim N(0, 1)\\) zu berechnen, wird \\(X\\) wie folgt standardisiert:</p> </li> </ul> \\[ Z = \\frac{X - \\mu}{\\sigma} \\] <ul> <li>Korrektur der Stetigkeit</li> </ul> <p>Da die Binomialverteilung diskret ist und die Normalverteilung stetig, wird h\u00e4ufig die Korrektur der Stetigkeit angewendet, indem man bei der Berechnung $ \\pm 0.5 $ ber\u00fccksichtigt:</p> <ul> <li> <p>F\u00fcr \\(P(X \\leq k)\\): \\(P(X \\leq k) \\approx P\\left(Z \\leq \\frac{k + 0.5 - \\mu}{\\sigma}\\right)\\)</p> </li> <li> <p>F\u00fcr \\(P(X \\geq k)\\): \\(P(X \\geq k) \\approx P\\left(Z \\geq \\frac{k - 0.5 - \\mu}{\\sigma}\\right)\\)</p> </li> <li> <p>F\u00fcr \\(P(X = k)\\): \\(P(X = k) \\approx P\\left(\\frac{k - 0.5 - \\mu}{\\sigma} \\leq Z \\leq \\frac{k + 0.5 - \\mu}{\\sigma}\\right)\\)</p> </li> </ul>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#verbindung-zum-pascalschen-dreieck","title":"Verbindung zum Pascalschen Dreieck","text":"<p>Der Binomialkoeffizient \\(\\binom{n}{k}\\) kann durch das Pascalsche Dreieck visualisiert werden. Das Pascalsche Dreieck ist ein dreieckiges Schema, in dem jede Zahl die Summe der beiden dar\u00fcberliegenden Zahlen ist.</p> <p>Aufbau des Pascalschen Dreiecks</p> <p>Jede Zeile des Dreiecks entspricht den Koeffizienten der Binomialverteilung f\u00fcr ein gegebenes \\(n\\):</p> <ul> <li>Die 0. Zeile entspricht \\(n = 0\\): \\(\\binom{0}{0} = 1\\)</li> <li>Die 1. Zeile entspricht \\(n = 1\\): \\(\\binom{1}{0} = 1, \\binom{1}{1} = 1\\)</li> <li>Die 2. Zeile entspricht \\(n = 2\\): \\(\\binom{2}{0} = 1, \\binom{2}{1} = 2, \\binom{2}{2} = 1\\)</li> </ul> <p>Die Position einer Zahl im Pascalschen Dreieck wird durch \\(n\\) und \\(k\\) bestimmt: - \\(n\\): Zeilennummer (beginnend bei 0) - \\(k\\): Spaltennummer (beginnend bei 0)</p> <p></p> <p>Berechnung von \\(\\binom{3}{2}\\)</p> <p>Um den Wert von \\(\\binom{3}{2}\\) zu finden:</p> <ul> <li>Gehe zur 3. Zeile (beginnend bei 0).</li> <li>W\u00e4hle die 2. Spalte (beginnend bei 0).</li> <li>Der Wert ist 3.</li> </ul> <p></p> <p>Beispielrechnung f\u00fcr Basketballspiel</p> <p>Ein Basketballspieler trifft mit Wahrscheinlichkeit \\(p = 0.7\\). Er wirft \\(n = 10\\) Mal. Wie hoch ist die Wahrscheinlichkeit, genau 7 Treffer zu erzielen?</p> <ul> <li>Binomialkoeffizient:</li> </ul> \\[\\binom{10}{7} = \\frac{10!}{7!(10-7)!} = \\frac{10!}{7!3!} = 120\\] <ul> <li>Berechnung der Wahrscheinlichkeit:</li> </ul> \\[P(X = 7) = 120 \\cdot 0.7^7 \\cdot 0.3^3\\] \\[= 120 \\cdot 0.0823543 \\cdot 0.027\\] \\[\\approx 0.2668\\] <p>Die Wahrscheinlichkeit, genau 7 Treffer zu erzielen, betr\u00e4gt 26.68%.</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#pascalsches-dreieck-fur-n-10","title":"Pascalsches Dreieck f\u00fcr \\(n = 10\\)","text":"<pre><code>                             1\n                          1     1\n                       1     2     1\n                    1     3     3     1\n                 1     4     6     4     1\n              1     5    10    10     5     1\n           1     6    15    20    15     6     1\n        1     7    21    35    35    21     7     1\n     1     8    28    56    70    56    28     8     1\n  1     9    36    84   126   126    84    36     9     1\n1   10    45   120   210   252   210   120    45    10    1\n</code></pre> <p>Hier kann der Wert \\(\\binom{10}{7} = 120\\) direkt in der Zeile \\(n = 10\\), Spalte \\(k = 7\\) abgelesen werden.</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#rechenaufgaben-zur-binomialverteilung","title":"Rechenaufgaben zur Binomialverteilung","text":"<p>Fertigungsprozess-Fehleranalyse</p> <p>In einem Fertigungsprozess hat ein Produkt eine Fehlerwahrscheinlichkeit von \\(p = 0.05\\). Eine Stichprobe von \\(n = 20\\) Produkten wird untersucht. Berechne:</p> <ol> <li>Die Wahrscheinlichkeit, dass genau 2 Produkte fehlerhaft sind.</li> <li>Die Wahrscheinlichkeit, dass h\u00f6chstens 1 Produkt fehlerhaft ist.</li> </ol> L\u00f6sung <p>Schritt 1: Berechnung f\u00fcr genau 2 fehlerhafte Produkte (\\(X = 2\\)):</p> <p>Formel der Binomialverteilung anwenden:</p> \\[ P(X = 2) = \\binom{20}{2} \\cdot (0.05)^2 \\cdot (0.95)^{18} \\] <ol> <li> <p>Binomialkoeffizient berechnen:    $$ \\binom{20}{2} = \\frac{20!}{2! \\cdot (20-2)!} = \\frac{20 \\cdot 19}{2} = 190 $$</p> </li> <li> <p>Wahrscheinlichkeit berechnen:    $$ P(X = 2) = 190 \\cdot (0.05)^2 \\cdot (0.95)^{18} $$    $$ = 190 \\cdot 0.0025 \\cdot 0.4228 $$    $$ \\approx 0.201 $$</p> </li> </ol> <p>Die Wahrscheinlichkeit, dass genau 2 Produkte fehlerhaft sind, betr\u00e4gt 20.1%.</p> <p>Schritt 2: Wahrscheinlichkeit f\u00fcr h\u00f6chstens 1 fehlerhaftes Produkt (\\(P(X \\leq 1)\\)):</p> \\[ P(X \\leq 1) = P(X = 0) + P(X = 1) \\] <ol> <li> <p>Wahrscheinlichkeit f\u00fcr \\(X = 0\\):    $$ P(X = 0) = \\binom{20}{0} \\cdot (0.05)^0 \\cdot (0.95)^20 $$    $$ = 1 \\cdot 1 \\cdot 0.3585 \\approx 0.3585 $$</p> </li> <li> <p>Wahrscheinlichkeit f\u00fcr \\(X = 1\\):    $$ P(X = 1) = \\binom{20}{1} \\cdot (0.05)^1 \\cdot (0.95)^19 $$    $$ = 20 \\cdot 0.05 \\cdot 0.3761 $$    $$ \\approx 0.3761 $$</p> </li> <li> <p>Addieren der Wahrscheinlichkeiten:    $$ P(X \\leq 1) = 0.3585 + 0.3761 = 0.7346 $$</p> </li> </ol> <p>Die Wahrscheinlichkeit, dass h\u00f6chstens 1 Produkt fehlerhaft ist, betr\u00e4gt 73.46%.</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#aufgabe-2-umfrageergebnisse","title":"Aufgabe 2: Umfrageergebnisse","text":"Umfrage zur Produktzufriedenheit <p>In einer Umfrage geben 70% der Teilnehmer an, dass sie mit einem neuen Produkt zufrieden sind. Aus einer Stichprobe von 15 Teilnehmern wird gefragt:</p> <ol> <li>Wie hoch ist die Wahrscheinlichkeit, dass genau 10 Personen zufrieden sind?</li> <li>Wie hoch ist die Wahrscheinlichkeit, dass mindestens 12 Personen zufrieden sind?</li> </ol> L\u00f6sung <p>Schritt 1: Wahrscheinlichkeit f\u00fcr genau 10 zufriedene Personen (\\(X = 10\\)):</p> <p>Formel anwenden:</p> \\[P(X = 10) = \\binom{15}{10} \\cdot (0.7)^{10} \\cdot (0.3)^5\\] <ul> <li>Binomialkoeffizient berechnen:</li> </ul> \\[ \\binom{15}{10} = \\frac{15!}{10! \\cdot (15-10)!} = \\frac{15 \\cdot 14 \\cdot 13 \\cdot 12 \\cdot 11}{5 \\cdot 4 \\cdot 3 \\cdot 2 \\cdot 1} = 3003 \\] <ul> <li>Wahrscheinlichkeit berechnen:</li> </ul> \\[ P(X = 10) = 3003 \\cdot (0.7)^{10} \\cdot (0.3)^5 \\] \\[ \\approx 3003 \\cdot 0.0282 \\cdot 0.00243 \\] \\[ \\approx 0.205 \\] <p>Die Wahrscheinlichkeit, dass genau 10 Personen zufrieden sind, betr\u00e4gt 20.5%.</p> <p>Schritt 2: Wahrscheinlichkeit f\u00fcr mindestens 12 zufriedene Personen (\\(P(X \\geq 12)\\)):</p> \\[ P(X \\geq 12) = P(X = 12) + P(X = 13) + P(X = 14) + P(X = 15) \\] <p>F\u00fcr jeden Wert von \\(X\\) wird die Wahrscheinlichkeit einzeln berechnet und anschlie\u00dfend addiert (siehe Formel der Binomialverteilung).</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#multiple-choice-fragen","title":"Multiple-Choice-Fragen","text":"MC-Frage 1 <p>Welche der folgenden Situationen l\u00e4sst sich mit einer Binomialverteilung modellieren?</p> <p>A) Die Anzahl der Minuten, die ein Kunde in einem Gesch\u00e4ft verbringt.</p> <p>B) Die Anzahl der Treffer bei 10 geworfenen Freiw\u00fcrfen im Basketball.</p> <p>C) Die Anzahl defekter Teile in einer Lieferung von 50 Bauteilen bei einer Fehlerwahrscheinlichkeit von 1%.</p> L\u00f6sung <p>B, C</p> MC-Frage 2 <p>Was ist der Binomialkoeffizient \\(\\binom{5}{2}\\)?</p> <p>A) 6</p> <p>B) 10</p> <p>C) 15</p> L\u00f6sung <p>B</p> MC-Frage 2 <p>Welche Aussage ist korrekt?         </p> <p>A) Die Binomialverteilung erfordert unabh\u00e4ngige Versuche.</p> <p>B) Die Erfolgswahrscheinlichkeit \\(p\\) darf sich \u00e4ndern.</p> <p>C) Die Binomialverteilung wird immer durch das Pascalsche Dreieck berechnet.</p> L\u00f6sung <p>A</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#quiz-zum-thema-binomialverteilung","title":"Quiz zum Thema Binomialverteilung","text":"Frage 1 <p>Was beschreibt die Binomialverteilung?</p> L\u00f6sung <p>Die Anzahl der Erfolge in einer festen Anzahl von unabh\u00e4ngigen Versuchen mit Erfolgswahrscheinlichkeit \\(p\\).</p> Frage 2 <p>Wie berechnet man den Binomialkoeffizienten \\(\\binom{n}{k}\\)?</p> L\u00f6sung <p>\\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\)</p> Frage 3 <p>Was ist die kumulierte Binomialverteilung?</p> L\u00f6sung <p>Die Wahrscheinlichkeit, dass \\(X \\leq k\\).</p> Frage 4 <p>Wie berechnet man den Erwartungswert einer Binomialverteilung?</p> L\u00f6sung <p>\\(E(X) = n \\cdot p\\).</p> Frage 5 <p>Was ist der Unterschied zwischen der Wahrscheinlichkeitsfunktion und der Verteilungsfunktion?</p> L\u00f6sung <p>Die Wahrscheinlichkeitsfunktion gibt die Wahrscheinlichkeit f\u00fcr einen bestimmten Wert an, die Verteilungsfunktion summiert bis zu einem bestimmten Wert auf.</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_4_Binomialverteilung/#programmieraufgabe-mit-losung","title":"Programmieraufgabe mit L\u00f6sung","text":"Programmieraufgabe zur Binomialverteilung <p>Berechne die Wahrscheinlichkeit, dass bei 50 W\u00fcrfen mit einem fairen W\u00fcrfel genau 8-mal die Zahl 6 gew\u00fcrfelt wird.</p> L\u00f6sung <p>L\u00f6sung nur mit Standardmodul</p> <pre><code>import math\n\n# Parameter definieren\nn = 50  # Anzahl der Versuche\np = 1/6  # Wahrscheinlichkeit f\u00fcr eine 6\nk = 8  # Anzahl der Erfolge\n\n# Funktion zur Berechnung des Binomialkoeffizienten\ndef binomialkoeffizient(n, k):\n    return math.factorial(n) // (math.factorial(k) * math.factorial(n - k))\n\n# Binomialwahrscheinlichkeit berechnen\ndef binomial_wahrscheinlichkeit(n, k, p):\n    binom = binomialkoeffizient(n, k)\n    return binom * (p ** k) * ((1 - p) ** (n - k))\n\n# Wahrscheinlichkeit f\u00fcr genau 8 Erfolge berechnen\nwahrscheinlichkeit = binomial_wahrscheinlichkeit(n, k, p)\nprint(f\"Die Wahrscheinlichkeit, genau 8-mal eine 6 zu w\u00fcrfeln: {wahrscheinlichkeit:.4f}\")\n</code></pre> <p>Ausgabe: <pre><code>Die Wahrscheinlichkeit, genau 8-mal eine 6 zu w\u00fcrfeln, betr\u00e4gt ca. 0.114\n</code></pre></p> <p>L\u00f6sung mit Verwendung von <code>scipy</code></p> <pre><code>from scipy.stats import binom\n\n# Parameter definieren\nn = 50  # Anzahl der Versuche\np = 1/6  # Wahrscheinlichkeit f\u00fcr eine 6\nk = 8  # Anzahl der Erfolge\n\n# Wahrscheinlichkeit berechnen\nwahrscheinlichkeit = binom.pmf(k, n, p)\nprint(f\"Die Wahrscheinlichkeit, genau 8-mal eine 6 zu w\u00fcrfeln: {wahrscheinlichkeit:.4f}\")\n</code></pre> <p>Ausgabe: <pre><code>Die Wahrscheinlichkeit, genau 8-mal eine 6 zu w\u00fcrfeln, betr\u00e4gt ca. 0.114\n</code></pre></p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_5_Geometrische_Verteilung/","title":"Geometrische Verteilung","text":"<p>Die geometrische Verteilung ist eine diskrete Wahrscheinlichkeitsverteilung, die modelliert, wie viele Bernoulli-Versuche ben\u00f6tigt werden, um einen Erfolg zu erzielen. Ein Bernoulli-Versuch ist ein Experiment mit zwei m\u00f6glichen Ergebnissen: Erfolg (mit Wahrscheinlichkeit \\(p\\)) oder Misserfolg (mit Wahrscheinlichkeit \\(1-p\\)).</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_5_Geometrische_Verteilung/#definition","title":"Definition","text":"<p>Die geometrische Verteilung gibt die Wahrscheinlichkeit an, dass der erste Erfolg im \\(k\\)-ten Versuch eintritt. Die Wahrscheinlichkeit wird wie folgt berechnet:</p> <p>Formel</p> \\[ f(k|p) = P(X = k) = (1-p)^{k-1} \\cdot p \\] \\[ f_g(x|\\Theta) = \\begin{cases}                 (1-\\Theta)^{x-1} \\cdot \\Theta &amp; \\text{f\u00fcr } x \\in \\mathbb{N} \\\\                 0 &amp; \\text{f\u00fcr } sonst.             \\end{cases} \\] <p>Hierbei gilt:</p> <ul> <li>\\(X\\): Anzahl der Versuche bis zum ersten Erfolg (eine Zufallsvariable).</li> <li>\\(k\\): Anzahl der ben\u00f6tigten Versuche (\\(k = 1, 2, 3, \\dots\\)).</li> <li>\\(p\\): Wahrscheinlichkeit eines Erfolgs bei einem einzelnen Versuch (\\(0 &lt; p \\leq 1\\)).</li> </ul>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_5_Geometrische_Verteilung/#eigenschaften","title":"Eigenschaften","text":"<p>1. Erwartungswert (Mittelwert):</p> \\[ \\mathbb{E}[X] = \\frac{1}{p} \\] <p>2. Varianz:</p> \\[ \\text{Var}(X) = \\frac{1-p}{p^2} \\] <p>3. Ged\u00e4chtnislosigkeit:</p> <p>Die geometrische Verteilung ist ged\u00e4chtnislos. Das bedeutet, dass die Wahrscheinlichkeit f\u00fcr einen Erfolg unabh\u00e4ngig davon ist, wie viele Misserfolge bereits eingetreten sind:</p> \\[ P(X &gt; n + k \\mid X &gt; n) = P(X &gt; k) \\] <p>Beispiel</p> <p>Angenommen, die Wahrscheinlichkeit eines Erfolgs bei einem einzelnen Versuch betr\u00e4gt \\(p = 0,125\\) (12.5 %). Die Wahrscheinlichkeit, dass der erste Erfolg im dritten Versuch eintritt, ist:</p> \\[ P(X = 3) = (1-0,125)^{3-1} \\cdot 0,125 = 0,875^2 \\cdot 0,125 = 0.095703125 \\] <p> </p> <p>Note</p> <p>Die geometrische Verteilung wird h\u00e4ufig in Szenarien verwendet, bei denen ein Ereignis durch wiederholte Versuche erzielt werden soll, wie z. B.:</p> <ul> <li>Analyse von wiederholten Versuchen in Gl\u00fccksspielen</li> <li>Bestimmung sich wiederholdender gleich bleibender Ereignisse bis zum eintreffen des ersten Erfolgs</li> </ul>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_5_Geometrische_Verteilung/#fragen","title":"Fragen","text":""},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_5_Geometrische_Verteilung/#multiple-choice-aufgabenkopfrechnen","title":"Multiple Choice Aufgaben(Kopfrechnen)","text":"MC-Frage 1 <p>Was beschreibt die geometrische Verteilung?</p> <p>A) Die Anzahl der Erfolge bis zum ersten Misserfolg.</p> <p>B) Die Anzahl der Erfolge in einer festen Anzahl von Versuchen.</p> <p>C) Die Anzahl der Misserfolge bis zum ersten Erfolg.</p> <p>D) Die Anzahl der Misserfolge in einer festen Anzahl von Versuchen.</p> L\u00f6sung <p>C</p> MC-Frage 2 <p>Welche der folgenden Formeln beschreibt die Wahrscheinlichkeit \\( P(X = k) \\) f\u00fcr eine geometrische Verteilung mit Erfolgswahrscheinlichkeit \\( p \\)?</p> <p>A) \\( P(X = k) = (1-p)^{k-1} p \\)</p> <p>B) \\( P(X = k) = p^k (1-p) \\)</p> <p>C) \\( P(X = k) = (1-p)^k p \\)</p> <p>D) \\( P(X = k) = p (1-p)^{k-1} \\)</p> L\u00f6sung <p>A</p> MC-Frage 3 <p>Gegeben ist eine geometrische Verteilung mit Erfolgswahrscheinlichkeit \\( p = 0.5 \\). Berechne die Wahrscheinlichkeit, dass der erste Erfolg im 2. Versuch eintritt. A) 0.25</p> <p>B) 0.5</p> <p>C) 0.75</p> <p>D) 0.125</p> L\u00f6sung <p>A</p> MC-Frage 4 <p>Berechne den Erwartungswert \\( E(X) \\) einer geometrischen Verteilung mit Erfolgswahrscheinlichkeit \\( p = 0.2 \\). A) 4</p> <p>B) 5</p> <p>C) 6</p> <p>D) 7</p> L\u00f6sung <p>B</p> MC-Frage 5 <p>Eine geometrische Verteilung hat eine Erfolgswahrscheinlichkeit von \\( p = 0.2 \\). Was ist die Wahrscheinlichkeit, dass der erste Erfolg im 3. Versuch eintritt?</p> <p>A) 0.140625</p> <p>B) 0.1875</p> <p>C) 0.128</p> <p>D) 0.421875</p> L\u00f6sung <p>C</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_5_Geometrische_Verteilung/#freitext-fragen","title":"Freitext Fragen","text":"Freitext-Frage 1 <ol> <li>Ist die Geometrische Verteilung diskret oder kontinuierlich?</li> </ol> L\u00f6sung <p>Diskret.</p> Freitext-Frage 2 <ol> <li>Wie ver\u00e4ndert sich die Erfolgswahrscheinlichkeit im Verlauf der Versuche?</li> </ol> L\u00f6sung <p>Sie bleibt konstant.</p> Freitext-Frage 3 <ol> <li>Wie lautet die Formel f\u00fcr den Erwartungswert der GV?</li> </ol> L\u00f6sung \\[E(X) = \frac{1}{p}\\] Freitext-Frage 4 <ol> <li>Welche Schwierigkeiten k\u00f6nnten in einer realen Anwendung auftreten, wenn die Annahmen der geometrischen Verteilung verletzt werden?</li> </ol> L\u00f6sung <p>Variierende Erfolgswahrscheinlichkeiten.</p> Freitext-Frage 5 <ol> <li>Wie lautet die Wahrscheinlichkeitsfunktion der GV?</li> </ol> L\u00f6sung \\[ f_g(x|\\Theta) =  \\begin{cases} (1-\\Theta)^{x-1} \\cdot \\Theta &amp; \\text{f\u00fcr } x \\in \\mathbb{N} \\\\ 0 &amp; \\text{f\u00fcr } sonst. \\end{cases} \\] Freitext-Frage 6 <ol> <li>Wie lautet die Formel der Verteilungsfunktion der GV?</li> </ol> L\u00f6sung \\[     F_g(x|\\Theta) = \\begin{cases}                     0 &amp; \\text{f\u00fcr } x&lt;1 \\\\                     1-(1-\\Theta)^x &amp; \\text{f\u00fcr } x \\in \\mathbb{N}                 \\end{cases}     \\]"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_5_Geometrische_Verteilung/#rechenfragen-python-ist-benotigt","title":"Rechenfragen (Python ist ben\u00f6tigt)","text":"Rechenfragen 1 <p>Berechne die Varianz eines gleichm\u00e4\u00dfig verteilten 20-seitigen W\u00fcrfels.</p> L\u00f6sung <p>380</p> Rechenfragen 2 <p>Lottospiel: Du m\u00f6chtest endlich reich werden und beschlie\u00dft, ab sofort Lotto zu spielen. Dabei nimmst du dir vor, aufzuh\u00f6ren, sobald du gewonnen hast. Auf dem Lottoschein steht, dass die Wahrscheinlichkeit f\u00fcr einen Gewinn bei 1:1000 liegt. Wie oft musst du spielen, um mit mindestens 50 prozentiger Wahrscheinlichkeit einmal gewonnen zu haben?</p> L\u00f6sung <p>693</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_6_Hypergeometrische_Verteilung/","title":"Hypergeometrische Verteilung","text":""},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_6_Hypergeometrische_Verteilung/#1-was-ist-das-fur-eine-verteilung-und-wozu-dient-sie","title":"1. Was ist das f\u00fcr eine Verteilung und wozu dient sie?","text":"<ul> <li>\u00c4hnelt der Binomialverteilung, jedoch mit Stichprobenziehungen OHNE Zur\u00fccklegen</li> <li>Wird verwendet, wenn die Wahrscheinlichkeit f\u00fcr einen Treffer oder Nicht-Treffer von der vorherigen Ziehung abh\u00e4ngt, da die Gesamtpopulation endlich ist und sich mit jeder Ziehung reduziert</li> </ul>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_6_Hypergeometrische_Verteilung/#2-herleitung-anhand-eines-beispiels","title":"2. Herleitung anhand eines Beispiels","text":"<p>Beispiel</p> <p>Formeln der hypergeometrischen Verteilung</p> <p>Wahrscheinlichkeitsfunktion:</p> <pre><code>$$\n\\begin{equation}\nf_h(x \\mid N; M; n) = P(X = x) = \\begin{cases}\n\\frac{\\binom{M}{x} \\cdot \\binom{N - M}{n - x}}{\\binom{N}{n}} &amp; \\text{f\u00fcr } x = 0, 1, 2, \\dots, n \\\\\n0 &amp; \\text{sonst}\n\\end{cases}\n\\end{equation}\n$$\n\nmit\n\n- **X**: Die Zufallsvariable der interessierenden Eigenschaft\n- **N**: Zahl aller Elemente in der Gesamtheit\n- **M**: Zahl der Elemente (von der Gesamtheit), die die interessierende Eigenschaft tragen\n- **N - M**: Zahl der Elemente (von der Gesamtheit), die **nicht** die interessierende Eigenschaft tragen\n- **n**: Zahl der Elemente, die gew\u00e4hlt werden\n- **x**: Zahl der Elemente mit der interessierenden Eigenschaft in dieser Auswahl\n</code></pre> <p>Zusammengefasst f\u00fcr das Beispiel</p> <p>Anzahl der M\u00f6glichkeiten, 1 rote Kugel zu ziehen:</p> <p>W\u00e4hle genau 1 rote Kugel aus den M = 2 roten Kugeln:</p> \\[  \\begin{equation} \\binom{M}{x} = \\binom{2}{1} = 2 \\end{equation} \\] <p>Anzahl der M\u00f6glichkeiten, die restlichen 2 blauen Kugeln zu ziehen:</p> <p>W\u00e4hle 2 blaue Kugeln aus den N - M = 4 blauen Kugeln:</p> \\[  \\begin{equation} \\binom{N - M}{n - x} = \\binom{4}{2} = 6  \\end{equation} \\] <p>Gesamtanzahl der m\u00f6glichen Ziehungen von 3 Kugeln aus 6:</p> <p>Ziehe 3 Kugeln aus den N = 6 Kugeln:</p> \\[ \\begin{equation} \\binom{N}{n} = \\binom{6}{3} = 20 \\end{equation} \\] <p>Setze die Werte in die Formel ein:</p> \\[ \\begin{equation} P(X = 0) = \\frac{\\binom{2}{0} \\cdot \\binom{4}{3}}{\\binom{6}{3}} = \\frac{1 \\cdot 4}{20} = \\frac{4}{20} = 0.2 \\end{equation} \\] \\[ \\begin{equation} P(X = 1) = \\frac{\\binom{2}{1} \\cdot \\binom{4}{2}}{\\binom{6}{3}} = \\frac{2 \\cdot 6}{20} = \\frac{12}{20} = 0.6 \\end{equation} \\] \\[ \\begin{equation} P(X = 2) = \\frac{\\binom{2}{2} \\cdot \\binom{4}{1}}{\\binom{6}{3}} = \\frac{1 \\cdot 4}{20} = \\frac{4}{20} = 0.2 \\end{equation} \\] Aufgabe <p>Was ist die Wahrscheinlichkeit f\u00fcr \\(P(X = 3)\\)?</p> <p>\\(x\\) kann dabei natu\u0308rlich h\u00f6chstens so gro\u00df sein wie der kleinere der beiden Werte  \\(\\min(n, M)\\). </p> <p></p> <p>Verteilungsfunktion</p> \\[ \\begin{equation} F_h(x \\mid N; M; n) = P(X \\leq x) =  \\begin{cases} 0 &amp; \\text{f\u00fcr } x &lt; 0, \\\\ \\sum_{k=0}^{x} \\frac{\\binom{M}{k} \\cdot \\binom{N - M}{n - k}}{\\binom{N}{n}} &amp; \\text{f\u00fcr } x = 0, 1, 2, \\dots, n\\\\ 1 &amp; \\text{f\u00fcr } x &gt; n. \\end{cases} \\end{equation} \\] <p></p> <p>Formel</p> <p>F\u00fcr die hypergeometrisch verteilte Zufallsvariable \\(X_h\\) ist der Erwartungswert: </p> \\[ E[X_h] = n \\cdot \\frac{M}{N} \\] <p>Formel</p> <p>F\u00fcr die hypergeometrisch verteilte Zufallsvariable \\(X_h\\) ist der Erwartungswert: </p> \\[ var(X_h)= E[(X_h - E[X_h])^2] = n \\cdot \\frac{M}{N} \\cdot \\left(1-\\frac{M}{N}\\right) \\cdot \\frac{N-n}{N-1}\\]"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_6_Hypergeometrische_Verteilung/#gegeben","title":"Gegeben:","text":"<p>Insgesamt 6 Kugeln in der Urne.</p> <ul> <li>Es gibt 2 rote Kugeln.</li> <li>Es gibt 4 blaue Kugeln.</li> </ul>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_6_Hypergeometrische_Verteilung/#aufgabe","title":"Aufgabe:","text":"<p>Es werden 3 Kugeln ohne Zur\u00fccklegen gezogen. Dabei soll genau 1 rote Kugel gezogen werden. Wie gro\u00df ist die Wahrscheinlichkeit?</p> <p></p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_6_Hypergeometrische_Verteilung/#ansatz","title":"Ansatz","text":"<p>Formel</p> \\[  \\begin{align}     P(\\text{x rote Kugel ziehen}) &amp;= \\frac{\\text{Anzahl der zutreffenden Kombinationen}}{\\text{Anzahl aller Kombinationen }}  \\\\&amp; =\\frac{\\text{(Kombinationen f\u00fcr x rote Kugeln)}  \\cdot \\text{(Kombinationen f\u00fcr $n-x$ nicht-rote (blaue) Kugeln)}}{\\text{ Kombinationen f\u00fcr n Kugeln}} \\end{align} \\]"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_6_Hypergeometrische_Verteilung/#berechnung","title":"Berechnung:","text":""},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_6_Hypergeometrische_Verteilung/#1-anzahl-aller-kombinationen","title":"1. Anzahl aller Kombinationen","text":"<p>Erinnere daran, wie du die Anzahl der Kombinationen berechnest, aus k Objekte aus einer Menge von n Objekten ausw\u00e4hlen kannst. (ohne Zur\u00fccklegen, ohne Beachtung der Reihenfolge) \\(\\rightarrow\\) Formel des Binomialkoeffizienten: (siehe F.1.2.8, Seite 107 im Buch) </p> \\[ \\binom{n}{k} = \\frac{n!}{k!(n-k)!} \\] <p>Daraus ergeben sich insgesamt</p> \\[\\binom{6}{3} =  \\frac{6!}{3!(6-3)!} = 20 \\] <p>M\u00f6glichkeiten, 3 Kugeln zu ziehen.</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_6_Hypergeometrische_Verteilung/#2-anzahl-aller-zutreffenden-kombinationen","title":"2. Anzahl aller zutreffenden Kombinationen","text":"<p>Nun zeigen wir alle m\u00f6glichen Kombinationen der 3 gezogenen Kugeln mit genau 1 roter Kugel.</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_6_Hypergeometrische_Verteilung/#tabellarisch","title":"Tabellarisch:","text":"Kombinations-Nr. Rote Kugeln (1 von 2) Blaue Kugeln (2 von 4) 1 {R1} {B1, B2} 2 {R1} {B1, B3} 3 {R1} {B1, B4} 4 {R1} {B2, B3} 5 {R1} {B2, B4} 6 {R1} {B3, B4} 7 {R2} {B1, B2} 8 {R2} {B1, B3} 9 {R2} {B1, B4} 10 {R2} {B2, B3} 11 {R2} {B2, B4} 12 {R2} {B3, B4} <p>Anzahl der zutreffenden Kombinationen: 12</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_6_Hypergeometrische_Verteilung/#oder-anzahl-der-zutreffenden-kombinationen-rechnerisch-dargestellt","title":"Oder Anzahl der zutreffenden Kombinationen rechnerisch dargestellt:","text":""},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_6_Hypergeometrische_Verteilung/#1-es-gibt-2-kombinationen-fur-die-1-rote-kugel-r1-oder-r2","title":"1. Es gibt 2 Kombinationen f\u00fcr die 1 rote Kugel (R1 oder R2):","text":"\\[\\binom{2}{1} =  \\frac{2!}{1!(2-1)!} = 2 \\]"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_6_Hypergeometrische_Verteilung/#2-es-gibt-6-kombinationen-fur-die-2-blauen-kugeln-b1-b2-b3-b4","title":"2. Es gibt 6 Kombinationen f\u00fcr die 2 blauen Kugeln (B1, B2, B3, B4):","text":"\\[ \\binom{4}{2} = 6 \\] <p>Diese Anzahl entspricht der Berechnung: </p> \\[ \\text{(Kombinationen f\u00fcr x rote Kugeln)} \\cdot \\text{ (Kombinationen f\u00fcr $n-x$ nicht-rote (blaue) Kugeln)} = 2 \\cdot 6 = 12 \\]"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_6_Hypergeometrische_Verteilung/#3-wahrscheinlich-berechnen","title":"3. Wahrscheinlich berechnen:","text":"\\[ P(\\text{Eine rote Kugel ziehen}) = \\frac{\\text{Anzahl der zutreffenden Kombinationen}}{\\text{Anzahl aller Kombinationen }} = \\frac{12}{20} \\] <p>Oder allgemeiner ausgedr\u00fcckt:</p> \\[ \\begin{align*} P(X = x) &amp;= \\frac{\\binom{M}{x} \\cdot \\binom{N - M}{n - x}}{\\binom{N}{n}} \\end{align*} \\] <p>mit</p> <ul> <li>X: Die Zufallsvariable f\u00fcr gezogene rote Kugeln</li> <li>N = 6: Insgesamt 6 Kugeln in der Urne.</li> <li>M= 2: Es gibt 2 rote Kugeln.</li> <li>N - M = 4: Es gibt 4 blaue Kugeln.</li> <li>n = 3: Es werden 3 Kugeln ohne Zur\u00fccklegen gezogen.</li> <li>x = 1: Es sollen genau 1 rote Kugel gezogen werden.</li> </ul>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_6_Hypergeometrische_Verteilung/#fragen","title":"Fragen","text":"Freitext Frage 1 <p>Wie unterscheidet sich die hypergeometrische Verteilung von der Binomialverteilung?</p> L\u00f6sung <p>Hypergeometrische Verteilung: Ziehen ohne Zur\u00fccklegen, die Wahrscheinlichkeiten \u00e4ndern sich nach jeder Ziehung.</p> <p>Binomialverteilung: Ziehen mit Zur\u00fccklegen, die Wahrscheinlichkeiten bleiben konstant.</p> Freitext Frage 2 <p>Was bedeuten die Parameter \\(N\\), \\(M\\), \\(n\\) und \\(x\\) in der hypergeometrischen Verteilung \\(f_h(x \\mid N; M; n)\\)?</p> L\u00f6sung <p>\\(N\\): Gesamtanzahl der Elemente in der Grundgesamtheit.</p> <p>\\(M\\): Anzahl der Erfolgsf\u00e4lle in der Grundgesamtheit.</p> <p>\\(n\\): Anzahl der gezogenen Elemente (Stichprobengr\u00f6\u00dfe).</p> <p>\\(x\\): Anzahl der Erfolgsf\u00e4lle in der Stichprobe.</p> Freitext Frage 3 <p>In welchen realen Szenarien findet die hypergeometrische Verteilung Anwendung?</p> L\u00f6sung <p>Ziehen von Karten ohne Zur\u00fccklegen.</p> <p>Qualit\u00e4tskontrolle, bei der Bauteile auf Fehler untersucht werden.</p> <p>Auswahl von Teams oder Gruppen mit bestimmten Merkmalen (z. B. M\u00e4nner/Frauen).</p> <p>Gewinnchancen bei Losen oder Tombolas.</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_7_Poisson_Verteilung/","title":"Poisson-Verteilung","text":"<p>Die Poisson-Verteilung beschreibt die Wahrscheinlichkeit daf\u00fcr, dass ein Ereignis \\(k\\)-mal in einem festen Zeitraum auftritt oder Raum, wenn das Ereignis zuf\u00e4llig und mit konstanter durchschnittlicher Rate (\\(\\lambda\\)) (Erwartungswert) passiert.</p> <p>Anwendungsbereiche:</p> <ul> <li>Wird verwendet, wenn Ereignisse unabh\u00e4ngig voneinander auftreten und eine durchschnittliche Rate (\\(\\lambda\\)) bekannt ist.</li> <li>Es gibt keine feste Anzahl von Versuchen (wie bei der Binomialverteilung).</li> <li>H\u00e4ufig bei seltenen Ereignissen pro Zeiteinheit oder pro Bereich.</li> <li>Wenn \ud835\udc5b \u2192 \u221e (sehr viele Versuche) und \u0398\u21920 (die Wahrscheinlichkeit f\u00fcr einen einzelnen Erfolg wird sehr klein) n\u00e4hert sich die Binomialverteilung der Poisson-Verteilung mit Parameter \u03bb.</li> </ul> <p>Poisson-Verteilung</p> <p>\\(\\lambda\\): Durchschnittliche Anzahl von Ereignissen pro Zeiteinheit oder pro Bezugsgr\u00f6\u00dfe.</p> <p>Wahrscheinlichkeitsfunktion:</p> \\[P_p(x|\\lambda) = \\frac{\\lambda^x}{x!} e^{-\\lambda}\\] <p>Verteilungsfunktion:</p> \\[F_p(x|\\lambda) = e^{-\\lambda} \\cdot \\sum \\limits_{k = 0} \\limits^{x} \\dfrac{\\lambda^k}{k!}\\] <p>Visualisierung der Verteilung</p> <p>Wahrscheinlichkeitsfunktionen:</p> <p></p> <p>Verteilungsfunktionen:</p> <p></p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_7_Poisson_Verteilung/#vergleich-zwischen-binomialverteilung-und-poisson-verteilung","title":"Vergleich zwischen Binomialverteilung und Poisson-Verteilung","text":"Kriterium Binomialverteilung Poisson-Verteilung Ereignisse pro Versuch Entweder Erfolg oder Misserfolg Mehrere Ereignisse in einem Zeitraum/Bezugsraum m\u00f6glich Anzahl der Versuche Fixe Anzahl von \\(n\\) Versuchen Variabel oder unendlich gro\u00df, Ereignisse treten zuf\u00e4llig auf Parameter \\(n\\) und \\(p\\) \\(\\lambda\\) (Durchschnittsrate der Ereignisse) Typische Anwendung Treffer oder Fehler in \\(n\\) unabh\u00e4ngigen Versuchen Auftreten von Ereignissen in Zeit oder Raum N\u00e4he zur Poisson Kann bei \\(n \\to \\infty\\), \\(p \\to 0\\), und \\(np = \\lambda\\) in die Poisson-Verteilung \u00fcbergehen Ist eine N\u00e4herung zur Binomialverteilung unter oben genannten Bedingungen"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_7_Poisson_Verteilung/#herleitung-poisson-verteilung","title":"Herleitung Poisson-Verteilung","text":"\\[f_p(x|\\lambda) =\\dfrac{\\lambda^x}{x!}\\cdot e^{-\\lambda}\\] <p>\\(f_b(x|\\lambda) = \\text{Wahrscheinlichkeit, dass das Ereignis x-mal innerhalb der Bezugsgr\u00f6\u00dfe auftritt.}\\)</p> <p>\\(\\lambda = \\text{H\u00e4ufigkeit, in der durchschnittlich die Ereignisse zur stetigen Bezugsgr\u00f6\u00dfe passieren.}\\)</p> <p>\\(x = \\text{Anzahl der Ereignisse innerhalb der Bezugsgr\u00f6\u00dfe.}\\)</p> <p>\\(x \\in \\mathbb{N}_{0}\\)</p> <p>\\(e \\approx 2.71828 \\cdots\\) (Das ist die Euler'sche Zahl!)</p> <p>Beispiel</p> <p>Eine Fertigungsstra\u00dfe produziert durchschnittlich 6 fehlerhafte Bauteile pro Stunde.</p> <p>\\(\\lambda_{Stunde} = 6\\) </p> <p>\\(\\lambda_{Minute} = \\dfrac{6}{60}=0.1\\)</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_7_Poisson_Verteilung/#herleitung","title":"Herleitung","text":"<p>Die Poisson-Verteilung ist die Erweiterung der Binomial-Verteilung und kann in F\u00e4llen angewendet werden, wo man aus einem diskreten Raum in den stetigen Raum wechselt. In solch einem Fall wird \\(n\\) zu einer beliebig gro\u00dfen Zahl welche, wenn man sich die Funktion f\u00fcr die Binomial-Verteilung anguckt, sehr schnell zum Problem f\u00fchrt.</p> \\[f_b(x|\\Theta; n) = \\binom{n}{x} \\cdot \\Theta^x \\cdot (1 - \\Theta)^{n-x}\\] \\[\\binom{n}{x} = \\dfrac{n!}{x! \\cdot (n-x)!}\\] <p>Wenn wir ein \\(\\lim \\limits_{n \\to \\infty}\\) haben, so w\u00fcrden wir bei nicht in der Lage sein diese Funktion in deren aktuellen Form auszurechnen.</p> <p>In der Poisson-Verteilung wird erstmal festgelegt, dass \\(n \\cdot \\Theta = \\lambda = E_X\\)</p> <p>Wird dies nun umgestellt nach \\(\\Theta\\) erhalten wir \\(\\Theta = \\dfrac{\\lambda}{n}\\)</p> <p>Setzen wir dies nun in die Funktion der Binomial-Verteilung ein erhalten wir</p> \\[\\lim \\limits_{n \\to \\infty} f_b(x|\\tfrac{\\lambda}{n};n) = \\lim \\limits_{n \\to \\infty} \\binom{n}{x} \\cdot (\\tfrac{\\lambda}{n})^x \\cdot (1 - \\tfrac{\\lambda}{n})^{n-x}\\] <p>Als n\u00e4chsten Schritt fokussiert man sich auf den Teil</p> \\[\\lim \\limits_{n \\to \\infty} (1 - \\tfrac{\\lambda}{n})^{n-x}\\] <p>Da \\(x\\) im Vergleich zu \\(n\\) hier vernachl\u00e4ssigbar klein wird, k\u00f6nnen wir den Exponenten so umschreiben, dass wir folgendes bekommen:</p> \\[\\lim \\limits_{n \\to \\infty} (1 - \\tfrac{\\lambda}{n})^n\\] <p>Warum machen wir das aber? Der Grund ist der, dass es eine sehr \u00e4hnlich aussehende Folge gibt die wie folgt aussieht:</p> \\[\\lim \\limits_{n \\to \\infty} (1 + \\tfrac{k}{n})^n = e^k\\] <p>Sieht sehr \u00e4hnlich aus, nicht wahr? Tats\u00e4chlich k\u00f6nnen wir unsere vorherige Funktion umformen, um die \\(e^k\\) Folge-Funktion zu erhalten:</p> \\[\\lim \\limits_{n \\to \\infty} (1 + \\tfrac{-\\lambda}{n})^n\\] <p>Da \\(k = -\\lambda\\) erhalten wir am Ende</p> \\[\\lim \\limits_{n \\to \\infty} (1 + \\tfrac{-\\lambda}{n})^n = e^{-\\lambda}\\] <p>und somit</p> \\[\\lim \\limits_{n \\to \\infty} f_b(x|\\tfrac{\\lambda}{n};n) = \\lim \\limits_{n \\to \\infty} \\binom{n}{x} \\cdot (\\tfrac{\\lambda}{n})^x \\cdot e^{-\\lambda}\\] <p>Als n\u00e4chstes l\u00f6sen wir alle Klammern und den Binomial-Koeffizienten auf</p> \\[\\lim \\limits_{n \\to \\infty} f_b(x|\\tfrac{\\lambda}{n};n) = \\lim \\limits_{n \\to \\infty} \\dfrac{n!}{x! \\cdot (n-x)!} \\cdot \\dfrac{\\lambda^x}{n^x} \\cdot e^{-\\lambda}\\] <p>Tauschen wir nun \\(x!\\) mit \\(n^x\\) gem\u00e4\u00df Kommutativgesetz bekommt man</p> \\[\\lim \\limits_{n \\to \\infty} f_b(x|\\tfrac{\\lambda}{n};n) = \\lim \\limits_{n \\to \\infty} \\dfrac{n!}{n^x \\cdot (n-x)!} \\cdot \\dfrac{\\lambda^x}{x!} \\cdot e^{-\\lambda}\\] <p>Fokussieren wir uns nun auf</p> \\[\\lim \\limits_{n \\to \\infty} \\dfrac{n!}{n^x \\cdot (n-x)!}\\] <p>Da \\(x &lt;&lt; n\\) (d.h. \\(x\\) is vernachl\u00e4ssigbar klein zu \\(n\\)), k\u00f6nnen wir diesen Bruch vereinfachen als</p> \\[\\lim \\limits_{n \\to \\infty} \\dfrac{n!}{n!} = 1\\] <p>Setzen wir dies nun auch in unsere vorherige Funktion ein erhalten wir</p> \\[\\lim \\limits_{n \\to \\infty} f_b(x|\\tfrac{\\lambda}{n};n) = 1 \\cdot \\dfrac{\\lambda^x}{x!} \\cdot e^{-\\lambda}\\] <p>und voila... damit w\u00e4ren wir von der Funktion der diskreten Binomial-Verteilung in die Funktion der stetigen Poisson-Verteilung angekommen.</p> \\[f_p(x|\\lambda) =\\dfrac{\\lambda^x}{x!}\\cdot e^{-\\lambda}\\] <p>Beispiel</p> <p>Stell dir vor, t\u00e4glich kommen 100 Personen in die Fakult\u00e4t. Im Durchschnitt vergisst davon eine Person ihren Laptop.</p> <p>Frage: Wie hoch ist die Wahrscheinlichkeit, dass genau 3 Personen an einem Tag ihren Laptop vergessen?</p> <p>L\u00f6sung:</p> <p>Die Situation wird mithilfe der Poisson-Verteilung modelliert. Die Formel lautet:</p> \\[P(X = k) = \\dfrac{\\lambda^k}{k!} \\cdot e^{-\\lambda}\\] <ul> <li>\\(\\lambda\\) = Erwartungswert = 1 (Durchschnittlich vergisst eine Person pro Tag ihren Laptop).</li> <li>\\(k\\) = Anzahl der Personen, die ihren Laptop vergessen = 3.</li> </ul> <p>Berechnung:</p> <p>Einsetzen in die Formel:</p> \\[P(X = 3) = \\dfrac{1^3}{3!} \\cdot e^{-1}\\] <p>Zwischenschritte:</p> <ul> <li>\\(1^3 = 1\\)</li> <li>\\(3! = 6\\)</li> <li>\\(e^{-1} \\approx 0,3679\\)</li> </ul> \\[P(X = 3) = \\dfrac{1}{6} \\cdot 0,3679 \\approx 0,0613\\] <p>Ergebnis Die Wahrscheinlichkeit, dass genau 3 Personen an einem Tag ihren Laptop vergessen, betr\u00e4gt 6,13 %.</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_7_Poisson_Verteilung/#jetzt-seid-ihr-dran","title":"Jetzt seid Ihr dran!","text":"Beispiel 1 <p>Anzahl von Anrufen, die in einer Stunde bei einem Callcenter eingehen (\\(\\lambda = 10\\)).</p> <p>Gesucht: Wahrscheinlichkeit, dass genau 12 Anrufe in einer Stunde eingehen.</p> L\u00f6sung <p>Berechnung:</p> \\[ P(X = 12) = \\frac{10^{12} \\cdot e^{-10}}{12!} \\] <p>Ergebnis:</p> \\[ P(X = 12) \\approx 0.0948 \\] Beispiel 2 <p>Anzahl von Verkehrsunf\u00e4llen an einer bestimmten Kreuzung an einem Tag (\\(\\lambda = 2\\)).</p> <p>Gesucht: Wahrscheinlichkeit, dass an diesem Tag genau 3 Unf\u00e4lle passieren.</p> L\u00f6sung <p>Berechnung:</p> \\[ P(X = 3) = \\frac{2^3 \\cdot e^{-2}}{3!} = \\frac{8 e^{-2}}{6} \\] <p>Ergebnis:</p> \\[ P(X = 3) \\approx 0.1804 \\] Beispiel 3 <p>Anzahl von Maschinenfehlern in einer Produktionslinie pro Schicht (\\(\\lambda = 1\\)).</p> <p>Gesucht: Wahrscheinlichkeit, dass genau 0 Maschinenfehler in einer Schicht auftreten.</p> L\u00f6sung <p>Berechnung:</p> \\[ P(X = 0) = \\frac{1^0 \\cdot e^{-1}}{0!} = e^{-1} \\] <p>Ergebnis:</p> \\[ P(X = 0) \\approx 0.3679 \\] Beispiel 4 <p>Anzahl von E-Mails, die ein Mitarbeiter pro Tag erh\u00e4lt (\\(\\lambda = 15\\)).</p> <p>Gesucht: Wahrscheinlichkeit, dass ein Mitarbeiter an einem Tag genau 10 E-Mails erh\u00e4lt.</p> L\u00f6sung <p>Berechnung:</p> \\[ P(X = 10) = \\frac{15^{10} \\cdot e^{-15}}{10!} \\] <p>Ergebnis:</p> \\[ P(X = 10) \\approx 0.0347 \\] Beispiel 5 <p>Anzahl von Flugversp\u00e4tungen an einem Flughafen pro Woche (\\(\\lambda = 4\\)).</p> <p>Gesucht: Wahrscheinlichkeit, dass an einem Tag genau 5 Flugversp\u00e4tungen auftreten.</p> Tipp <p>Achte auf die Einheiten!</p> L\u00f6sung <p>Umrechnung auf pro Tag: \\(\\lambda_{\\text{Tag}} = \\frac{4}{7} \\approx 0.5714\\).</p> <p>Berechnung:</p> \\[ P(X = 5) = \\frac{0.5714^5 \\cdot e^{-0.5714}}{5!} \\] <p>Ergebnis:</p> \\[ P(X = 5) \\approx 0.0698 \\]"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_7_Poisson_Verteilung/#mc-fragen-zur-poisson-verteilung","title":"MC-Fragen zur Poisson Verteilung","text":"MC-Frage 1 <p>Wof\u00fcr wird die Poisson Verteilung vor allem genutzt?</p> <p>A) Um Ereignisse kontinuierlich zu modellieren</p> <p>B) Um die Varianz einer kontinuierlichen Verteilung zu berechnen</p> <p>C) Um diskrete Ereignisse \u00fcber eine Zeitspanne zu modellieren</p> <p>D) Um die Anzahl der Lachanf\u00e4lle w\u00e4hrend eines Witzes zu berechnen</p> L\u00f6sung <p>C</p> MC-Frage 2 <p>Welches Symbol steht in der Poisson Verteilung f\u00fcr die durchschnittlich erwartete Anzahl an Ereignissen?</p> <p>A) Lambda</p> <p>B) Omega</p> <p>C) Alpha</p> <p>D) Mu</p> L\u00f6sung <p>A</p> MC-Frage 3 <p>Wie wird der Erwartungswert der Poisson Verteilung beschrieben?</p> <p>A) Durch Lambda</p> <p>B) Durch die Anzahl der Kaffeepausen</p> <p>C) Durch den Binomialkoeffizienten</p> <p>D) Durch Pi</p> L\u00f6sung <p>A</p> MC-Frage 4 <p>Welche Art von Verteilung ist die Poisson Verteilung?</p> <p>A) Normale Verteilung</p> <p>B) Kontinuierliche Verteilung</p> <p>C) Verr\u00fcckte Verteilung</p> <p>D) Diskrete Verteilung</p> L\u00f6sung <p>D</p> MC-Frage 5 <p>Welche Aussage \u00fcber die Varianz der Poisson Verteilung ist korrekt?</p> <p>A) Die Varianz ist das Doppelte von Lambda</p> <p>B) Die Varianz entspricht Lambda</p> <p>C) Die Varianz h\u00e4ngt von der Tageszeit ab</p> <p>D) Die Varianz ist immer Null</p> L\u00f6sung <p>B</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_8_Exponentialverteilung/","title":"Exponentialverteilung","text":""},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_8_Exponentialverteilung/#was-ist-das-fur-eine-verteilung-und-wozu-dient-sie","title":"Was ist das f\u00fcr eine Verteilung und wozu dient sie?","text":"<p>Die Exponentialverteilung ist eine spezielle Wahrscheinlichkeitsverteilung, die in der Statistik verwendet wird. Sie beschreibt, wie lange es im Durchschnitt dauert, bis ein bestimmtes zuf\u00e4lliges Ereignis, das erste Mal eintritt.</p> <ul> <li>Sie ist ged\u00e4chtnislos, was bedeutet, dass es keinen unterschied macht, wie lange man auf ein Ereignis wartet. Die Wahrscheinlichkeit, dass es Eintritt ist zu jedem Zeitpunkt gleich. Die Warscheinlichkeit h\u00e4ngt also nur davon ab, wie lange man noch darauf warten muss, und nicht davon, wie lange man schon gewartet hat.</li> <li>Der Wert ist immer \\(\\geq 0\\) (es gibt keine negativen Zeiten).</li> </ul> <p>Warning</p> <p>Die Exponentialverteilung ist ein theoretisches Modell, das in der Praxis oft verwendet wird, um Prozesse zu vereinfachen. Tats\u00e4chliche Ereignisse k\u00f6nnen von dieser Verteilung abweichen, da reale Daten h\u00e4ufig nicht exakt der Exponentialverteilung folgen. Dennoch ist sie n\u00fctzlich, weil sie mathematisch leicht handhabbar ist und grundlegende Einblicke liefert.</p> <p>Dichtefunktion</p> <p>Die Dichtefunktion, gibt die Wahrscheinlichkeitsdichte an, dass das Ereignis nach einer Zeit \\(x\\) eintritt.</p> \\[f(x; \\lambda) = \\begin{cases} \\lambda e^{-\\lambda x}, &amp; x \\geq 0 \\\\ 0, &amp; sonst \\end{cases}\\] <p>F\u00fcr \\(x &lt; 0\\) gilt \\(f(x) = 0\\), da negative Zeiten keinen Sinn ergeben.</p> <p>Verteilungsfunktion</p> <p>Die Dichtefunktion gibt die Wahrscheinlichkeitsdichte daf\u00fcr an, dass das Ereignis nach einer Zeit \\(x\\) eintritt:</p> \\[F(x; \\lambda) = \\begin{cases} 1 - e^{-\\lambda x}, &amp; x \\geq 0 \\\\ 0, &amp; sonst \\end{cases}\\] <p>\\(\\lambda\\): Kehrwert der durchschnittlichen Zeit bis zum Ereignis</p> <p>\\(e\\): Die Eulersche Zahl (\\(\\approx 2{,}71828\\)).</p> <p>Beispiel</p> <p>Tritt ein Ereignis im Mittel einmal alle 5 Minuten ein, dann ist \\(\\lambda = \\frac{1}{5} = 0{,}2\\).</p> <p>Erwartungswert</p> <p>Wie lange muss man durchschnittlich warten, bis das Ereignis auftritt:</p> \\[\\mathbb{E}(X) = \\frac{1}{\\lambda}\\] <p>Tritt ein Ereignis im Durchschnitt \\(\\lambda\\)-mal pro Einheit auf, dann dauert es im Mittel \\(\\frac{1}{\\lambda}\\) Einheiten, bis es eintritt.</p> <p>Varianz</p> <p>Sie misst, wie stark die Wartezeiten um den Erwartungswert streuen</p> \\[Var(X) = \\frac{1}{\\lambda^2}\\] <p>Eine gr\u00f6\u00dfere Rate \\(\\lambda\\) f\u00fchrt dazu, dass die Varianz kleiner wird, da die Streuung bei h\u00e4ufigeren Ereignissen kleiner ist.</p> <p>Anwendungsbereiche</p> <p>Man verwendet die Exponentialverteilung, wenn ...</p> <ul> <li>Wie lange ein Ger\u00e4t funktioniert, bevor es kaputtgeht.</li> <li>Radioaktiver Zerfall</li> <li>Zeit zwischen zwei Anrufen(im Callcenter)</li> </ul> <p>Beispiel: Wartezeit an einer Kasse</p> <p>An einer Supermarktkasse dauert es im Durchschnitt 3 Minuten, bis ein Kunde fertig bedient wird.</p> <p>Fragen:</p> <p>Wie gro\u00df ist die Wahrscheinlichkeit, dass die Bedienung eines Kunden weniger als 2 Minuten dauert?</p> <p>Wie gro\u00df ist die Wahrscheinlichkeit, dass die Bedienung eines Kunden l\u00e4nger als 4 Minuten dauert?</p> <p>Gegebene Werte:</p> <p>Durchschnittliche Wartezeit: \\(E(X) = 3\\) Minuten</p> <p>Parameter der Exponentialverteilung:</p> \\[\\lambda = \\frac{1}{E(X)} = \\frac{1}{3} \\approx 0{,}333\\] <p>Formeln:</p> <p>Verteilungsfunktion f\u00fcr P(X &lt;= x) :</p> \\[P(X \\leq x) = F(x) = 1 - e^{-\\lambda x}\\] <p>Gegenwahrscheinlichkeit f\u00fcr P(X &gt; x) :</p> \\[P(X &gt; x) = 1 - F(x) = e^{-\\lambda x}\\] <p>Rechnungen:</p> <p>Wahrscheinlichkeit f\u00fcr weniger als 2 Minuten:</p> \\[P(X \\leq 2) = 1 - e^{-\\lambda \\cdot 2} = 1 - e^{-0{,}333 \\cdot 2}\\] \\[P(X \\leq 2) = 1 - e^{-0{,}666} \\approx 1 - 0{,}514 \\approx 0{,}486\\] <p>Ergebnis: Die Wahrscheinlichkeit, dass die Bedienung weniger als 2 Minuten dauert, betr\u00e4gt etwa 48,6 %.</p> <p>Wahrscheinlichkeit f\u00fcr mehr als 4 Minuten:</p> \\[P(X &gt; 4) = e^{-\\lambda \\cdot 4} = e^{-0{,}333 \\cdot 4}\\] \\[P(X &gt; 4) = e^{-1{,}332} \\approx 0{,}264\\] <p>Ergebnis: Die Wahrscheinlichkeit, dass die Bedienung l\u00e4nger als 4 Minuten dauert, betr\u00e4gt etwa 26,4 %.</p> <p>Zusammenfassung:</p> \\[P(X \\leq 2) \\approx 48{,}6\\,\\%\\] \\[P(X &gt; 4) \\approx 26{,}4\\,\\%\\]"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_8_Exponentialverteilung/#rechenaufgabenmc-fragenfreitext-aufgabenwissenfragen-erstellen","title":"Rechenaufgaben/MC-Fragen/Freitext-Aufgaben/Wissenfragen erstellen.","text":"Aufgabe 1 <p>Die t\u00e4gliche Ausfallrate ist laut Hersteller \\(\\lambda = 0{,}001\\).</p> <ol> <li>Wie ist die durchschnittliche Zeit bis ein Handy nicht mehr funktioniert</li> <li>Wie hoch ist die Wahrscheinlichkeit, dass dein Handy nach einem Jahr noch funktioniert ?</li> </ol> L\u00f6sung <p>Erwartungswert: \\(E(X) = \\frac{1}{\\lambda} = \\frac{1}{0{,}001} = 1000\\)</p> <p>Antwort 1: Sie durchschnittliche Zeit bis ein Handy nicht mehr funktioniert sind 1000 Tage</p> \\[F(x;\\lambda) = F(365;0,001) = 1-e^{\u2212\u03bb\u22c5t} = 1-e^{\u22120,001\u22c5365} \u2248 0,306\\] <p>Antwort 2: Nach 365 Tagen sind 30,6% aller Ger\u00e4te ausgefallen. Die Wahrscheinlichkeit, dass unser Ger\u00e4t nach 365 Tagen noch funktionieren wird, liegt also bei 69,4%.</p> Aufgabe 2 <p>Nach 400 Tagen sind 3% der Computer defekt. Bestimme \\(\\lambda\\).</p> L\u00f6sung <p>\\(\\(F(400) = 0,03\\)\\) In die Verteilungsfunktion Eingesetzt:</p> \\[0,03 = 1 - e^{\\lambda * 400}\\] <p>Nach \\(\\lambda\\) aufl\u00f6sen:</p> <p>Exponentialausdruck isolieren:</p> \\[e^{\\lambda*400} = 1 - 0,03 = 0,97\\] <p>Nat\u00fcrlichen logarithmus auf beiden Seiten anwenden:</p> \\[-\\lambda * 400 = ln(0,97)\\] <p>ln(0,97) berechnen:</p> \\[ln(0,97) \u2248 -0,03046\\] <p>Eingesetzt in die Gleichung:</p> \\[-\\lambda * 400 = -0,03046\\] <p>Zum Schluss nach \\(\\lambda\\) aufl\u00f6sen:</p> \\[\\lambda = \frac{0,03046}{400}\\] \\[\\lambda \u2248 0,00007615\\] <p>Antwort: Der Wert von \\(\\lambda\\) betr\u00e4gt etwa 0,00007615 pro Tag.</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_8_Exponentialverteilung/#multiple-choice-aufgaben","title":"Multiple Choice Aufgaben","text":"MC-Frage 1 <p>Ein Ger\u00e4t hat eine mittlere Haltbarkeit von 5 Jahren. Mit welcher Wahrscheinlichkeit ist ein Ger\u00e4t 5 Jahre nach Anschaffung noch gut?</p> <p>A) 37%</p> <p>B) 50%</p> <p>C) 63%</p> <p>D) keine dieser Antworten</p> L\u00f6sung <p>A</p> MC-Frage 2 <p>Welche Verteilungsfunktion geh\u00f6rt zum gr\u00f6\u00dferen Lambda?</p> <p></p> <p>A) die Helle</p> <p>B) die Dunkle</p> <p>C) Die Linien haben das gleiche Lambda</p> <p>D) Das kann man so nicht beurteilen</p> L\u00f6sung <p>B</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_8_Exponentialverteilung/#programmierung","title":"Programmierung","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 1000)\nlmbda = 1/6\n\ndichtefunktion = lmbda * np.exp(-lmbda * x)\nverteilungsfunktion = 1 - np.exp(-lmbda * x)\n\nplt.figure()\n\nplt.subplot(1,2,1)\nplt.plot(x, dichtefunktion)\nplt.title(\"Dichtefunktion\")\nplt.xlabel(\"Zeit\")\nplt.ylabel(\"Wahrscheinlichkeit\")\n\nplt.subplot(1,2,2)\nplt.plot(x, verteilungsfunktion)\nplt.title(\"Verteilungsfunktion\")\nplt.xlabel(\"Zeit\")\nplt.ylabel(\"Wahrscheinlichkeit\")\n\nplt.show()\n</code></pre> <ul> <li>Numpy wurde hier verwendet, um einen gleichm\u00e4\u00dfigen Wertebreich zu erstellen, und um die Exponentialberechnungen durchzuf\u00fchren</li> <li>Matplotlib.pyplot wurde zum Visualisieren verwendet.</li> </ul>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/F_5_X_Pareto_Verteilung/","title":"Pareto-Verteilung","text":"<p>Link zur Websim-Seite</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/test/","title":"Test zu Verteilungen","text":""},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/test/#mc-fragen","title":"MC-Fragen","text":"<p>Mehrere Antworten k\u00f6nnen richtig sein.</p> Binomial Verteilung 1 <p>Welche der folgenden Situationen ist durch eine Binomialverteilung modellierbar?</p> <p>A) Ein W\u00fcrfel wird 10-mal geworfen, und wir z\u00e4hlen die Anzahl der Sechsen.  </p> <p>B) Die L\u00e4nge von 10 Pflanzen wird gemessen, und wir bestimmen den Durchschnitt.  </p> <p>C) Eine M\u00fcnze wird 20-mal geworfen, und wir z\u00e4hlen die Anzahl von Kopf.  </p> <p>D) Ein Fahrer legt 100 Kilometer zur\u00fcck, und wir berechnen die Benzinkosten.</p> L\u00f6sung <p>A, C</p> Binomial Verteilung 2 <p>Was ist der Erwartungswert \\( E(X) \\) mit \\( n=8 \\) und \\( p=0.4 \\)?</p> <p>A) \\( E(X) = 4 \\) </p> <p>B) \\( E(X) = 3.2 \\) </p> <p>C) \\( E(X) = 2.8 \\) </p> <p>D) \\( E(X) = 3.6 \\)</p> L\u00f6sung <p>B</p> MC-Frage 3 <p>Wie gro\u00df ist der Binomialkoeffizient \u201e6 \u00fcber 2\u201c?</p> <p>A) 12  </p> <p>B) 15  </p> <p>C) 20  </p> <p>D) 10</p> L\u00f6sung <p>B</p> Geometrische Verteilung 1 <p>Welche der folgenden Aussagen treffen auf die geometrische Verteilung zu?</p> <p>A) Sie modelliert die Anzahl der Erfolge in einer festen Anzahl von Versuchen.</p> <p>B) Sie hat eine konstante Erfolgswahrscheinlichkeit in jedem Versuch.</p> <p>C) Sie modelliert die Anzahl der Misserfolge bis zum ersten Erfolg.</p> <p>D) Sie hat eine variable Erfolgswahrscheinlichkeit in jedem Versuch.</p> L\u00f6sung <p>B und C</p> Geometrische Verteilung 2 <p>Bei welchen der folgenden Zufallsversuche handelt es sich um eine geometrische Verteilung?</p> <p>A) Ein W\u00fcrfel wird so lange geworfen, bis eine 6 erscheint.</p> <p>B) Eine M\u00fcnze wird so lange geworfen, bis Kopf erscheint.</p> <p>C) Ein Kartenspieler zieht Karten aus einem Deck, bis er ein Ass zieht.</p> <p>D) Ein Basketballspieler wirft so lange Freiw\u00fcrfe, bis er trifft. Die Chance auf ein Treffer ver\u00e4ndert sich zwischen den W\u00fcrfen nicht.</p> L\u00f6sung <p>A, B und D</p> Geometrische Verteilung 3 <p>Welche Bedingung muss f\u00fcr jeden Versuch in einer Geometrischen Verteilung erf\u00fcllt sein?</p> L\u00f6sung <p>Die Versuche m\u00fcssen unabh\u00e4ngig sein.</p> Pareto Verteilung 1 <p>F\u00fcr welche Werte des Parameters \\( \\alpha \\) ist der Erwartungswert einer Pareto-Verteilung definiert?</p> <p>A) F\u00fcr alle \\( \\alpha &gt; 0 \\).  </p> <p>B) F\u00fcr \\( \\alpha \\geq 1 \\).  </p> <p>C) F\u00fcr \\( \\alpha &gt; 1 \\).  </p> <p>D) F\u00fcr \\( \\alpha &gt; 2 \\).</p> L\u00f6sung <p>C</p> Pareto Verteilung 2 <p>Die Verteilungsfunktion (CDF) der Pareto-Verteilung lautet \\( F(x) = 1 - \\left(\\frac{x_m}{x}\\right)^a \\) f\u00fcr \\( x \\geq x_m \\). Wie lautet die Wahrscheinlichkeit \\( P(a \\leq X \\leq b) \\) f\u00fcr \\( a, b \\geq x_m \\)?</p> <p>A) \\( P(a \\leq X \\leq b) = \\left(\\frac{x_m}{a}\\right)^\\alpha - \\left(\\frac{x_m}{b}\\right)^\\alpha \\) </p> <p>B) \\( P(a \\leq X \\leq b) = \\left(\\frac{b}{x_m}\\right)^\\alpha - \\left(\\frac{a}{x_m}\\right)^\\alpha \\) </p> <p>C) \\( P(a \\leq X \\leq b) = \\left(\\frac{x_m}{b}\\right)^\\alpha - \\left(\\frac{x_m}{a}\\right)^\\alpha \\) </p> <p>D) \\( P(a \\leq X \\leq b) = 1 - \\left(\\frac{x_m}{b}\\right)^\\alpha - \\left(\\frac{x_m}{a}\\right)^\\alpha \\)</p> L\u00f6sung <p>C</p> Pareto Verteilung 3 <p>Die Pareto-Verteilung wird oft verwendet, um die Verteilung von Einkommen zu modellieren. Welches der folgenden Szenarien passt am besten zur Pareto-Verteilung?</p> <p>A) Die meisten Personen haben ein mittleres Einkommen, und extrem hohe Einkommen sind selten.  </p> <p>B) Die meisten Personen haben ein sehr niedriges Einkommen, und wenige Personen haben extrem hohe Einkommen.  </p> <p>C) Einkommen sind gleichm\u00e4\u00dfig verteilt.  </p> <p>D) Einkommen haben eine Normalverteilung mit gleichem Mittelwert und Standardabweichung.</p> L\u00f6sung <p>B</p> Poisson Verteilung 1 <p>Was beschreibt die Poisson-Verteilung?</p> <p>A) Die Wahrscheinlichkeit, dass eine zuf\u00e4llige Zahl gleich 0 ist</p> <p>B) Die Anzahl der Ereignisse in einem festen Zeitintervall</p> <p>C) Die Wahrscheinlichkeit, dass genau eine Zahl in einem Intervall auftritt</p> <p>D) Die Wahrscheinlichkeit, dass ein Ereignis in einem kontinuierlichen Intervall auftritt</p> L\u00f6sung <p>B</p> Poisson Verteilung 2 <p>Wie ver\u00e4ndert sich die Poisson-Verteilung, wenn \u03bb kleiner wird?</p> <p>A) Die Verteilung wird enger um den Mittelwert</p> <p>B) Die Verteilung wird breiter</p> <p>C) Die Verteilung wird symmetrisch</p> <p>D) Die Verteilung wird asymmetrisch</p> L\u00f6sung <p>A, D</p> Poisson Verteilung 3 <p>Wann ist der Wert von \u03bb in der Poisson-Verteilung am gr\u00f6\u00dften?</p> <p>A) Wenn die Ereignisse sehr selten sind</p> <p>B) Wenn die Ereignisse sehr h\u00e4ufig sind</p> <p>C) Wenn die Ereignisse unabh\u00e4ngig voneinander auftreten</p> <p>D) Wenn die Ereignisse in einem festen Intervall auftreten</p> L\u00f6sung <p>B</p> Exponentielle Verteilung 1 <p>Ein Ergebnis tritt durchschnittlich alle 10 Stunden ein. Wie gro\u00df ist die Wahrscheinlichkeit, dass es sp\u00e4testens nach 5 Stunden eintritt?</p> <p>A) 0,2231</p> <p>B) 0,3935</p> <p>C) 0,5229</p> <p>D) 0,6321</p> L\u00f6sung <p>B</p> Exponentielle Verteilung 2 <p>Die Exponentialverteilung ist ged\u00e4chtnislos. Was bedeutet das?</p> <p>A) die Anzahl der Ereignisse in einem festen Zeitraum zu beschreiben.</p> <p>B) gleichm\u00e4\u00dfig verteilte Ereignisse darzustellen.</p> <p>C) die Zeit bis zum n\u00e4chsten zuf\u00e4lligen Ereignis zu modellieren.</p> <p>D) die Lebensdauer von unzerst\u00f6rbaren Objekten zu berechnen.</p> L\u00f6sung <p>C</p> Exponentielle Verteilung 3 <p>Die Lebensdauer einer Lampe folgt einer Exponentialverteilung  mit \\(\\lambda = 0,5\\).  Was ist die durchschnittliche Lebensdauer der Lampe?</p> <p>A) 1 Stunde</p> <p>B) 2 Stunden</p> <p>C) 0,5 Stunden</p> <p>D) 4 Stunden</p> L\u00f6sung <p>B</p> Hypergeometrische Verteilung 1 <p>Welche der folgenden Aussagen trifft auf die hypergeometrische Verteilung zu?</p> <p>A) Sie modelliert die Wahrscheinlichkeit von Erfolgen in unabh\u00e4ngigen Bernoulli-Versuchen.  </p> <p>B) Sie beschreibt die Wahrscheinlichkeit von Erfolgen beim Ziehen ohne Zur\u00fccklegen aus einer endlichen Population.  </p> <p>C) Sie ist eine Approximation der Binomialverteilung f\u00fcr gro\u00dfe Stichproben.  </p> <p>D) Sie wird verwendet, um die Zeit zwischen zwei Ereignissen zu modellieren.</p> L\u00f6sung <p>B</p> Hypergeometrische Verteilung 2 <p>Wie verh\u00e4lt sich die hypergeometrische Verteilung im Vergleich zur Binomialverteilung?</p> <p>A) Sie konvergiert zur Binomialverteilung, wenn die Population unendlich gro\u00df ist.  </p> <p>B) Sie ist identisch mit der Binomialverteilung f\u00fcr kleine Stichproben.  </p> <p>C) Sie kann durch die Binomialverteilung approximiert werden, wenn die Stichprobe klein im Vergleich zur Population ist.  </p> <p>D) Sie ist eine spezielle Form der Poisson-Verteilung.</p> L\u00f6sung <p>C</p> Hypergeometrische Verteilung 3 <p>In welchem Szenario ist die hypergeometrische Verteilung anwendbar?</p> <p>A) Beim Werfen einer fairen M\u00fcnze.  </p> <p>B) Beim Ziehen von Karten aus einem Kartendeck ohne Zur\u00fccklegen.  </p> <p>C) Beim Messen der Zeit zwischen zwei Anrufen in einem Callcenter.  </p> <p>D) Beim Z\u00e4hlen der Anzahl von Kunden, die in einem Gesch\u00e4ft ankommen.</p> L\u00f6sung <p>B</p>"},{"location":"content/F_Theoretische%20Wahrscheinlichkeitsmodelle/test/#rechenaufgaben","title":"Rechenaufgaben","text":"Rechenaufgabe 1 <p>Gegeben sei eine Pareto-Verteilung mit dem Parameter \\( \\alpha = 2 \\) und dem minimalen Wert \\( x_m = 1 \\). Berechnen Sie die Wahrscheinlichkeit, dass ein Wert gr\u00f6\u00dfer als 3 ist, also \\( P(X &gt; 3) \\).</p> L\u00f6sung <p>Die Wahrscheinlichkeit, dass \\( X &gt; 3 \\), betr\u00e4gt \\( \\frac{1}{9} \\).</p> Rechenaufgabe 2 <p>Ein Gl\u00fccksrad ist in 5 gleich gro\u00dfe Abschnitte unterteilt. Zwei Abschnitte sind rot, einer ist blau, und zwei sind gr\u00fcn. Das Gl\u00fccksrad wird 4-mal gedreht. Die Wahrscheinlichkeit, dass das Gl\u00fccksrad auf einen roten Abschnitt zeigt, ist \\( P(Rot) = 0.4 \\).</p> <p>Fragen: 1. Wie gro\u00df ist die Wahrscheinlichkeit, dass das Gl\u00fccksrad genau 2-mal auf einen roten Abschnitt zeigt? 2. Wie gro\u00df ist die Wahrscheinlichkeit, dass das Gl\u00fccksrad h\u00f6chstens 1-mal auf einen roten Abschnitt zeigt?</p> L\u00f6sung <ol> <li>34,56 %  </li> <li>47,52 %</li> </ol> Rechenaufgabe 3 <p>Du m\u00f6chtest spontan mit einem Freund ins Kino gehen. Aus Erfahrungswerten wei\u00dft du, dass 30 % deiner Freunde bei der Frage, ob sie spontan mit ins Kino gehen m\u00f6chten, mit \"Ja\" antworten. Berechne die Wahrscheinlichkeit, dass der zweite Freund, den du fragst, diese Frage mit \"Ja\" beantwortet. Beachte, dass du den zweiten Freund nur dann fragst, wenn der erste Freund abgesagt hat.</p> L\u00f6sung <p>21%</p> Rechenaufgabe 4 <p>Die Durchschnittliche Zeit zwischen zwei Ereignissen betr\u00e4gt 60 Sekunden. Wie gro\u00df ist die Wahrscheinlichkeit, dass nach 2 Minuten noch kein Ereignis eingetreten ist?</p> L\u00f6sung <p>L\u00f6sung mit Poisson:</p> <p>Poisson:</p> <p>F\u00fcr betrachteten Zeitraum 2 Minuten: \\(\\text{Rate} = 1 \\frac{\\text{Erg}}{\\text{Min}}\\)</p> <p>\\(\\text{Intervallgr\u00f6\u00dfe} = 2 \\text{Min}\\)</p> <p>lambda  = Rate * Intervallgr\u00f6\u00dfe = 1 Erg./Min * 2 Min = 2 Erg.</p> <p>F\u00fcr betrachteten Zeitraum 120 Sekunden:</p> <p>Rate = 1/60 Erg. / Sek Intervallgr\u00f6\u00dfe = 120 Sek</p> <p>lambda = Rate * Intervallgr\u00f6\u00dfe = 1/60 Erg./Sek * 120 Sek = 2 Erg.</p> <p>P(0|2)= 2^0 / 0! *e^-2 = 1/1 * 0,1353...</p> <p>L\u00f6sung mit Exponentialverteilung:</p> <p>P(X&gt;2) = 1 - P(X&lt;=2) = 1 - F(2,1) = 1 - (1 - e^-(2*1)) oder = 1 - F(120, 1/60) = 1 - (1 - e^-(120 * 1/60))</p> <p>= e^-2</p> <p>Antwort: Die Wahrscheinlichkeit, dass nach zwei Minuten kein Ereignis eingetreten ist, betr\u00e4gt etwa 13,53%.</p> Rechenaufgabe 5 <p>Ziehen von Karten:</p> <p>In einem Kartenspiel gibt es 52 Karten, darunter 13 Herz-Karten. Du spielst ein Spiel, bei dem du zuf\u00e4llig 5 Karten aus dem Stapel ziehst, ohne sie zur\u00fcckzulegen.</p> <p>a) Wie gro\u00df ist die Wahrscheinlichkeit, dass du genau 2 Herz-Karten in deiner Hand hast?</p> <p>b) Wie gro\u00df ist die Wahrscheinlichkeit, dass du h\u00f6chstens 2 Herz-Karten in deiner Hand hast?</p> <p>c) (zus\u00e4tzliche Herausforderung): Wie gro\u00df ist die Wahrscheinlichkeit, dass du mindestens 3 Herz-Karten in deiner Hand hast?</p> L\u00f6sung <p>a) L\u00f6sung \u00fcber hypergeometrische Verteilung</p> <p>Unsere Parameter:</p> <ul> <li>Gesamtzahl der Karten \\( N = 52 \\)</li> <li>Anzahl der Herz-Karten \\( M = 13 \\)</li> <li>Stichprobengr\u00f6\u00dfe \\( n = 5 \\)</li> </ul> <p>Unsere Formel:</p> <p>\\( P(X = k) = \\frac{\\binom{M}{k} \\cdot \\binom{N - M}{n - k}}{\\binom{N}{n}} \\)</p> <p>a) Wahrscheinlichkeit, genau 2 Herz-Karten zu ziehen:</p> <p>F\u00fcr \\( k = 2 \\):</p> <p>\\( P(X = 2) = \\frac{\\binom{13}{2} \\cdot \\binom{39}{3}}{\\binom{52}{5}} \\)</p> <p>mit</p> <p>\\( \\binom{13}{2} = \\frac{13!}{2!(13-2)!} = 78 \\)</p> <p>\\( \\binom{39}{3} = \\frac{39!}{3!(39-3)!} = 9.139 \\)</p> <p>\\( \\binom{52}{5} = \\frac{52!}{5!(52-5)!} = 2.598.960 \\)</p> <p>Also:</p> <p>\\( P(X = 2) = \\frac{78 \\cdot 9.139}{2.598.960} \\approx 0,274 \\)</p> <p>b) Wahrscheinlichkeit, h\u00f6chstens 2 Herz-Karten zu ziehen: Dies entspricht der Summe der Wahrscheinlichkeiten f\u00fcr 0, 1 und 2 Herz-Karten:</p> <p>\\( P(X \\leq 2) = P(X = 0) + P(X = 1) + P(X = 2) \\)</p> <p>Berechnungen:</p> <p>\\( P(X = 0) = \\frac{\\binom{13}{0} \\cdot \\binom{39}{5}}{\\binom{52}{5}} = \\frac{1 \\cdot 575.757}{2.598.960} \\approx 0,221 \\)</p> <p>\\( P(X = 1) = \\frac{\\binom{13}{1} \\cdot \\binom{39}{4}}{\\binom{52}{5}} = \\frac{13 \\cdot 82.251}{2.598.960} \\approx 0,412 \\)</p> <p>\\( P(X = 2) \\approx 0,274 \\quad \\text{(wie oben berechnet)} \\)</p> <p>Somit:</p> <p>\\( P(X \\leq 2) = 0,221 + 0,412 + 0,274 = 0,907 \\)</p> <p>c) Wahrscheinlichkeit, mindestens 3 Herz-Karten zu ziehen:</p> <p>Dies ist das Gegenereignis zu \"h\u00f6chstens 2 Herz-Karten\":</p> <p>\\( P(X \\geq 3) = 1 - P(X \\leq 2) \\)</p> <p>Berechnung:</p> <p>\\( P(X \\geq 3) = 1 - 0,907 = 0,093 \\)</p> Rechenaufgabe 6 <p>Ein Verkehrsz\u00e4hler erfasst in einer Woche durchschnittlich 35 Autos, die eine bestimmte Stra\u00dfe passieren. </p> <p>Wie hoch ist die Wahrscheinlichkeit, dass an einem einzelnen Tag genau 6 Autos diese Stra\u00dfe passieren.</p> L\u00f6sung <p>Umrechnung des Parameters \\(\\lambda\\) von einer Woche auf einen Tag:</p> \\[ \\lambda_{\\text{Tag}} = \\frac{\\lambda_{\\text{Woche}}}{7} = \\frac{35}{7} = 5 \\] <p>Wahrscheinlichkeit berechnen:</p> \\[ P(X = 6) = \\frac{\\lambda_{\\text{Tag}}^6 e^{-\\lambda_{\\text{Tag}}}}{6!} \\] <p>Einsetzen von \\(\\lambda_{\\text{Tag}} = 5\\):</p> \\[ P(X = 6) = \\frac{5^6 e^{-5}}{6!} = \\frac{15625 \\cdot e^{-5}}{720} \\] <p>Ergebnis: $$ P(X = 6) \\approx 0.1462 $$</p> <p>Die Wahrscheinlichkeit, dass an einem einzelnen Tag genau 6 Autos die Stra\u00dfe passieren, betr\u00e4gt etwa 14,62%.</p>"},{"location":"content/Neuronale_Netze/nn/","title":"Neuronale Netze","text":"<p>\ud83d\udcd5Download Powerpoint</p>"},{"location":"content/PCA/0_einf%C3%BChrung/","title":"Einf\u00fchrung in PCA","text":"<p>Die Idee von PCA ist es das Koordinatensystem des Raumes so zu transformieren, dass die Dimensionen des Raums nach der Informationsf\u00fclle (der Varianz) der Daten sortiert sind. Dann k\u00f6nnen n\u00e4mlich sp\u00e4tere Achsen des Koordinatensystems weggelassen werden. Dies sorgt daf\u00fcr, dass die Komplexit\u00e4t der Daten verringert wird, aber gleichzeitig nur wenig Informationen verloren gehen.</p> <p>Merkmale haben verschiedenen Informationsgehalt</p> <p>Du siehst hier Pullover mit verschiedenen Farben und Gr\u00f6\u00dfen.</p> <p>Welche der beiden Eigenschaften hat eine gr\u00f6\u00dfere Aussagekraft?</p> <p>Welche der beiden Eigenschaften hat die gr\u00f6\u00dfere Varianz?</p> <p></p> <p>Projektion auf Achsen</p> <p>Im einfachsten Fall k\u00f6nnte man sich anschauen, welche Varianz eine einzelnes Merkmal der Daten aufweist:</p> <pre><code>df = pd.DataFrame(\n{\n    'x' : [1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.1, 1.2, 1.15, 1.14, 1.25, 1.22],\n    'y' : [3.3, 3.5, 3.5, 3.6, 3.65, 3.6, 3.3, 3.4, 3.4, 3.32, 3.42, 3.45],\n})\n</code></pre> <p>Datensatz:</p> <p></p> <p>Projektion auf die x-Achse:</p> <p></p> <p>Projektion auf die y-Achse:</p> <p></p> <p>Gesucht ist eine Achse, bei der die auf die Achse projezierten Punkte eine m\u00f6glichst hohe Varianz behalten.</p> <p>Projektion mit PCA:</p> <p></p> <p>Verleich der drei Varianten:</p> <p></p>"},{"location":"content/PCA/0_einf%C3%BChrung/#grobes-vorgehen","title":"Grobes Vorgehen","text":"<p>Bei PCA werden zun\u00e4chst die Daten in die Mitte des Koordinatensystems verschoben.</p> <p></p> <p>Danach wird ein Projektionsvektor gefunden, bei dem die Daten eine gr\u00f6\u00dftm\u00f6gliche Varianz behalten.</p> <p></p> <p>Die Informationen des gefunden Vektors k\u00f6nnen aus den Daten herausgerechnet werden. Dann kann das Verfahren erneut angewandt werden, um den n\u00e4chsten Vektor zu finden, der senkrecht auf dem alten steht und die maximale Varianz bewahrt. Im zweidimensionalen Fall ist da nicht viel zu tun, aber wenn z.B. 30 Dimensionen an Daten vorliegen, ist das nicht trivial.</p> <p></p> <p>Der Kn\u00fcller: Die gesuchten Vektoren sind genau die Eigenvektoren der Kovarianzmatrix der Daten! Das macht die ganze Berechnung sehr einfach und ist dazu noch faszinierend.</p>"},{"location":"content/PCA/0_einf%C3%BChrung/#verstandnisplan","title":"Verst\u00e4ndnisplan","text":"<ul> <li>Was ist die Kovarianzmatrix?</li> <li>Wie projiziert man einen Punkt auf einen Vektor?</li> <li>Was ist ein Eigenvektor?</li> <li>Wie bestimmt man Eigenvektoren?</li> <li>Warum sind die Varianz maximierenden Vektoren genau die Eigenvektoren?</li> </ul> <p>Wir werden feststellen, dass noch weitere Fragen relevant werden:</p> <ul> <li>Wie kann ich eine Funktion mit einer Nebenbedingung maximieren?</li> <li>Wie normalisiere ich einen Vektor?</li> </ul>"},{"location":"content/PCA/1_projektion/","title":"Projektion","text":""},{"location":"content/PCA/1_projektion/#projektion-auf-einen-vektor","title":"Projektion auf einen Vektor","text":"<p>Wir m\u00fcssen uns zun\u00e4chst klar machen, was eine Projektion auf einen Vektor bedeutet.</p> <p>Aufgespannter Vektorraum</p> <p>Sei \\(v \\in \\mathbb{R}^D\\) ein Vektor. Dann ist </p> \\[ \\langle v \\rangle := \\{ kv | k \\in \\mathbb{R} \\} \\] <p>der von \\(v\\) aufgespannte Vektorraum. Man spricht \\(\\langle v \\rangle\\) aus als \"Der Aufspann von \\(v\\)\" oder \"Der von \\(v\\) aufgespannte Vektorraum\".</p> <p>Einen solchen aufgespannten Vektorraum kannst du dir als eine (eindimensionale) Linie in einem Raum vorstellen.</p> Projektionsraum bestimmen <p>Gegeben seien die folgenden Vektoren:</p> \\[ v = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix}, \\quad w = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad z = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}, \\quad u = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\\\ 2\\end{pmatrix} \\] <p>und die folgenden Punkte:</p> \\[ a = \\begin{pmatrix} 4 \\\\ -2 \\\\ 0 \\end{pmatrix}, \\quad b = \\begin{pmatrix} -4 \\\\ 2 \\\\ 0 \\end{pmatrix}, \\quad c = \\begin{pmatrix} 6 \\\\ -3 \\\\ 9 \\end{pmatrix}, \\quad d = \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix}, \\quad e = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad f = \\begin{pmatrix} 6 \\\\ -3 \\\\ 9 \\\\ 6 \\end{pmatrix}, \\quad g = \\begin{pmatrix} 4 \\\\ -5 \\\\ 6 \\\\ 4 \\end{pmatrix} \\] <p>Welche der Punkte liegen in den Unterr\u00e4umen \\(\\langle v \\rangle\\), \\(\\langle w \\rangle\\), \\(\\langle z \\rangle\\) und \\(\\langle u \\rangle\\)?</p> <p>Bonus: Geben Sie die Vektoren in GeoGebra ein (soweit m\u00f6glich) und verifizieren Sie Ihre L\u00f6sung.</p> Tipp <p>Achte auf die Gr\u00f6\u00dfe der Vektoren. In welchen R\u00e4umen k\u00f6nnen Sie dann \u00fcberhaupt nur liegen?</p> L\u00f6sung <p>Da \\(v \\in \\langle w \\rangle\\) ist, gilt \\(\\langle v \\rangle = \\langle w \\rangle\\)</p> \\(\\langle v \\rangle = \\langle w \\rangle\\) \\(\\langle z \\rangle\\) \\(\\langle u \\rangle\\) \\(a\\) \\(a=2v = -2w\\) \u274c unm\u00f6glich \\(b\\) \\(a=-2v = 2w\\) \u274c unm\u00f6glich \\(c\\) \u274c \\(c=3z\\) unm\u00f6glich \\(d\\) \u274c \u274c unm\u00f6glich \\(e\\) \\(e = 0v = 0w\\) \\(d=0z\\) unm\u00f6glich \\(f\\) unm\u00f6glich unm\u00f6glich \\(f = 3u\\) \\(g\\) unm\u00f6glich unm\u00f6glich \u274c <p>Senkrecht stehen</p> <p>Seien \\(v\\) und \\(w\\) zwei Vektoren gleicher Gr\u00f6\u00dfe. Dann ist \\(v\\) senkrecht auf \\(w\\) (geschrieben \\(v \\perp w\\)) genau dann wenn \\(v^tw=0\\) ist. Oft wird \\(v^tw\\) als \\(v\\cdot w\\) notiert. </p> Senkrechte Vektoren bestimmen <p>Welche der folgenden Vektoren sind senkrecht aufeinander?</p> \\[ \\mathbf{a} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 1 \\\\ 2 \\\\ -2 \\end{pmatrix}, \\quad \\mathbf{c} = \\begin{pmatrix} -3 \\\\ 6 \\\\ 1 \\end{pmatrix}\\quad \\mathbf{d} = \\begin{pmatrix} 4 \\\\ -2 \\end{pmatrix}, \\quad \\mathbf{e} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\\quad \\mathbf{f} = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 2 \\end{pmatrix}, \\quad \\mathbf{g} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 2 \\\\ 1 \\end{pmatrix} \\] <p>Bonus: Gebe die Vektoren in Geogebra ein (soweit m\u00f6glich) und verifiziere so deine L\u00f6sung.</p> L\u00f6sung <p>Es m\u00fcssen nur die Vektoren gleicher gr\u00f6\u00dfe verglichen werden.</p> <p>\\(\\mathbf{a} \\cdot \\mathbf{b} = 0\\) </p> <p>\\(\\mathbf{a} \\cdot \\mathbf{c} = -12\\) </p> <p>\\(\\mathbf{b} \\cdot \\mathbf{c} = 7\\) </p> <p>\\(\\mathbf{d} \\cdot \\mathbf{e} = 0\\) </p> <p>\\(\\mathbf{f} \\cdot \\mathbf{g} = 2\\) </p> <p>Ergebnis:</p> <p>\\(\\mathbf{b} \\not \\perp \\mathbf{a} \\perp \\mathbf{b} \\not\\perp \\mathbf{c}\\) </p> <p>\\(\\mathbf{d} \\perp \\mathbf{e}\\)</p> <p>\\(\\mathbf{f} \\not \\perp \\mathbf{g}\\)</p> <p>Projektionsfunktion</p> <p>Sei \\(v \\in \\mathbb{R}^D\\) ein Vektor, auf den projeziert werden soll.</p> <p>Dann sei \\(pr_v : \\mathbb{R}^D \\to \\langle v \\rangle : w \\mapsto kv\\), wobei ein \\(d\\in \\mathbb{R}^D\\) existiert, mit \\(w = kv + d\\) und \\(d \\perp v\\) (das hei\u00dft \\(d\\) steht senkrecht auf \\(v\\)).</p> <p></p> <p>Die Projektionsfunktion ist derzeit noch sehr unbefriedigend, da sie keine Rechenvorschrift angibt, wie man auf \\(k\\) kommt.</p> <p>Rechenvorschrift der Projektionsfunktion</p> <p>Sei \\(v \\in \\mathbb{R}^D\\) ein Vektor. Dann ist</p> \\[pr_v(w) = \\frac{w^tv}{|v|^2} v\\] <p>Das in der Definition noch nicht n\u00e4her bestimmte \\(k\\) ist also \\(\\frac{w^tv}{|v|^2}\\).</p> <p>Ist \\(|v|=1\\) (ein sog. Einheitsvektor), so gilt sogar:</p> \\[pr_v(w) = v^twv\\] Projektion von Punkten auf Vektoren <p>Gegeben sind die folgenden Punkte und Vektoren in unterschiedlichen Dimensionen:</p> <p>2D:</p> \\[ w_1 = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}, \\quad w_2 = \\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix} \\] \\[ v_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad v_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\quad \\] <p>3D:</p> \\[ w_3 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}, \\quad w_4 = \\begin{pmatrix} -1 \\\\ 0 \\\\ 2 \\end{pmatrix} \\] \\[ v_3 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} \\quad v_4 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\] <p>4D:</p> \\[ w_5 = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\\\ 4 \\end{pmatrix}, \\quad w_6 = \\begin{pmatrix} 0 \\\\ 2 \\\\ -1 \\\\ 1 \\end{pmatrix} \\] \\[ v_5 = \\begin{pmatrix} 2 \\\\ 1 \\\\ 2 \\\\ 1 \\end{pmatrix}, \\quad v_6 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\quad \\] <p>Bestimmen Sie die Projektionen der Punkte auf die jeweiligen Vektoren.</p> <p>Berechnen Sie dann die L\u00e4nge der Projektionen.</p> L\u00f6sung <p>Berechnung der L\u00e4ngen der Vektoren</p> Vektor \\(v\\) L\u00e4nge von \\(v\\) \\(v_1\\) \\(1.0\\) \\(v_2\\) \\(1.0\\) \\(v_3\\) \\(1.0\\) \\(v_4\\) \\(1.73\\) \\(v_5\\) \\(3.16\\) \\(v_6\\) \\(1.0\\) <p>Berechnung der Projektionen:</p> Punkt \\(w\\) Vektor \\(v\\) Projektion \\(pr_v(w)\\) ~L\u00e4nge von \\(pr_v(p)\\) \\(w_1\\) \\(v_1\\) \\((3, 0)\\) \\(3\\) \\(w_1\\) \\(v_2\\) \\((0, 4)\\) \\(4\\) \\(w_2\\) \\(v_1\\) \\((-2, 0)\\) \\(2\\) \\(w_2\\) \\(v_2\\) \\((0, 1)\\) \\(0.71\\) \\(w_3\\) \\(v_3\\) \\((0, 2, 0)\\) \\(2\\) \\(w_3\\) \\(v_4\\) \\((2, 2, 2)\\) \\(3.46\\) \\(w_4\\) \\(v_3\\) \\((0, 0, 0)\\) \\(0\\) \\(w_4\\) \\(v_4\\) \\((\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})\\) \\(0.58\\) \\(w_5\\) \\(v_5\\) \\((2.6, 1.3, 2.6, 1.3)\\) \\(4.11\\) \\(w_5\\) \\(v_6\\) \\((2, 0, 0, 0)\\) \\(2.0\\) \\(w_6\\) \\(v_5\\) \\((0.2, 0.1, 0.2, 0.1)\\) \\(0.32\\) \\(w_6\\) \\(v_6\\) \\((0, 0, 0, 0)\\) \\(0.0\\) <p>Vektor normieren</p> <p>Um einen Vektor zu normieren, teilen wir ihn durch seine Norm.</p> \\[ \\Bigl| \\frac{v}{|v|} \\Bigl| = 1 \\] <p></p> <p>Warum funktioniert diese Rechenregel? Betrachten wir dazu die folgende Herleitung:</p> <p>Herleitung</p> <p>Sei \\(v\\in \\mathbb{R}^D\\setminus \\{0\\}\\) ein Vektor, auf den \\(w\\in \\mathbb{R}^D\\) projiziert werden soll.</p> <p>Zun\u00e4chst normieren wir \\(v\\) zu \\(u := \\frac{v}{|v|}\\). Das hei\u00dft \\(|u| = 1\\).</p> <p>Wir suchen nun ein \\(k \\in \\mathbb{R}\\) mit \\(ku + d = w\\) und dabei ist \\(d \\in \\mathbb{R}^D\\) so, dass \\(d \\perp v, u\\) ist.</p> <p>Da \\(d \\perp u\\) und damit auch \\(d \\perp ku\\) ist, gilt:</p> \\[ 0 = d^t(ku) \\] <p>Da \\(ku + d = w\\) gilt \\(d = w - ku\\):</p> \\[ 0 = (w-ku)^t(ku) = (w^t-ku^t)(ku) = w^tku - ku^tku = kw^tu - k^2u^tu \\] <p>Da \\(|u| = 1\\) ist, gilt \\(u^tu=1\\). Also:</p> \\[ 0 = kw^tu-k^2 = k(w^tu-k) \\] <p>Nun gibt es zwei F\u00e4lle, warum die Gleichung \\(0\\) ergibt.</p> <p>1. Fall: Ist \\(k=0\\), so bedeutet das nach \\(w = ku + d = 0u + d = d\\), dass \\(w = d\\perp u\\) ist. \\(w\\) steht also senkrecht auf \\(u\\) bzw. \\(v\\) und wird daher auf \\(0\\) projeziert.</p> <p>2. Fall: Ist \\(k\\neq 0\\), so muss \\(w^tu-k = 0\\) sein. Dann k\u00f6nnen wir die Gleichung umstellen zu:</p> \\[ w^tu = k \\] <p>Damit haben wir auch unser gesuchtes \\(k\\) gefunden.</p> <p>Wir k\u00f6nnen also sagen, dass \\(pr_v(w)=ku= w ^t u u\\) gilt. Wir k\u00f6nnen dies auch noch umschreiben nach \\(v\\):</p> \\[ pr_v(w) = \\frac{w^t v v}{|v|^2} \\] Projektionsvorschrift bei Einheitsvektoren <p>Erkl\u00e4re, warum </p> \\[ pr_v(w) = v^twv \\] <p>f\u00fcr \\(v, w \\in \\mathbb{R}^D\\) gilt, wenn \\(|v|=1\\) gilt.</p> Tipp <p>\u00dcberlege dir, warum und wann \\(a^tb = b^ta\\) gilt.</p> L\u00f6sung <p>Es gilt immer \\(a^tb = b^ta\\), da</p> \\[ a^tb = a_1 \\cdot b_1 + a_2 \\cdot b_2 + \\cdots + a_n \\cdot b_n = b_1 \\cdot a_1 + b_2 \\cdot a_2 + \\cdots b_n \\cdot + a_n = b^t a \\] <p>Es gilt immer \\(v^tw = w^tv\\). Da der Nenner 1 ist, k\u00f6nnen wir den Nenner aus der allgemeinen Formel weglassen.</p>"},{"location":"content/PCA/1_projektion/#programmierung","title":"Programmierung","text":"Vektoren normieren <p>Erstelle eine Funktion <code>norm(v)</code>, die \\(|v|\\) berechnet.</p> <p>Erstelle dann eine Funktion <code>normalize(v)</code>, die einen Vektor \\(w\\) zur\u00fcckgibt, sodass \\(w\\in \\langle v \\rangle\\)  und \\(|w| = 1\\) gilt.</p> Senkrecht stehen <p>Erstelle eine Funktion <code>is_perpendicular(v, w)</code>, die pr\u00fcft, ob zwei Vektoren <code>w</code> und <code>v</code> senkrecht stehen.</p> Projektionsfunktion programmieren <p>Erstelle eine Funktion <code>projection_faktor(v, w)</code>, die den Vektoren <code>w</code> auf den Vektor <code>v</code> projeziert. Das Ergebnis soll dabei nur der Faktor \\(k\\) aus der Rechnung \\(pr_v(w) = kv\\) sein, der oben beschrieben ist.</p> L\u00f6sung <pre><code>### Daniel ####\nimport numpy as np\nfrom unittest import TestCase\nimport unittest\nfrom parameterized import parameterized\n\ndef projection(n, w):\n     return [x - ((x @ n) / (n.T @ n)) * n.T for x in w]\n\nX = np.matrix([[2,3,5], [1,4,4], [3,2,6], [4,5,7], [2,3,5]])\n\n# a)\nv1 = np.matrix([1,0,0]).T\nv2 = np.matrix([0,0,1]).T\nv3 = np.matrix([0,1,0]).T\n\n# a) Projektion auf v1\nproj_v1_e1 = projection(X, v1)\nprint(\"a) Projektion auf v1:\")\nprint(proj_v1_e1,\"\\n\")\n\n# a) Projektion auf V2\nproj_e1_v2 = projection(proj_v1_e1, v2)\nprint(\"a) Projektion auf v2:\")\nprint(proj_e1_v2, \"\\n\")\n\n# a) Projektion auf V3\nproj_e1_v2 = projection(proj_e1_v2, v3)\nprint(\"a) Projektion auf v3:\")\nprint(proj_e1_v2, \"\\n\")\nprint()\n\n\n# b)\nv1 = np.matrix([1,-2,0.5]).T\nv2 = np.matrix([0.5,1.5,5]).T\nv3 = np.matrix([3,1,0]).T\n\n# b) Projektion auf v1\nproj_v1_e1 = projection(X, v1)\nprint(\"b) Projektion auf v1:\")\nprint(proj_v1_e1,\"\\n\")\n\n# b) Projektion auf V2\nproj_e1_v2 = projection(proj_v1_e1, v2)\nprint(\"b) Projektion auf v2:\")\nprint(proj_e1_v2, \"\\n\")\n\n# b) Projektion auf V3\nproj_e1_v2 = projection(proj_e1_v2, v3)\nprint(\"b) Projektion auf v3:\")\nprint(proj_e1_v2, \"\\n\")\nprint()\n\n\n# c)\nv1 = np.matrix([1,-2,0.5]).T\nv2 = np.matrix([0.5,1.5,5]).T\nv3 = np.matrix([3,1,0]).T\n\n# c) Projektion auf v1\nproj_v1_e1 = projection(X, v1)\nprint(\"c) Projektion auf v1:\")\nprint(proj_v1_e1,\"\\n\")\n\n# c) Projektion auf V2\nproj_e1_v2 = projection(proj_v1_e1, v2)\nprint(\"c) Projektion auf v2:\")\nprint(proj_e1_v2, \"\\n\")\n\n# c) Projektion auf V3\nproj_e1_v2 = projection(proj_e1_v2, v3)\nprint(\"c) Projektion auf v3:\")\nprint(proj_e1_v2, \"\\n\")\n\n\nclass TestProjection(TestCase):\n    @parameterized.expand([\n        (np.array([[2,3,5], [1,4,4], [3,2,6], [4,5,7], [2,3,5]]), np.array([1,0,0]), \n         [np.array([0, 3, 5]), np.array([0, 4, 4]), np.array([0, 2, 6]), np.array([0, 5, 7]), np.array([0, 3, 5])]),\n        (np.array([[2,3,5], [1,4,4], [3,2,6], [4,5,7], [2,3,5]]), np.array([0,0,1]), \n         [np.array([2, 3, 0]), np.array([1, 4, 0]), np.array([3, 2, 0]), np.array([4, 5, 0]), np.array([2, 3, 0])]),\n        (np.array([[2,3,5], [1,4,4], [3,2,6], [4,5,7], [2,3,5]]), np.array([0,1,0]), \n         [np.array([2, 0, 5]), np.array([1, 0, 4]), np.array([3, 0, 6]), np.array([4, 0, 7]), np.array([2, 0, 5])]),\n    ])\n    def test_projection(self, X, n, expected):\n        result = projection(X, n)\n        for r, e in zip(result, expected):\n            np.testing.assert_array_almost_equal(r, e)\n\n\nif __name__ == '__main__':\n    unittest.main()\n\n-------Johannes-------\nimport doctest\ndoctest.testmod()\nimport numpy as np\n\n\ndef projection(x, v):\n    \"\"\"\n    X = np.array([[2, 3, 5],\n              [1, 4, 4],\n              [3, 2, 6],\n              [3, 5, 7],\n              [2, 3, 5]])\n    v1_a, v2_a, v3_a = np.array([1,0,0]), np.array([0,0,1]), np.array([0,1,0])\n    &gt;&gt;&gt; projection(X, v1_a)\n    array([[0, 3, 5],\n              [0, 4, 4],\n              [0, 2, 6],\n              [0, 5, 7],\n              [0, 3, 5]])\n    \"\"\"\n    return x - np.outer(np.dot(x, v) / np.dot(v, v), v)\n\n-----------Nick---------------\n\n\ndef projection(n, w):\n    if not n or not w:\n        raise ValueError('Bitte keine leeren Listen \u00fcbergeben')\n\n    vector_n = np.array(n)\n    vector_w = np.array(w)\n\n    if vector_n.shape != vector_w.shape:\n        raise ValueError('Die Vektoren m\u00fcssen den gleichen shape haben')\n\n\n    projection_1 = ((vector_w.T @ vector_n)/norm(n)**2) * vector_n\n\n    return vector_w - projection_1\n\n\n  -----------Henrik---------------\n\ndef norm(v):\n    sum = 0\n    for i in range(len(v)):\n        sum += v[i]**2\n    return math.sqrt(sum)\n\ndef normalize(v):\n    return v / norm(v)\n\ndef is_perpendicular(v, w):\n    return np.dot(w, v) == 0\n\ndef projection_faktor(v, w):\n    if is_perpendicular(v, w):\n        return 0\n    return np.dot(w, v) / (norm(v)**2)\n\ndef projection(n, w):\n    proj_w_n = w - projection_faktor(n, w) * n\n    return proj_w_n\n</code></pre> L\u00f6sung der Programmieraufgaben <pre><code>### David\n\nimport numpy as np\n\ndef norm(vec: list[int|float]) -&gt; float:\n    return float((np.array(vec) @ np.array(vec).T) ** 0.5)\n\ndef normalize(vec: list[int|float]) -&gt; np.ndarray:\n    return np.array(vec) / norm(vec)\n\ndef is_perpendicular(vec_1: list[int|float], vec_2: list[int|float]) -&gt; bool:\n    return np.array(vec_1) @ np.array(vec_2).T == 0\n\ndef projection_factor(coord: list[int|float], vec: list[int|float]) -&gt; float:\n    return float(np.array(coord).T @ normalize(vec)) / norm(vec) \n\n\n\n  -----johannes-----\n\nimport doctest\ndoctest.testmod()\nimport numpy as np\ndef norm(v):\n    \"\"\"    \n    &gt;&gt;&gt; norm(np.array([3,4]))\n    np.float64(5.0)\n    &gt;&gt;&gt; norm(np.array([0,1]))\n    np.float64(1.0)\n    &gt;&gt;&gt; norm(np.array([1,1]))\n    np.float64(1.4142135623730951)\n    \"\"\"\n    return np.linalg.norm(v)\n\ndef normalize(v):\n\n    v_norm = norm(v)\n    if v_norm == 0:\n        raise ZeroDivisionError(\"Der Nullvektor kann nicht normiert werden.\")\n    return v / v_norm\n\n\ndef is_perpendicular(v, w, toleranz=0.0000001):\n    \"\"\"\n    &gt;&gt;&gt; is_perpendicular(np.array([1, 2]), np.array([-2, 1]))\n    np.True_\n    &gt;&gt;&gt; is_perpendicular(np.array([3, 4]), np.array([6, 8]))\n    np.False_\n    &gt;&gt;&gt; is_perpendicular(np.array([0, 1]), np.array([1, 0]))\n    np.True_\n    &gt;&gt;&gt; is_perpendicular(np.array([1, 1]), np.array([1, -1]))\n    np.True_\n    \"\"\"\n    return abs(np.dot(v, w)) &lt; toleranz\n\n  def projection_faktor(v, w):\n    \"\"\"\n    &gt;&gt;&gt; projection_faktor(np.array([1, 0]), np.array([2, 3]))\n    np.float64(2.0)\n    &gt;&gt;&gt; projection_faktor(np.array([1, 1]), np.array([2, 2]))\n    np.float64(2.0)\n    &gt;&gt;&gt; projection_faktor(np.array([3, 4]), np.array([6, 8]))\n    np.float64(2.0)\n    &gt;&gt;&gt; projection_faktor(np.array([1, 2]), np.array([-2, 1]))\n    np.float64(0.0)\n    &gt;&gt;&gt; projection_faktor(np.array([2, 1]), np.array([-2, 1]))\n    np.float64(-0.6)\n    \"\"\"\n    dot_product = v @ w\n    norm_squared = v@v\n\n    if norm_squared == 0:\n        raise ZeroDivisionError(\"Der Vektor v darf nicht der Nullvektor sein.\")\n\n    return dot_product / norm_squared\n\n\n\n\n  ########Daniel#########\n\n  import unittest\nfrom parameterized import parameterized\n\ndef norm(v: list):\n    return sum(i**2 for i in v)**0.5\n\ndef normalize(v: list):\n    length = norm(v)\n    if length &gt; 0:\n        erg = []\n        for i in v:\n            erg.append(i / length)\n        return erg\n    else:\n        raise ValueError(\"Die L\u00e4nge des zu normalisierenden Vektors muss gr\u00f6\u00dfer als 0 sein!\")\n\ndef is_perpendicular(v: list, w: list):\n    if len(v) == len(w):\n        erg = 0\n        for i in range(len(v)):\n            erg += v[i] * w[i]\n        return erg == 0\n    else:\n        raise ArithmeticError(\"Dimension der Vektoren m\u00fcssen \u00fcbereinstimmen\")\n\ndef projection_faktor(v, w):\n    if len(v) != len(w):\n        raise ArithmeticError(\"Die Dimensionen der Vektoren m\u00fcssen \u00fcbereinstimmen\")\n\n    # Berechnung des Skalarprodukts von v und w\n    dot_product_vw = sum(v_i * w_i for v_i, w_i in zip(v, w))\n\n    # Berechnung der Norm von v\n    length_v = norm(v)\n\n    if length_v == 0:\n        raise ValueError(\"Der Vektor v darf nicht der Nullvektor sein\")\n\n    # Berechnung des Projektionsfaktors\n    k = dot_product_vw / (length_v ** 2)\n    return k\n\nclass TestVectorFunctions(unittest.TestCase):\n\n    @parameterized.expand([\n        ([3, 4], 5.0),\n        ([1, 1, 1], (3**0.5)),\n        ([0, 0, 0], 0.0),\n        ([2, 2, 2], (12**0.5)),\n        ([-3, -4], 5.0)\n    ])\n    def test_norm(self, v, expected):\n        self.assertAlmostEqual(norm(v), expected, places=7)\n\n    @parameterized.expand([\n        ([3, 4], [0.6, 0.8]),\n        ([1, 1, 1], [1/(3**0.5)]*3),\n        ([0, 0, 0], ValueError),\n        ([2, 2, 2], [1/(3**0.5)]*3),\n        ([-3, -4], [-0.6, -0.8])\n    ])\n    def test_normalize(self, v, expected):\n        if expected == ValueError:\n            with self.assertRaises(ValueError):\n                normalize(v)\n        else:\n            result = normalize(v)\n            for r, e in zip(result, expected):\n                self.assertAlmostEqual(r, e, places=7)\n\n    @parameterized.expand([\n        ([1, 0], [0, 1], True),\n        ([1, 2, 3], [4, 5, 6], False),\n        ([1, 2], [2, -1], True),\n        ([0, 0, 0], [0, 0, 0], True),\n        ([1, 2, 3], [1, 2], ArithmeticError)\n    ])\n    def test_is_perpendicular(self, v, w, expected):\n        if expected == ArithmeticError:\n            with self.assertRaises(ArithmeticError):\n                is_perpendicular(v, w)\n        else:\n            self.assertEqual(is_perpendicular(v, w), expected)\n\n    @parameterized.expand([\n        ([1, 2, 3], [4, 5, 6], 2.2857142857142856),\n        ([3, 4], [1, 2], 0.44),\n        ([1, 0], [0, 1], 0.0),\n        ([2, 2, 2], [1, 1, 1], 0.5),\n        ([1, 2], [2, -1], 0.0),\n        ([0, 0, 0], [1, 2, 3], ValueError),\n        ([1, 2, 3], [1, 2], ArithmeticError)\n    ])\n    def test_projection_faktor(self, v, w, expected):\n        if expected == ValueError:\n            with self.assertRaises(ValueError):\n                projection_faktor(v, w)\n        elif expected == ArithmeticError:\n            with self.assertRaises(ArithmeticError):\n                projection_faktor(v, w)\n        else:\n            self.assertAlmostEqual(projection_faktor(v, w), expected, places=7)\n\nif __name__ == '__main__':\n    unittest.main()\n\n\n\n------Marc------\nimport numpy as np\ntest = np.array([[1],[2],[2]])\ndef norm(v):\n    return (v.T @ v)**(1/2)\n\ndef normalize(v):\n    norm_v = norm(v)\n    if norm_v == 0:\n        raise ValueError(\"Der Vektor hat eine Norm von 0, kann nicht normalisiert werden.\")\n    return v / norm_v  \n\n\n\ndef is_perpendicular(v,w):\n    return v.T @ w == 0\n\n\n\ndef projection_factor(v, w):\n    return (w.T @ v).item() / (norm(v) ** 2)  \n\n\n\n\n\n######### Marina\n\n\nimport numpy as np\n\ndef projection_vector(w, v):\n\n    w = np.array(w)\n    v = np.array(v)\n\n    #sum(w_i * v_i for w_i, v_i in zip(w, v))\n    dot_product = np.dot(w, v)\n\n    vector_sqaured = np.dot(v, v)\n    return (dot_product/vector_sqaured) * v\n\n\ndef normalize_vector(v):\n    return (sum(x**2 for x in v))**0.5\n\n\ndef are_perpendicular(v1, v2):\n\n    v1 = np.array(v1)\n    v2 = np.array(v2)\n\n    dot_product = np.dot(v1, v2)\n    return np.isclose(dot_product, 0)\n\n\ndef length_vector(w, v):\n    proj_vec = projection_vector(w,v)\n    return normalize_vector(proj_vec)\n\n\n### Fabian \n\ndef norm(v) -&gt; float:\n    return np.sqrt(sum([(n**2) for n in v]))\n\n\ndef normalize(v) -&gt; list[float]:\n    if not any(v):\n        raise ValueError('Nullvektor kann nicht normalisiert werden')\n    l = norm(v)\n    return [s / l for s in v]\n\n\ndef is_nullvektor(v):\n    return all([s==0 for s in v])\n\n\ndef is_perpendicular(v, w) -&gt; bool:\n    if is_nullvektor(v) or is_nullvektor(w):\n        return False\n    return np.array(v).T @ w == 0\n\n\ndef projection_factor(v, w) -&gt; float:\n    return (np.array(w).T @ v) / norm(v) ** 2\n\n\n###### Noah\n\ndef norm(v):\n    return (sum(num**2 for num in v))**0.5\n\ndef normalize(v):\n    div = norm(v)\n    return [num/div for num in v]\n\ndef perpendicular(v, w):\n    vt_x_w = sum(v*w for v,w in zip(v,w))\n    if vt_x_w == 0:\n        return [f\"The two vectors {v}, {w} are perpendicular.\", vt_x_w]\n    else:\n        return [f\"The two vectors {v}, {w} are NOT perpendicular.\", vt_x_w]\n\ndef projection_factor(v, w):\n    return perpendicular(v, w)[1] / (norm(v)**2)\n\ndef projection(v, w):\n    factor = projection_factor(v, w)\n    return [factor*num for num in v]\n\n\n########## Tobias\n\nimport numpy as np\nfrom unittest import TestCase, main\nfrom parameterized import parameterized\n\n\ndef norm(v):\n    return np.sqrt(sum([elem ** 2 for elem in v]))\n\n\nclass NormTest(TestCase):\n    @parameterized.expand([\n        [[1, 0, 0], 1],\n        [[1, 1, 1, 1], 2],\n        [[0, 0], 0],\n    ])\n    def test_norm_0(self, v, result):\n        self.assertEqual(norm(v), result)\n\n\ndef normalize(v):\n    v = np.array(v)\n    return v / norm(v)\n\n\nclass NormalizeTest(TestCase):\n    @parameterized.expand(([\n        [[2, 0, 0], [1, 0, 0]],\n        [[1, 0], [1, 0]]\n    ]))\n    def test_normalize_0(self, v, w):\n        self.assertListEqual(list(normalize(v)), w)\n\n\ndef is_perpendicular(v, w):\n    v = np.array(v)\n    w = np.array(w)\n    return v.T @ w == 0\n\n\nclass PerpendicularTest(TestCase):\n    @parameterized.expand([\n        [[1, 0, 0], [0, 0, 1], True],\n        [[1, 2], [2, 4], False],\n        [[1, 2, 3], [2, 4, 1], False]\n    ])\n    def test_perpendicular_0(self, v, w, result):\n        self.assertEqual(is_perpendicular(v, w), result)\n\n\ndef projection_factor(v, w):\n    if is_perpendicular(v, w):\n        return 0\n\n    w = np.array(w)\n\n    return (w.T @ normalize(v)) / norm(v)\n\n\nclass ProjectionTest(TestCase):\n    @parameterized.expand([\n        [[1, 0], [3, 4], 3],\n        [[0, 1], [3, 4], 4],\n        [[0, 1, 0], [1, 2, 3], 2],\n        [[0, 1, 0], [-1, 0, 2], 0],\n        [[1, 1, 1], [1, 2, 3], 2]\n    ])\n    def test_projection(self, v, w, k):\n        self.assertAlmostEqual(projection_factor(v, w), k)\n</code></pre>"},{"location":"content/PCA/1_projektion/#projektion-auf-einen-hyperraum","title":"Projektion auf einen Hyperraum","text":"<p>Hyperebene</p> <p>Eine Hyperebene ist ein Raum, der eine Dimension kleiner ist, als der Hauptraum.</p> <p>Beispiel</p> <p>Wenn der Hauptraum der 3-dimensionale Raum ist, dann ist ein Hyperraum eine Ebene, die unendlich gro\u00df ist und  durch den Nullpunkt geht. Diese Ebene muss nicht auf einer der Achsen liegen, sondern kann auch \"quer\" im Raum sein. Das wichtige ist, dass man sich auf dieser Ebene nur noch in zwei verschiedene Richtungen bewegen kann, w\u00e4hrend man im 3d-Raum noch drei orthogonale Richtungsvektoren hatte.</p> <p>Wenn der Hauptraum ein 2d-Koordinatensystem ist, dann ist jede Gerade, die durch den Nullpunkt geht eine Hyperebene.</p> <p>Jeder Hyperraum hat einen Normalenvektor, der senkrecht auf den Raum steht. Dieser ist n\u00fctzlich, um alle  Punkte aus dem gro\u00dfen Raum auf den Hyperraum herunter zu projizieren.</p> <p>Normalenvektor</p> <p>Sei \\(n \\in \\mathbb{R}^D\\) ein Vektor. Dann erzeugt \\(n\\) eine Hyperebene  \\(H := \\{ x \\in \\mathbb{R}^D | x \\perp n \\}\\)</p> <p>Hessesche Normalform</p> <p>Es sei \\(E\\) eine Ebene mit zugeh\u00f6rigen Normalenvektor \\(n\\). Falls die Ebene nicht durch den Nullpunkt geht, kann ein St\u00fctzvektor \\(a \\in E\\) genutzt werden, um die gesamte Ebene als die Punkte \\(x\\) zu beschreiben, die die Gleichung </p> \\[ (x-a)\\cdot n = 0 \\text{ bzw. } (x-a)^t n = 0 \\] <p>erf\u00fcllen.</p> <p>In unseren Untersuchungen sind ist jedoch immer \\(a = 0\\), da alle betrachteten R\u00e4ume durch den Nullpunkt gehen.</p> <p>Beispiel</p> <p>Gegeben sei eine Ebene \\(E\\), die durch die Vektoren  \\(v = \\begin{pmatrix} 1 \\\\ -2 \\\\ 0.5 \\end{pmatrix}\\) und \\(u = \\begin{pmatrix} 3 \\\\ -1 \\\\ 0 \\end{pmatrix}\\) aufgespannt wird.</p> <p>Dann ist \\(n = \\begin{pmatrix} 0.5 \\\\ 1.5 \\\\ 5 \\end{pmatrix}\\) Normalenvektor von \\(E\\) und \\(n \\perp v, u\\). </p> <p></p> Normalenvektor bestimmen <p>Gegeben sei eine Ebene </p> \\[ E: x = r\\cdot \\begin{pmatrix} -3 \\\\ 3 \\\\ 4 \\end{pmatrix} + s\\cdot \\begin{pmatrix} 12 \\\\ 5 \\\\ 1 \\end{pmatrix} \\] <p>Finde einen Normalenvektor der Ebene. Wie viele Normalnvektoren gibt es?</p> Tipp <p>Stelle ein Lineares Gleichungsssystem auf. Die Unbekannten sind die Koordinaten des Normalenvektors.</p> L\u00f6sung <p>Um den Normalenvektor der Ebene zu bestimmen, ben\u00f6tigen wir einen Vektor, der senkrecht auf beiden Richtungsvektoren steht. Dies erreichen wir, indem wir ein lineares Gleichungssystem (LGS) zur Bestimmung eines solchen Vektors \\(\\mathbf{n} = (a, b, c)^T\\) aufstellen.</p> <p>Der Normalenvektor muss orthogonal zu den beiden Spannvektoren sein:</p> \\[ \\begin{pmatrix} -3 \\\\ 3 \\\\ 4 \\end{pmatrix}^t \\cdot \\begin{pmatrix} a \\\\ b \\\\ c \\end{pmatrix} = 0 \\] \\[ \\begin{pmatrix} 12 \\\\ 5 \\\\ 1 \\end{pmatrix}^t \\cdot \\begin{pmatrix} a \\\\ b \\\\ c \\end{pmatrix} = 0 \\] <p>Dies ergibt das lineare Gleichungssystem:</p> \\[ -3a + 3b + 4c = 0 \\] \\[ 12a + 5b + c = 0 \\] <p>Um eine L\u00f6sung zu finden, setzen wir \\( c = 1 \\) (eine beliebige Wahl zur Bestimmung eines konkreten Normalenvektors):</p> \\[ -3a + 3b + 4 = 0 \\] \\[ 12a + 5b + 1 = 0 \\] <p>Umgeformt:</p> \\[ -3a + 3b = -4 \\] \\[ 12a + 5b = -1 \\] <p>L\u00f6sen durch das Additionsverfahren:</p> <p>Multiplizieren der ersten Gleichung mit 4:</p> \\[ -12a + 12b = -16 \\] <p>Addition mit der zweiten Gleichung:</p> \\[ (-12a + 12b) + (12a + 5b) = -16 -1 \\] \\[ 17b = -17 \\] \\[ b = -1 \\] <p>Einsetzen von \\( b = -1 \\) in \\( -3a + 3(-1) = -4 \\):</p> \\[ -3a - 3 = -4 \\] \\[ -3a = -1 \\] \\[ a = \\frac{1}{3} \\] <p>Damit ergibt sich ein Normalenvektor:</p> \\[ \\mathbf{n} = \\begin{pmatrix} \\frac{1}{3} \\\\ -1 \\\\ 1 \\end{pmatrix} \\] <p>Wir hatten zu Beginn \\(c=1\\) gesetzt. Nat\u00fcrlich ist auch jedes vielfache von \\(\\mathbf{n}\\) ein Normalenvektor.</p> Projektion auf Ebene <p>Finde heraus und erkl\u00e4re, wie die Projektion auf die Ebene funktioniert, wenn man den Normalenvektor vorliegen hat.</p> Tipp <p>Nutze gerne diese Quelle von studyflix.</p> L\u00f6sung <p>Wenn der Normalenvektor \\(n\\) der Ebene \\(E\\) auf dem Nullpunkt aufliegt, so gilt die Formel:</p> \\[ pr_E(w) = w - \\frac{w^tn}{|n|^2}\\cdot n  \\] <p>f\u00fcr alle \\(w\\), die projiziert werden sollen.</p> Daten projizieren <p>Gegeben ist eine Matrix \\( X \\) mit drei Merkmalen und f\u00fcnf Beobachtungen:</p> \\[ X = \\begin{pmatrix} 2 &amp; 3 &amp; 5 \\\\ 1 &amp; 4 &amp; 4 \\\\ 3 &amp; 2 &amp; 6 \\\\ 4 &amp; 5 &amp; 7 \\\\ 2 &amp; 3 &amp; 5 \\end{pmatrix} \\] <p>Dabei entspricht jede Spalte einem Merkmal und jede Zeile einer Beobachtung.</p> <p>a) \\(v_1 = (1,0,0)^t,v_2 = (0,0,1)^t,v_3 = (0,1,0)^t\\)</p> <p>b) \\(v_1 = (1,-2,0.5)^t,v_2 = (0.5, 1.5, 5)^t,v_3 = (43, 19, -10)^t\\)</p> <p>c) \\(v_1 = (1,-2,0.5)^t,v_2 = (0.5, 1.5, 5)^t,v_3 = (3,1,0)^t\\)</p> <p>Projiziere die Daten zun\u00e4chst auf \\(v_1\\) (i) und  auf den von \\(v_1\\) aufgespannten Hyperraum \\(E_1\\) (ii).</p> <p>Die auf \\(v_1\\) projizierten Daten bilden ein neues Merkmal.</p> <p>Die Daten in \\(E_1\\) werden nun wieder projiziert auf den Vektor \\(v_2\\) (iii) und auf den von \\(v_2\\) aufgespannten Hyperraum \\(E_2\\) (iv).</p> <p>Die auf \\(v_2\\) projizierten Daten bilden ein neues Merkmal.</p> <p>Schlie\u00dflich werden die Daten in \\(E_2\\) werden nun wieder projiziert auf den Vektor \\(v_3\\) (v) und  auf den von \\(v_3\\) aufgespannten Hyperraum \\(E_3\\) (vi) .</p> <p>Die auf \\(v_3\\) projizierten Daten bilden ein neues Merkmal.</p> <p>Notieren Sie Matrizen mit den neuen Merkmalen.</p> Tipp <p>Nach der Projektion auf den den Vektor k\u00f6nnen die Zwischenergebnisse genutzt werden, um schneller die Projektion auf die Ebene zu bestimmen.</p> L\u00f6sung <p></p> <p>a)</p> \\[ \\begin{pmatrix} 2 &amp; 5 &amp; 3 \\\\ 1 &amp; 4 &amp; 4 \\\\ 3 &amp; 6 &amp; 2 \\\\ 4 &amp; 7 &amp; 5 \\\\ 2 &amp; 5 &amp; 3 \\end{pmatrix} \\] <p>b)</p> \\[ \\begin{pmatrix} -0.29 &amp; 1.11 &amp; \\frac{31}{770} \\\\ -0.95 &amp; 0.96 &amp; \\frac{79}{2310} \\\\ 0.38 &amp; 1.25 &amp; \\frac{107}{2310} \\\\ -0.48 &amp; 1.62 &amp; \\frac{197}{2310} \\\\ -0.29 &amp; 1.11 &amp; \\frac{31}{770}  \\end{pmatrix} \\] Projektion auf Hyperebene Programmieren <p>Baue eine Funktion <code>projection(n, w)</code>, die einen Vektor <code>w</code> auf den vom Normalenvektor <code>n</code> aufgespannten Teilraum projiziert.</p>"},{"location":"content/PCA/2_varianz/","title":"Varianz maximieren","text":"<p>Wir haben gesehen, wie man einen Punkt auf einen Vektor projiziert.</p> <p>Wir nutzen dies, um bei PCA ein neues Merkmal zu erzeugen. Alle unsere Daten werden auf einen Vektor projiziert mit der Formel \\(pr_v(w)=kv\\). im neuen Merkmal wird dann das \\(k\\) als neues Merkmal gespeichert.</p> <p>Nur wollen wir \\(v\\) so w\u00e4hlen, dass die Varianz in dem neuen Merkmal m\u00f6glichst gro\u00df ist.</p> <p>Hier k\u00f6nnen wir ein paar Rechnungen durchf\u00fchren, die wichtige Erkenntnisse liefern.</p> <p>Wiederholung: Varianz</p> <p>Sei \\(x = (x_1, \\cdots , x_N) \\in (\\mathbb{R}^D)^N\\) ein Datensatz aus \\(N\\) Daten und \\(D\\) Merkmalen. Dann ist die Varianz von \\(X\\) definiert als:</p> \\[ var(x) = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\bar{x})^2  \\] <p>Angenommen wir haben nun einen Datensatz \\(x\\) und einen Vektor \\(v\\), auf den die Daten projiziert wurden. Wir wollen dann in \\(k = (k_1, k_2, \\cdots , k_N) \\in \\mathbb{R}^{N}\\) die Faktoren die bei der Projektion entstehen speichern. Wir wissen, dass \\(k_i = \\frac{x_i^t v}{|v|^2} = \\frac{v^t x_i}{|v|^2}\\) gilt.</p> <p>Berechnen wir nun \\(var(k)\\):</p> \\[\\begin{align}     var(k) &amp;= \\frac{1}{N} \\sum_{i=1}^{N} (k_i - \\bar{k})^2 \\\\     &amp;= \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\frac{v^t x_i}{|v|^2} - \\frac{v^t \\bar{x}}{|v|^2} \\right)^2 \\\\     &amp;= \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\frac{1}{|v|^2} \\left( v^t x_i - v^t \\bar{x} \\right) \\right)^2 \\\\     &amp;= \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\frac{1}{|v|^2} \\right)^2 \\left( v^t x_i - v^t \\bar{x} \\right)^2 \\\\     &amp;= \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{|v|^4} \\left( v^t x_i - v^t \\bar{x} \\right)^2 \\\\     &amp;= \\frac{1}{|v|^4}\\cdot \\frac{1}{N} \\sum_{i=1}^{N} \\left( v^t x_i - v^t \\bar{x} \\right)^2 \\\\     &amp;= \\frac{1}{|v|^4}\\cdot \\frac{1}{N} \\sum_{i=1}^{N} \\left( v^t (x_i - \\bar{x}) \\right)^2 \\\\     &amp;= \\frac{1}{|v|^4}\\cdot \\frac{1}{N} \\sum_{i=1}^{N} \\left( v^t (x_i - \\bar{x}) \\right)\\left( v^t (x_i - \\bar{x}) \\right) \\\\     &amp;= \\frac{1}{|v|^4}\\cdot \\frac{1}{N} \\sum_{i=1}^{N} \\left( v^t (x_i - \\bar{x}) \\right)\\left( (x_i - \\bar{x})^t v \\right) \\\\     &amp;= \\frac{1}{|v|^4} v^t \\left( \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\bar{x}) (x_i - \\bar{x})^t \\right) v \\\\ \\end{align}\\] <p>Und nun? Hier verbirgt sich ein Schatz, denn der Ausdruck \\(\\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\bar{x}) (x_i - \\bar{x})^t\\), der hier so m\u00fchsam ausgeklammert wurde, ist etwas Besonderes, n\u00e4mlich die geschlossene Form der Kovarianzmatrix \\(S\\) von \\(x\\)! Damit k\u00f6nnen wir die Gleichung zusammenfassen zu:</p> \\[ var(k) = \\frac{1}{|v|^4} v^t S v \\] <p>Wir werden gleich sehen, wie man die Kovarianzmatrix berechnet, doch ich versichere schonmal, dass dies ein gro\u00dfer Schritt in Richtung unseres Zieles ist, einen Vektor \\(v\\) zu finden, sodass \\(var(k)\\) maximal ist.</p> Projezierter Mittelwert <p>Es sei \\(x = (x_1, \\cdots , x_N) \\in (\\mathbb{R}^D)^N\\) ein Datensatz aus \\(N\\) Daten und \\(D\\) Merkmalen. Es Sei \\(v \\in \\mathbb{R}^N\\) der Vektor auf den die \\(x_i\\) projiziert werden.</p> <p>Es sei \\(k = (k_1, k_2, \\cdots , k_N) \\in \\mathbb{R}^{N}\\) mit  \\(k_i = \\frac{x_i^t v}{|v|^2} = \\frac{v^t x_i}{|v|^2}\\) die Projektionsfaktoren.</p> <p>Man erkl\u00e4re, warum \\(\\bar{k} = \\frac{v^t \\bar{x}}{|v|^2}\\) ist.</p> L\u00f6sung \\[ \\bar{k}  = \\frac{1}{N} \\sum_{i=1}^N k_i  = \\frac{1}{N} \\sum_{i=1}^N \\frac{v^t x_i}{|v|^2} = \\frac{v^t}{|v|^2} \\frac{1}{N} \\sum_{i=1}^N x_i = \\frac{v^t \\bar{x}}{|v|^2} \\]"},{"location":"content/PCA/2_varianz/#kovarianz","title":"Kovarianz","text":"<p>Kovarianz</p> <p>Es seien \\(x, y \\in \\mathbb{R}^N\\) zwei Merkmale mit \\(N\\) Auspr\u00e4gungen. Die Kovarianz von \\(x\\) und \\(y\\) berechnet sich mit der Formel</p> \\[ cov(x,y) = \\frac{1}{N} \\sum_{i=1}^N (x_i - \\bar{x})(y_i - \\bar{y})  \\] <p>Oft wird die Notation \\(cov(x,y) = \\sigma_{xy}\\) verwendet.</p> <p>Verbildlichung der Kovarianz</p> <p>\ud83d\udcd9Notebook mit ausf\u00fchrlicher Erkl\u00e4rung zur Kovarianz</p> <p></p> Kovarianzen berechnen <p>Gegeben ist eine Matrix \\( X \\) mit drei Merkmalen und zehn Beobachtungen:</p> \\[ X = \\begin{pmatrix} 2 &amp; 7 &amp; 2 \\\\ 4 &amp; 8 &amp; 6 \\\\ 2 &amp; 6 &amp; 1 \\\\ 7 &amp; 6 &amp; 3 \\\\ 1 &amp; 5 &amp; 9 \\\\ 1 &amp; 4 &amp; 6 \\\\ 2 &amp; 6 &amp; 8 \\\\ 1 &amp; 8 &amp; 3 \\\\ 4 &amp; 6 &amp; 7 \\\\ 6 &amp; 4 &amp; 5 \\end{pmatrix} \\] <p>Dabei entspricht jede Spalte einem Merkmal und jede Zeile einer Beobachtung.</p> <p>Berechnen Sie die Mittelwerte der drei Merkmale.</p> <p>Bestimmen Sie die Kovarianzen zwischen allen Merkmals-Paaren.</p> L\u00f6sung <p>Berechnung der Mittelwerte: </p> \\[ \\bar{x}_1 = \\frac{2+4+2+7+1+1+2+1+4+6}{10} = 3.0 \\] \\[ \\bar{x}_2 = \\frac{7+8+6+6+5+4+6+8+6+4}{10} = 6.0 \\] \\[ \\bar{x}_3 = \\frac{2+6+1+3+9+6+8+3+7+5}{10} = 5.0 \\] <p>Berechnung der Kovarianzen: </p> \\( a \\) \\( b \\) \\( \\text{cov}(a,b) \\) \\( x_1 \\) \\( x_1 \\) \\( 4.2 \\) \\( x_1 \\) \\( x_2 \\) \\( -0.3 \\) \\( x_1 \\) \\( x_3 \\) \\( -0.7 \\) \\( x_2 \\) \\( x_1 \\) \\( -0.3 \\) \\( x_2 \\) \\( x_2 \\) \\( 1.8 \\) \\( x_2 \\) \\( x_3 \\) \\( -1.1 \\) \\( x_3 \\) \\( x_1 \\) \\( -0.7 \\) \\( x_3 \\) \\( x_2 \\) \\( -1.1 \\) \\( x_3 \\) \\( x_3 \\) \\( 6.4 \\) Mathematik der Kovarianz <p>Was ist \\(cov(x,x)\\) f\u00fcr ein Merkmal \\(x\\)?</p> <p>In wie fern ist die Kovarianz kommutativ?</p> <p>Wie l\u00e4sst sich die Kovarianz mit Hilfe des Erwartungswertes ausdr\u00fccken?</p> <p>Welche Alternative Rechenregel l\u00e4sst sich so f\u00fcr die Berechnung der Kovarianz bilden und wie?</p> L\u00f6sung <p>1. Was ist \\( cov(x,x) \\) f\u00fcr ein Merkmal \\( x \\)?</p> <p>Die Kovarianz eines Merkmals mit sich selbst entspricht der Varianz dieses Merkmals:</p> \\[ cov(x, x) = Var(x) = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\bar{x})^2 \\] <p>Das bedeutet, dass die Kovarianzmatrix entlang der Diagonalen die Varianzen der einzelnen Merkmale enth\u00e4lt.</p> <p>2. Inwiefern ist die Kovarianz kommutativ?</p> <p>Die Kovarianz ist symmetrisch, d. h.:</p> \\[ cov(x, y) = cov(y, x) \\] <p>Das folgt direkt aus der Definition:</p> \\[ cov(x, y) = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y}) \\] <p>Da die Multiplikation kommutativ ist \\((a \\cdot b = b \\cdot a)\\), folgt sofort die Symmetrie der Kovarianz.</p> <p>3. Wie l\u00e4sst sich die Kovarianz mit Hilfe des Erwartungswertes ausdr\u00fccken?</p> <p>Die Kovarianz kann mit dem Erwartungswertoperator geschrieben werden als:</p> \\[ cov(x, y) = E[(x - E[x])(y - E[y])] \\] <p>Das bedeutet, dass die Kovarianz der Erwartungswert des Produkts der zentrierten Variablen ist.</p> <p>4. Welche alternative Rechenregel l\u00e4sst sich so f\u00fcr die Berechnung der Kovarianz bilden und wie?</p> <p>Eine alternative Berechnung der Kovarianz ergibt sich aus folgender Umformung:</p> \\[ cov(x, y) = E[xy] - E[x]E[y] \\] <p>Diese Formel ergibt sich durch Ausmultiplizieren der Definition:</p> \\[ cov(x, y) = E[(x - E[x])(y - E[y])] = E[xy - x\\cdot E[y] - y\\cdot E[x] + E[x]E[y]] = E[xy] - E[x \\cdot E[y] - E[y\\cdot E[x]] + E[E[x]E[y]] = E[xy] - E[x]E[y] - E[y]E[x] + E[x]E[y] = E[xy] - E[x]E[y] \\] <p>Diese alternative Rechenregel kann die Berechnung der Kovarianz vereinfachen, da sie den Mittelwert von Produkten direkt nutzt.</p> Kovarianzmatrix programmieren <p>Implementiere Sie das Berechnen Kovarianz auf verschiedene Arten, die hier in Definitionen und \u00dcbungen vorgestellt wurden.</p> L\u00f6sung <pre><code>###HDaniel###\nimport numpy as np\n\ndef mittel(X: np.matrix):\n    if not isinstance(X, np.matrix):\n        raise TypeError(\"Input must be a numpy matrix.\")\n    if X.shape[0] == 0 or X.shape[1] == 0:\n        raise ValueError(\"Matrix dimensions must be greater than 0.\")\n    return [np.mean(x) for x in X]\n\ndef cov(X: np.matrix):\n    if not isinstance(X, np.matrix):\n        raise TypeError(\"Input must be a numpy matrix.\")\n    if X.shape[0] == 0 or X.shape[1] == 0:\n        raise ValueError(\"Matrix dimensions must be greater than 0.\")\n    m = mittel(X)\n    n = X.shape[1]\n    return np.matrix([[(1 / n) * sum((X[i, k] - m[i]) * (X[j, k] - m[j]) for k in range(n)) for j in range(X.shape[0])] for i in range(X.shape[0])])\n\nX = np.matrix([[2,7,2],[4,8,6],[2,6,1],[7,6,3],[1,5,9],[1,4,6],[2,6,8],[1,8,3],[4,6,7],[6,4,5]]).T\nprint(\"Mittel:\", mittel(X))\nprint(\"Covariance Matrix:\\n\", cov(X))\n\n# Unit tests\nimport unittest\nfrom parameterized import parameterized\n\nclass TestMatrixFunctions(unittest.TestCase):\n\n    @parameterized.expand([\n        (np.matrix([[2,7,2],[4,8,6],[2,6,1],[7,6,3],[1,5,9],[1,4,6],[2,6,8],[1,8,3],[4,6,7],[6,4,5]]).T, [3.0, 6.0, 5.0]),\n        (np.matrix([[1,2,3],[4,5,6],[7,8,9]]).T, [4.0, 5.0, 6.0])\n    ])\n    def test_mittel(self, X, expected):\n        result = mittel(X)\n        self.assertEqual(result, expected)\n\n    @parameterized.expand([\n        (np.matrix([[2,7,2],[4,8,6],[2,6,1],[7,6,3],[1,5,9],[1,4,6],[2,6,8],[1,8,3],[4,6,7],[6,4,5]]).T, \n         np.matrix([[ 4.2, -0.3, -0.7], [-0.3,  1.8, -1.1], [-0.7, -1.1,  6.4]])),\n        (np.matrix([[1,2,3],[4,5,6],[7,8,9]]).T, \n         np.matrix([[6.0, 6.0, 6.0], [6.0, 6.0, 6.0], [6.0, 6.0, 6.0]]))\n    ])\n    def test_cov(self, X, expected):\n        result = cov(X)\n        np.testing.assert_array_almost_equal(result, expected)\n\n    @parameterized.expand([\n        (\"not a matrix\", TypeError),\n        (np.matrix([]), ValueError)\n    ])\n    def test_mittel_errors(self, X, expected_exception):\n        with self.assertRaises(expected_exception):\n            mittel(X)\n\n    @parameterized.expand([\n        (\"not a matrix\", TypeError),\n        (np.matrix([]), ValueError)\n    ])\n    def test_cov_errors(self, X, expected_exception):\n        with self.assertRaises(expected_exception):\n            cov(X)\n\nif __name__ == '__main__':\n    unittest.main()\n\n\n#### Henrik\n\n\ndef mean(m):\n    mean_values = np.mean(m, axis=0)\n    return mean_values\n\ndef covarianz(m, means):\n    distance = np.array([[col - means[i] for i, col in enumerate(row)] for row in m])\n    n = distance.shape[0]\n    cov_matrix = 1/n * distance.T @ distance\n    return cov_matrix\n\n\n\n### Ly\n\ndef calc_covariance(x, y):\n    x_mean = np.mean(x)\n    y_mean = np.mean(y)\n\n    cov = np.mean((x-x_mean)*(y-y_mean))\n    return cov\n\ndef calc_covariance_matrix(X: np.array):\n    _, n_features = X.shape\n    cov_matrix = np.zeros((n_features, n_features))\n    for i in range(n_features):\n        for j in range(n_features):\n            cov_matrix[i, j] = calc_covariance(X[:, i], X[:, j])\n    return cov_matrix\n\n\n\n############## Noah\ndef covariance(arr1, arr2):\n    arr1_mean = sum(arr1)/len(arr1)\n    arr2_mean = sum(arr2)/len(arr2)\n\n    return 1/len(arr1) * (sum((arr1_i - arr1_mean)*(arr2_i - arr2_mean) for arr1_i, arr2_i in zip(arr1, arr2)))\n\ndef covariance_matrix(*arrays):\n    num_arrays = len(arrays)\n    cov_matrix = np.zeros((num_arrays, num_arrays))\n    for i in range(num_arrays):\n        for j in range(num_arrays):\n            cov_value = covariance(arrays[i], arrays[j])\n            cov_matrix[i][j] = cov_value\n\n    return cov_matrix\n\n\n\n\n------Johannes------\nimport numpy as np\n\nX = np.array([[2, 7, 2],\n              [4, 8, 6],\n              [2, 6, 1],\n              [7, 6, 3],\n              [1, 5, 9],\n              [1, 4, 6],\n              [2, 6, 8],\n              [1, 8, 3],\n              [4, 6, 7],\n              [6, 4, 5]])\n\n\nprint(f\"{X.T[0].mean()}\")\nprint(f\"{X.T[1].mean()}\")\nprint(f\"{X.T[2].mean()}\")\nnp.cov(X.T, bias=True)\n\n\n\n#Nick\n\ndef cov(x, y):\n    if not x or not y:\n        raise ValueError('Bitte zwei Vektoren \u00fcbergeben')\n\n    vector_x = np.array(x)\n    vector_y = np.array(y)\n    x_ = sum(vector_x)/len(vector_x)\n    y_ = sum(vector_y)/len(vector_y)\n\n    return sum((number_1 - x_)*(number_2 - y_) for number_1, number_2 in zip(vector_x, vector_y))/len(vector_x)\n\nclass Test_Cov(unittest.TestCase):\n  x_1 = [2, 4, 2, 7, 1, 1, 2, 1, 4, 6]\n    x_2 = [7, 8, 6, 6, 5, 4, 6, 8, 6, 4]\n    x_3 = [2, 6, 1, 3, 9, 6, 8, 3, 7, 5]\n    def test_empty(self):\n        with self.assertRaises(ValueError):\n            cov([],[])\n\n\n\n    @parameterized.expand([\n\n        (x_1,x_2, -0.3),         \n        (x_2, x_1,-0.3), \n        (x_3,x_3, 6.4),\n\n    ])\n    def test_parametrized(self, vector_1,vector_2, expected):\n        self.assertAlmostEqual(cov(vector_1,vector_2), expected, places=2)\n\nif __name__ == \"__main__\":\n\n        unittest.main(argv=[''], verbosity=2, exit=False)  \n\n\n--------------\nKevin\n\nX = np.array([[2, 7, 2],\n              [4, 8, 6],\n              [2, 6, 1],\n              [7, 6, 3],\n              [1, 5, 9],\n              [1, 4, 6],\n              [2, 6, 8],\n              [1, 8, 3],\n              [4, 6, 7],\n              [6, 4, 5]])\n\nx1 = X[:,0]\nx2 = X[:,1]\nx3 = X[:,2]\n\ndef covariance(x, y):\n    mean_x = np.mean(x)\n    mean_y = np.mean(y)\n\n    return np.sum((x - mean_x) * (y - mean_y)) / (len(x))\n\n\ndef covariance_matrix(X):\n    spalten = X.shape[1]\n    cov_matrix = np.zeros((spalten, spalten))\n\n    for x in range(spalten):\n        for y in range(spalten):\n            cov_matrix[x, y] = covariance(X[:, x], X[:, y])\n\n    return cov_matrix\n\n\n-------- Tom\nimport numpy as np\n\nX = np.array([\n    [2,7,2],\n    [4,8,6],\n    [2,6,1],\n    [7,6,3],\n    [1,5,9],\n    [1,4,6],\n    [2,6,8],\n    [1,8,3],\n    [4,6,7],\n    [6,4,5]\n])\n\nnp.cov(X,rowvar=False, bias=True)\n\ud83e\uddab\n\n\n\n### Marina\n\n\ndef covariance(X):\n    X = np.array(X)\n\n    # print(X.shape)\n    n = X.shape[0]\n\n    cov_matrix = np.zeros((X.shape[1], X.shape[1]))\n    # print(cov_matrix)\n\n    for i in range(X.shape[1]):\n        for j in range(X.shape[1]):\n            sum_diff = np.sum((X[:, i] - np.mean(X[:, i])) * (X[:, j] - np.mean(X[:, j])))\n            cov_matrix[i, j] = sum_diff / n\n\n    return cov_matrix\n\n# Beispielmatrix\nX = np.array([\n    [2, 7, 2],\n    [4, 8, 6],\n    [2, 6, 1],\n    [7, 6, 3],\n    [1, 5, 9],\n    [1, 4, 6],\n    [2, 6, 8],\n    [1, 8, 3],\n    [4, 6, 7],\n    [6, 4, 5]\n])\n\ncov_matrix = covariance(X)\nprint(cov_matrix)\n</code></pre>"},{"location":"content/PCA/2_varianz/#kovarianzmatrix","title":"Kovarianzmatrix","text":"<p>Kovarianzmatrix</p> <p>Es sei \\( X \\in \\mathbb{R}^{N \\times D} \\) eine Datenmatrix mit \\( N \\) Beobachtungen und \\( D \\) Merkmalen. Es sei \\(X_i \\in \\mathbb{R}^N\\) dabei das \\(i\\)-te Merkmal.  Die Eintr\u00e4ge der Kovarianzmatrix \\( S \\) berechnen sich mit der Formel:</p> \\[ S_{ij} = cov(X_i, X_j), \\text{ f\u00fcr } i,j \\in \\{ 1, \\cdots , M\\} \\] Kovarianzmatrix ausrechnen <p>Gegeben ist eine Matrix \\( X \\) mit drei Merkmalen und zehn Beobachtungen:</p> \\[ X = \\begin{pmatrix} 2 &amp; 7 &amp; 2 \\\\ 4 &amp; 8 &amp; 6 \\\\ 2 &amp; 6 &amp; 1 \\\\ 7 &amp; 6 &amp; 3 \\\\ 1 &amp; 5 &amp; 9 \\\\ 1 &amp; 4 &amp; 6 \\\\ 2 &amp; 6 &amp; 8 \\\\ 1 &amp; 8 &amp; 3 \\\\ 4 &amp; 6 &amp; 7 \\\\ 6 &amp; 4 &amp; 5 \\end{pmatrix} \\] <p>Dabei entspricht jede Spalte einem Merkmal und jede Zeile einer Beobachtung.</p> <p>Bestimmen Sie die Kovarianzmatrix.</p> Tipp <p>Die Matrix sollte dir bekannt vorkommen\ud83d\ude09</p> L\u00f6sung \\[ \\begin{pmatrix}  4.2 &amp; -0.3 &amp; -0.7 \\\\ -0.3 &amp;  1.8 &amp; -1.1 \\\\ -0.7 &amp; -1.1 &amp;  6.4 \\end{pmatrix} \\] Geschlossene Form der Kovarianzmatrix <p>Erkl\u00e4re, die geschlossene Form der Koviaranzmatrix zustande kommt.</p> \\[ S = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\bar{x}) (x_i - \\bar{x})^t \\] Tipp <p>Ausf\u00fchrliches Video von ritvikmath</p> Kovarianzmatrix mit geschlossener Form berechnen <p>Gegeben ist eine Matrix \\( X \\) mit drei Merkmalen und zehn Beobachtungen:</p> \\[ X = \\begin{pmatrix} 2 &amp; 7 &amp; 2 \\\\ 4 &amp; 8 &amp; 6 \\\\ 2 &amp; 6 &amp; 1 \\\\ 7 &amp; 6 &amp; 3 \\\\ 1 &amp; 5 &amp; 9 \\\\ 1 &amp; 4 &amp; 6 \\\\ 2 &amp; 6 &amp; 8 \\\\ 1 &amp; 8 &amp; 3 \\\\ 4 &amp; 6 &amp; 7 \\\\ 6 &amp; 4 &amp; 5 \\end{pmatrix} \\] <p>Bestimmen Sie die Kovarianzmatrix mit Hilfe der geschlossenen Form.</p> <p>Baue gerne ein Programm, dass dir bei den Berechnungen hilft, sonst sind es sehr viele.</p> L\u00f6sung <p>Mittelwert bestimmen:</p> \\[ \\bar{X} = \\begin{pmatrix} 3 \\\\ 6 \\\\ 5 \\end{pmatrix} \\] <p>Faktoren bestimmen:</p> \\[ X_0 - \\bar{X} = \\begin{pmatrix}-1\\\\1\\\\-3\\end{pmatrix} \\quad X_1 - \\bar{X} = \\begin{pmatrix}1\\\\2\\\\1\\end{pmatrix} \\quad X_2 - \\bar{X} = \\begin{pmatrix}-1\\\\0\\\\-4\\end{pmatrix} \\quad X_3 - \\bar{X} = \\begin{pmatrix}4\\\\0\\\\-2\\end{pmatrix} \\quad X_4 - \\bar{X} = \\begin{pmatrix}-2\\\\-1\\\\4\\end{pmatrix} \\quad X_5 - \\bar{X} = \\begin{pmatrix}-2\\\\-2\\\\1\\end{pmatrix} \\quad X_6 - \\bar{X} = \\begin{pmatrix}-1\\\\0\\\\3\\end{pmatrix} \\quad X_7 - \\bar{X} = \\begin{pmatrix}-2\\\\2\\\\-2\\end{pmatrix} \\quad X_8 - \\bar{X} = \\begin{pmatrix}1\\\\0\\\\2\\end{pmatrix} \\quad X_9 - \\bar{X} = \\begin{pmatrix}3\\\\-2\\\\0\\end{pmatrix} \\] <p>Summanden bestimmen:</p> \\[ (X_0 - \\bar{X})(X_0 - \\bar{X})^T = \\begin{pmatrix}1 &amp; -1 &amp; 3\\\\-1 &amp; 1 &amp; -3\\\\3 &amp; -3 &amp; 9\\end{pmatrix} \\] \\[ (X_1 - \\bar{X})(X_1 - \\bar{X})^T = \\begin{pmatrix}1 &amp; 2 &amp; 1\\\\2 &amp; 4 &amp; 2\\\\1 &amp; 2 &amp; 1\\end{pmatrix} \\] \\[ (X_2 - \\bar{X})(X_2 - \\bar{X})^T = \\begin{pmatrix}1 &amp; 0 &amp; 4\\\\0 &amp; 0 &amp; 0\\\\4 &amp; 0 &amp; 16\\end{pmatrix} \\] \\[ (X_3 - \\bar{X})(X_3 - \\bar{X})^T = \\begin{pmatrix}16 &amp; 0 &amp; -8\\\\0 &amp; 0 &amp; 0\\\\-8 &amp; 0 &amp; 4\\end{pmatrix} \\] \\[ (X_4 - \\bar{X})(X_4 - \\bar{X})^T = \\begin{pmatrix}4 &amp; 2 &amp; -8\\\\2 &amp; 1 &amp; -4\\\\-8 &amp; -4 &amp; 16\\end{pmatrix} \\] \\[ (X_5 - \\bar{X})(X_5 - \\bar{X})^T = \\begin{pmatrix}4 &amp; 4 &amp; -2\\\\4 &amp; 4 &amp; -2\\\\-2 &amp; -2 &amp; 1\\end{pmatrix} \\] \\[ (X_6 - \\bar{X})(X_6 - \\bar{X})^T = \\begin{pmatrix}1 &amp; 0 &amp; -3\\\\0 &amp; 0 &amp; 0\\\\-3 &amp; 0 &amp; 9\\end{pmatrix} \\] \\[ (X_7 - \\bar{X})(X_7 - \\bar{X})^T = \\begin{pmatrix}4 &amp; -4 &amp; 4\\\\-4 &amp; 4 &amp; -4\\\\4 &amp; -4 &amp; 4\\end{pmatrix} \\] \\[ (X_8 - \\bar{X})(X_8 - \\bar{X})^T = \\begin{pmatrix}1 &amp; 0 &amp; 2\\\\0 &amp; 0 &amp; 0\\\\2 &amp; 0 &amp; 4\\end{pmatrix} \\] \\[ (X_9 - \\bar{X})(X_9 - \\bar{X})^T = \\begin{pmatrix}9 &amp; -6 &amp; 0\\\\-6 &amp; 4 &amp; 0\\\\0 &amp; 0 &amp; 0\\end{pmatrix} \\] \\[ S = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\bar{X}) (x_i - \\bar{X})^t \\begin{pmatrix}  4.2 &amp; -0.3 &amp; -0.7 \\\\ -0.3 &amp;  1.8 &amp; -1.1 \\\\ -0.7 &amp; -1.1 &amp;  6.4 \\end{pmatrix} \\] Geschlossene Form der Kovarianzmatrix programmieren <p>Implementiere das Berechnen der Kovarianzmatrix unter Verwendung der geschlossenen Form. Der Funktionsname lautet <code>cov</code>.</p> L\u00f6sung <pre><code># ----Johannes----\ndef covariance_matrix(X):\n    X = X.T # um sp\u00e4ter nicht verwirrt zu sein wie man das drehen und wenden muss\n    n = X.shape[1] # um die l\u00e4nge zu bekommen \n    # 10\n    mean_X = np.mean(X, axis=1, keepdims=True) # durchschnitt \u00fcber alle reihen \n    # [[3.] \n    # [6.]\n    # [5.]]\n    X_centered = X - mean_X\n    # [[-1.  1. -1.  4. -2. -2. -1. -2.  1.  3.]\n    # [ 1.  2.  0.  0. -1. -2.  0.  2.  0. -2.]\n    # [-3.  1. -4. -2.  4.  1.  3. -2.  2.  0.]]\n    return (X_centered @ X_centered.T) / n\n\nif __name__ == \"__main__\":\n    X = np.array([[2, 7, 2],\n        [4, 8, 6],\n        [2, 6, 1],\n        [7, 6, 3],\n        [1, 5, 9],\n        [1, 4, 6],\n        [2, 6, 8],\n        [1, 8, 3],\n        [4, 6, 7],\n        [6, 4, 5]])\n\n    print(f\"{covariance_matrix(X)}\")\n\n#-----Daniel---\ndef mittel(X: np.matrix):\n    if not isinstance(X, np.matrix):\n        raise TypeError(\"Input must be a numpy matrix.\")\n    if X.shape[0] == 0 or X.shape[1] == 0:\n        raise ValueError(\"Matrix dimensions must be greater than 0.\")\n    return [np.mean(x) for x in X]\n\n\ndef cov_geschlossen(X: np.matrix):\n    n = X.shape[1]\n    m = np.matrix(mittel(X)).T\n    return (1 / n) * (X - m) @ (X - m).T\n</code></pre>"},{"location":"content/PCA/3_lagrange/","title":"Lagrange","text":""},{"location":"content/PCA/3_lagrange/#zwischenstand-der-pca-herleitung","title":"Zwischenstand der PCA Herleitung","text":"<p>Wir haben uns bereits damit auseinandergesetzt, wie man einen Datensazt \\( X \\in \\mathbb{R}^{N \\times D} \\)  mit \\( N \\) Beobachtungen und \\( D \\) Merkmalen auf einen Vektor \\(v\\) und auf eine Hyperebene projizieren kann.</p> <p>Wir haben auch gesehen, wie man die Varianz des neuen Merkmals \\(k := pr_v(X)\\) bestimmen kann, indem man die Kovarianzmatrix \\(S\\) von \\(X\\) verwendet: \\(var(k) = \\frac{1}{|v|^4} v^t S v\\).</p> <p>Diese Formel hat aber noch ein gewisse Problematik, die wir durch die n\u00e4chsten \u00dcbungsaufgaben erkennen wollen.</p> Varianzen bei Projekt <p>Gegeben ist eine Matrix \\( X \\) mit drei Merkmalen und f\u00fcnf Beobachtungen:</p> \\[ X = \\begin{pmatrix} 2 &amp; 7 &amp; 2 \\\\ 4 &amp; 8 &amp; 6 \\\\ 2 &amp; 6 &amp; 1 \\\\ 7 &amp; 6 &amp; 3 \\\\ 1 &amp; 5 &amp; 9 \\\\ 1 &amp; 4 &amp; 6 \\\\ 2 &amp; 6 &amp; 8 \\\\ 1 &amp; 8 &amp; 3 \\\\ 4 &amp; 6 &amp; 7 \\\\ 6 &amp; 4 &amp; 5 \\end{pmatrix} \\] <p>Dabei entspricht jede Spalte einem Merkmal und jede Zeile einer Beobachtung.</p> <p>Projiziere die Daten Auf die Vektor \\(v_1 = (1,0,0)^t,v_2 = (2,0,0)^t,v_3 = (1, -2, 0.5)^t\\)</p> <p>Berechne die Varianzen der Vorfaktoren aus \\(pr_{v_1}(X), pr_{v_2}(X)\\) und \\(pr_{v_3}(X)\\) auf zwei Weisen:</p> <ul> <li>Unter Verwendung der Standardformel \\(\\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\bar{x})^2\\),</li> <li>unter Verwendung der hergeleiteten Formel \\(\\frac{1}{|v|^4} v^t S v\\).</li> </ul> L\u00f6sung <p>Vorfaktoren der Projizierten Daten:</p> Index \\(proj_{v_1}\\) \\(proj_{v_2}\\) \\(proj_{v_3}\\) 0 \\(2.0\\) \\(1.0\\) \\(-2.0952\\) 1 \\(4.0\\) \\(2.0\\) \\(-1.7143\\) 2 \\(2.0\\) \\(1.0\\) \\(-1.8095\\) 3 \\(7.0\\) \\(3.5\\) \\(-0.6667\\) 4 \\(1.0\\) \\(0.5\\) \\(-0.8571\\) 5 \\(1.0\\) \\(0.5\\) \\(-1.2857\\) 6 \\(2.0\\) \\(1.0\\) \\(-1.9048\\) 7 \\(1.0\\) \\(0.5\\) \\(-0.3810\\) 8 \\(4.0\\) \\(2.0\\) \\(-1.2381\\) 9 \\(6.0\\) \\(3.0\\) \\(-1.1905\\) <p>Varianz f\u00fcr Projektion auf \\(v_1\\): \\(3.04\\) Varianz f\u00fcr Projektion auf \\(v_2\\): \\(12.16\\) Varianz f\u00fcr Projektion auf \\(v_3\\): \\(7.84\\)</p> <p>Programm zur Berechnung:</p> <pre><code>import numpy as np\nfrom unittest import TestCase\nfrom parameterized import parameterized\n\nX = np.array([[2, 7, 2],\n              [4, 8, 6],\n              [2, 6, 1],\n              [7, 6, 3],\n              [1, 5, 9],\n              [1, 4, 6],\n              [2, 6, 8],\n              [1, 8, 3],\n              [4, 6, 7],\n              [6, 4, 5]])\n\nv1 = np.array([1, 0, 0])\nv2 = np.array([2, 0, 0])\nv3 = np.array([1, -2, 0.5])\n\n\ndef proj_factor(v, w):\n    return w.T @ v / (v.T @ v)\n\n\nclass TestProjection(TestCase):\n    @parameterized.expand([\n        (np.array([1, 0]), np.array([2, 3]), np.array(2)),\n        (np.array([1, 1]), np.array([2, 2]), np.array(2)),\n        (np.array([1, 2]), np.array([-2, 1]), np.array(0)),\n        (np.array([2, 1]), np.array([-2, 1]), np.array(-.6)),\n    ])\n    def test_projection(self, v, w, expected):\n        result = proj_factor(v, w)\n        np.testing.assert_array_almost_equal(expected, result)\n\n\nS = np.cov(X, rowvar=False, bias=True)\nprint(S)\n\nfor v in [v1, v2, v3]:\n    print(f\"Projection on {v}:\")\n    projected = np.apply_along_axis(lambda w: proj_factor(v, w), axis=1, arr=X)\n    print(projected)\n    print(f\"Varianz dieser Projektionsfaktoren ist:    {np.var(projected)}\")\n    print(f\"Varianz mit alternativer Formel berechnet: {v.T @ S @ v / (v.T @ v) ** 2}\")\n    print()\n    # v.T: 1x3\n    # S:   3x3\n    # v:   3x1\n    # v.T @ S: 1x3\n    # v.T @ S @ v : 1x1 -&gt; EINE Zahl\n    # |v|\u00b2 = v.T @ v\n</code></pre> <p>Wir stellen also fest, dass der Faktor \\(\\frac{1}{|v|^4}\\) in \\(var(k)\\) daf\u00fcr sorgt, dass wir \\(var(k)\\) minimieren k\u00f6nnen, indem wir ein \\(v\\) w\u00e4hlen mit m\u00f6glichst gro\u00dfem \\(|v|\\). Das kann aber kein sinnvolles Vorgehen sein.</p> <p>Daher m\u00fcssen wir eine weitere Bedingung hinzuf\u00fcgen, n\u00e4mlich dass \\(v\\) ein Einheitsvektor ist (d.h. es gilt \\(|v| = 1\\)).</p> <p>Wir suchen also \\(v\\in \\mathbb{R}^D\\), sodass \\(v^t S v\\) minimal ist, w\u00e4hrend gleichzeitig die Nebenbedingung \\(|v| = 1\\) gilt.</p> <p>Um so einen Extrempunkt einer Funktion zu finden, w\u00e4hrend gleichzeitig eine Nebenbedingung erf\u00fcllt werden soll, eignet sich die sog. Lagrange-Methode.</p>"},{"location":"content/PCA/3_lagrange/#lagrange-methode","title":"Lagrange-Methode","text":"<p>Erkl\u00e4rung f\u00fcr die Lagrange-Methode von studyflix</p> <p>Lagrange-Methode</p> <p>Gegegeben sei eine Funktion \\(f\\), deren Extremwerte \\(x\\in \\mathbb{R}^D\\) gesucht werden. Diese Extremwerte sollen aber auch gleichzeitig eine Bedingung \\(g(x)=0\\) erf\u00fcllen. Diese l\u00e4sst sich finden, indem die Extremstellen von </p> \\[ L(x,\\lambda) = f(x) + \\lambda g(x) \\] <p>gefunden werden. Man beachte, dass \\(x\\) mehrdimensional sein kann. Sollte es mehrere Bedingungen geben, k\u00f6nnen diese, mit weitern Faktoren der Funktion \\(L\\) hinzugef\u00fcgt werden.</p> <p>Das finden des Extremstellen von \\(L\\) erfolgt durch das gleichsetzten der partiellen Ableitungen von  \\(L\\) mit \\(0\\).</p> <p>Beispiel f\u00fcr Lagrange</p> <p>Gesucht sind die Extrema der Funktion \\(f(x,y) = x+2y\\) unter den Nebenbedingung \\(1 = x^2+y^2\\).</p> <p></p> <p>Aufstellen der Lagrange-Funktion</p> \\[ L(x, y, \\lambda) = x + 2y + \\lambda (1 - x^2 - y^2). \\] <p>Berechnung der Ableitungen</p> <p>Wir bestimmen die Ableitungen nach \\( x \\), \\( y \\) und \\( \\lambda \\):</p> \\[ \\frac{\\partial L}{\\partial x} = 1 - 2\\lambda x = 0 \\] \\[ \\frac{\\partial L}{\\partial y} = 2 - 2\\lambda y = 0 \\] \\[ \\frac{\\partial L}{\\partial \\lambda} = 1 - x^2 - y^2 = 0. \\] <p>L\u00f6sen des Gleichungssystems</p> <p>Aus der ersten Gleichung folgt:</p> \\[ 1 = 2\\lambda x \\Rightarrow \\lambda = \\frac{1}{2x}. \\] <p>Aus der zweiten Gleichung:</p> \\[ 2 = 2\\lambda y \\Rightarrow \\lambda = \\frac{1}{y}. \\] <p>Setzen wir beide Gleichungen gleich:</p> \\[ \\frac{1}{2x} = \\frac{1}{y} \\Rightarrow y = 2x. \\] <p>Setzen wir dies in die Nebenbedingung ein:</p> \\[ 1 = x^2 + (2x)^2 \\] \\[ 1 = x^2 + 4x^2 = 5x^2 \\] \\[ x^2 = \\frac{1}{5}, \\quad x = \\pm \\frac{1}{\\sqrt{5}}. \\] <p>Daraus folgt:</p> \\[ y = 2x = \\pm \\frac{2}{\\sqrt{5}}. \\] <p>Werte der Zielfunktion</p> <p>Die Funktionswerte an den kritischen Punkten:</p> \\[ f\\left(\\frac{1}{\\sqrt{5}}, \\frac{2}{\\sqrt{5}}\\right) = \\frac{1}{\\sqrt{5}} + 2 \\cdot \\frac{2}{\\sqrt{5}} = \\frac{5}{\\sqrt{5}} = \\sqrt{5}. \\] \\[ f\\left(-\\frac{1}{\\sqrt{5}}, -\\frac{2}{\\sqrt{5}}\\right) = -\\frac{1}{\\sqrt{5}} - 2 \\cdot \\frac{2}{\\sqrt{5}} = -\\frac{5}{\\sqrt{5}} = -\\sqrt{5}. \\] <p>Ergebnis Das Maximum von \\( f(x,y) \\) ist \\( \\sqrt{5} \\) bei \\( \\left(\\frac{1}{\\sqrt{5}}, \\frac{2}{\\sqrt{5}}\\right) \\). Das Minimum von \\( f(x,y) \\) ist \\( -\\sqrt{5} \\) bei \\( \\left(-\\frac{1}{\\sqrt{5}}, -\\frac{2}{\\sqrt{5}}\\right) \\).</p> <p>Info</p> <p>Wir untersuchen hier nur, ob ein Extremwert vorliegt. Ob dieser dann ein Hoch- oder Tiefpunkt ist,  l\u00e4sst sich nur unter Bestimmung der zweiten Ableitungen und dem Aufstellen und Untersuchen der Hesse-Matrix feststellen. Dies ist aber f\u00fcr uns im Rahmen von PCA nicht relevant und wird daher gekonnt ignoriert.</p> Extremwerte unter Nebenbedingungen <p>Untersuche die Funktion </p> \\[ z = x^2 + 2xy \\] <p>auf Extremwerte unter der Nebenbedingung </p> \\[ y = -1.5x + 6. \\] <p>Verwende die Methode der Lagrange-Multiplikatoren.</p> <p>Erstelle gerne eine Visualisierung in Geogebra f\u00fcr das Problem.</p> L\u00f6sung <p>Lagrange-Funktion aufstellen</p> \\[ L(x, y, \\lambda) = x^2 + 2xy + \\lambda (-1.5x - y + 6). \\] <p>Berechnung der Ableitungen:</p> \\[ \\frac{\\partial L}{\\partial x} = 2x + 2y - 1.5\\lambda = 0 \\] \\[ \\frac{\\partial L}{\\partial y} = 2x - \\lambda = 0 \\] \\[ \\frac{\\partial L}{\\partial \\lambda} = -1.5x - y + 6 = 0 \\] <p>Gleichungssystem l\u00f6sen:</p> <p>Setzen von \\(\\lambda = 2x\\) in die erste Gleichung:</p> \\[ 2x + 2y - 1.5(2x) = 0 \\] \\[ 2x + 2y - 3x = 0 \\] \\[ -x + 2y = 0 \\] \\[ y = \\frac{x}{2} \\] <p>Einsetzen in die Nebenbedingung:</p> \\[ -1.5x - \\frac{x}{2} + 6 = 0 \\] \\[ -3x - x + 12 = 0 \\] \\[ -4x + 12 = 0 \\Rightarrow x = 3 \\] \\[ y = \\frac{3}{2} = 1.5 \\] <p>Der Extrempunkt befindet sich bei \\((3, 1.5)\\).</p> <p></p> Extremwerte einer Funktion mit quadratischer Nebenbedingung <p>Bestimme die Extremstellen der Funktion </p> \\[ f(x,y) = 3xy \\] <p>unter der Nebenbedingung </p> \\[ x^2 + y^2 = 18. \\] <p>Verwende die Methode der Lagrange-Multiplikatoren.</p> <p>Erstelle gerne eine Visualisierung in Geogebra f\u00fcr das Problem.</p> L\u00f6sung <p>Lagrange-Funktion aufstellen</p> \\[ L(x, y, \\lambda) = 3xy + \\lambda (18 - x^2 - y^2). \\] <p>Berechnung der Ableitungen:</p> \\[ \\frac{\\partial L}{\\partial x} = 3y - 2\\lambda x = 0 \\] \\[ \\frac{\\partial L}{\\partial y} = 3x - 2\\lambda y = 0 \\] \\[ \\frac{\\partial L}{\\partial \\lambda} = 18 - x^2 - y^2 = 0 \\] <p>Gleichungssystem l\u00f6sen:</p> <p>Teilen der ersten durch die zweite Gleichung:</p> \\[ \\frac{3y}{3x} = \\frac{2\\lambda x}{2\\lambda y} \\] \\[ \\frac{y}{x} = \\frac{x}{y} \\] \\[ y^2 = x^2 \\Rightarrow y = \\pm x \\] <p>Einsetzen in die Nebenbedingung:</p> \\[ x^2 + x^2 = 18 \\] \\[ 2x^2 = 18 \\Rightarrow x^2 = 9 \\Rightarrow x = \\pm 3, y = \\pm 3 \\] <p>Ermittlung der Extremstellen:</p> <p>Die L\u00f6sungen sind:</p> \\[ (3,3), (-3,-3), (-3,3), (3,-3) \\] <p>Diese Punkte sind m\u00f6gliche Extremstellen.</p> <p></p> Extremwerte einer Funktion mit mehreren Variablen <p>Bestimme die Extremstellen der Funktion </p> \\[ f(x,y,z) = x^2 y^2 z^2 \\] <p>unter der Nebenbedingung </p> \\[ 18 - 2x - 2y - 2z = 0. \\] <p>Verwende die Methode der Lagrange-Multiplikatoren.</p> L\u00f6sung <p>Lagrange-Funktion aufstellen</p> \\[ L(x, y, z, \\lambda) = x^2 y^2 z^2 + \\lambda (18 - 2x - 2y - 2z). \\] <p>Berechnung der Ableitungen:</p> \\[ \\frac{\\partial L}{\\partial x} = 2xy^2z^2 - 2\\lambda = 0 \\] \\[ \\frac{\\partial L}{\\partial y} = 2x^2yz^2 - 2\\lambda = 0 \\] \\[ \\frac{\\partial L}{\\partial z} = 2x^2y^2z - 2\\lambda = 0 \\] \\[ \\frac{\\partial L}{\\partial \\lambda} = 18 - 2x - 2y - 2z = 0 \\] <p>Gleichungssystem l\u00f6sen:</p> <p>Da die ersten drei Gleichungen eine \u00e4hnliche Struktur haben, setzen wir sie gleich:</p> \\[ 2xy^2z^2 = 2x^2yz^2 = 2x^2y^2z = 2\\lambda \\] <p>Dies impliziert, dass</p> \\[ x = y = z \\] <p>Einsetzen in die Nebenbedingung:</p> \\[ 18 - 2x - 2x - 2x = 0 \\] \\[ 18 - 6x = 0 \\] \\[ x = 3, y = 3, z = 3 \\] <p>Bestimmung des Lagrange-Multiplikators:</p> \\[ \\lambda = 2(3)(3)^2(3)^2 = 243 \\] <p>Ergebnis:</p> <p>Der Extrempunkt ist </p> \\[ (3,3,3) \\] <p>mit dem Lagrange-Multiplikator </p> \\[ \\lambda = 243. \\] Extremwerte einer Funktion mit einer Nebenbedingung <p>Untersuche die Funktion </p> \\[ z = 4x^3 + xy - y + 2 \\] <p>auf Extremwerte unter der Nebenbedingung </p> \\[ y = xy + 3x. \\] <p>Verwende die Methode der Lagrange-Multiplikatoren.</p> <p>Erstelle gerne eine Visualisierung in Geogebra f\u00fcr das Problem.</p> L\u00f6sung <p>Lagrange-Funktion aufstellen</p> \\[ L(x,y,\\lambda) = 4x^3 + xy - y + 2 + \\lambda(y - xy - 3x). \\] <p>Berechnung der Ableitungen:</p> \\[ \\frac{\\partial L}{\\partial x} = 12x^2 + y - \\lambda(y + 3) = 0 \\] \\[ \\frac{\\partial L}{\\partial y} = x - 1 + \\lambda(1 - x) = 0 \\] \\[ \\frac{\\partial L}{\\partial \\lambda} = y - xy - 3x = 0 \\] <p>Gleichungssystem l\u00f6sen:</p> <p>Aus der zweiten Gleichung folgt:</p> \\[ x - 1 + \\lambda - \\lambda x = 0 \\] \\[ x - \\lambda x + \\lambda - 1 = 0 \\] \\[ x(1 - \\lambda) = 1 - \\lambda \\] <p>Falls \\(1 - \\lambda \\neq 0\\), dann folgt \\(x = 1\\).</p> <p>Einsetzen in die dritte Gleichung:</p> \\[ y - 1 \\cdot y - 3 \\cdot 1 = 0 \\] <p>$$ y - y - 3 = 0 \\Rightarrow -3 = 0 $$, was ein Widerspruch ist.</p> <p>Daher muss \\(1 - \\lambda = 0\\) gelten, also \\(\\lambda = 1\\).</p> <p>Setzen von \\(\\lambda = 1\\) in die erste Gleichung:</p> \\[ 12x^2 + y - 1(y + 3) = 0 \\] \\[ 12x^2 + y - y - 3 = 0 \\] \\[ 12x^2 - 3 = 0 \\] \\[ 12x^2 = 3 \\] \\[ x^2 = \\frac{3}{12} = \\frac{1}{4} \\] \\[ x = \\pm \\frac{1}{2} \\] <p>Einsetzen in die Nebenbedingung:</p> <p>F\u00fcr \\(x = \\frac{1}{2}\\):</p> \\[ y = \\frac{1}{2} y + 3 \\cdot \\frac{1}{2} \\] \\[ y - \\frac{1}{2} y = \\frac{3}{2} \\] \\[ \\frac{1}{2} y = \\frac{3}{2} \\] \\[ y = 3 \\] <p>F\u00fcr \\(x = -\\frac{1}{2}\\):</p> \\[ y = -\\frac{1}{2} y + 3 \\cdot (-\\frac{1}{2}) \\] \\[ y + \\frac{1}{2} y = -\\frac{3}{2} \\] \\[ \\frac{3}{2} y = -\\frac{3}{2} \\] \\[ y = -1 \\] <p>Bestimmung der Extremwerte durch zweite Ableitung:</p> <p>Berechnung von \\(z''\\) f\u00fcr die erhaltenen Werte:</p> \\[ z''(0.5) &gt; 0 \\Rightarrow \\text{ Minimum bei } (0.5,3) \\] \\[ z''(-0.5) &lt; 0 \\Rightarrow \\text{ Maximum bei } (-0.5,-1) \\] <p>Ergebnis:</p> <p>Minimum bei \\((0.5,3)\\), Maximum bei \\((-0.5,-1)\\).</p> <p></p>"},{"location":"content/PCA/3_lagrange/#verwendung-von-lagrange-fur-die-herleitung-von-pca","title":"Verwendung von Lagrange f\u00fcr die Herleitung von PCA","text":"<p>Wenn wir nun die Lagrange-Methode verwenden wollen, um PCA herzuleiten, m\u00fcssen wir uns zun\u00e4chst klar machen, dass </p> \\[ v^t S v \\] <p>die Zielfunktion und </p> \\[ |v| = 1 = v^t v \\] <p>die Nebenbedingung ist.</p> <p>Das hei\u00dft wir m\u00fcssen die partielle Ableitung von</p> \\[ L(v, \\lambda) = v^t S v + \\lambda (1 - v^t v) \\] <p>bilden.</p>"},{"location":"content/PCA/3_lagrange/#ableitung-von-xtax","title":"Ableitung von \\(x^tAx\\)","text":"<p>Beispiel</p> <p>Gegeben sei die Matrix</p> \\[ A = \\begin{pmatrix} 1 &amp; 0 \\\\ -1 &amp; 2  \\end{pmatrix} \\] <p>und der Vektor </p> \\[ x = \\begin{pmatrix} x_1 \\\\ x_2  \\end{pmatrix} \\] <p>Zun\u00e4chst berechnen wir \\(x^t A x\\):</p> \\[ \\begin{align} x^t A x &amp;= \\begin{pmatrix} x_1 &amp; x_2 \\end{pmatrix}\\begin{pmatrix} 1 &amp; 0 \\\\ -1 &amp; 2 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\\\         &amp;= \\begin{pmatrix} 1x_1 -1x_2 &amp; 2x_2 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\\\         &amp;= \\begin{pmatrix} (x_1 -x_2)x_1 + 2x_2\\cdot x_2 \\end{pmatrix} \\\\         &amp;= x_1^2 - x_1 x_2 + 2x_2^2 \\end{align} \\] <p>Die partiellen Ableitungen von \\(x^t A x\\) nach \\(x_1\\) und \\(x_2\\) sind:</p> \\[ \\frac{\\partial x^t A x}{\\partial x} =  \\begin{pmatrix} \\frac{\\partial x^t A x}{\\partial x_1} \\\\ \\frac{\\partial x^t A x}{\\partial x_2}  \\end{pmatrix} =  \\begin{pmatrix} 2x_1 - x_2 \\\\ 4x_2 - x_1  \\end{pmatrix} \\] Matrix-Vektor-Multiplikation durchschauen <p>Gegeben sei die Matrix</p> \\[ A = \\begin{pmatrix} 2 &amp; 1 \\\\ -3 &amp; 4  \\end{pmatrix} \\] <p>und der Vektor </p> \\[ x = \\begin{pmatrix} x_1 \\\\ x_2  \\end{pmatrix} \\] <p>Multipliziere einmal vollst\u00e4ndig das Produkt \\(x^t A x\\) aus. </p> <p>Bilde dann die die partiellen Ableitung \\(x^t A x\\) nach \\(x_1\\), \\(x_2\\) und \\(x_3\\).</p> <p>Notieren Sie</p> \\[ \\frac{\\partial x^t A x}{\\partial x} =  \\begin{pmatrix} \\frac{\\partial x^t A x}{\\partial x_1} \\\\ \\frac{\\partial x^t A x}{\\partial x_2}  \\end{pmatrix} \\] Tipp <p>Dabei erh\u00e4lst du ein Polynom mit zwei Unbekannten.</p> L\u00f6sung <p>Wir berechnen zun\u00e4chst \\(x^t A\\):</p> \\[ x^t A  = \\begin{pmatrix}  x_1 &amp;  x_2  \\end{pmatrix} \\begin{pmatrix}  2 &amp; 1 \\\\  -3 &amp; 4  \\end{pmatrix} \\begin{pmatrix}  x_1 \\\\  x_2  \\end{pmatrix} =  \\begin{pmatrix}  2 x_1 - 3 x_2 &amp; 1x_1 + 4x_2  \\end{pmatrix} \\begin{pmatrix}  x_1 \\\\  x_2  \\end{pmatrix} = (2 x_1 - 3 x_2) x_1 + (1x_1 + 4x_2) x_2 = 2x_1^2 -2 x_1 x_2 + 4x_2^2 \\] <p>Wir k\u00f6nnen nun die Ableitungen bestimmen:</p> \\[ \\frac{\\partial x^t A x}{\\partial x_1} = 4x_1 - 2x_2,\\quad \\frac{\\partial x^t A x}{\\partial x_2} = -2x_1 + 8x_2  \\] <p>Insgesamt k\u00f6nnen wir notieren:</p> \\[ \\frac{\\partial x^t A x}{\\partial x} = \\begin{pmatrix}  4x_1 - 2x_2 \\\\  -2x_1 + 8x_2  \\end{pmatrix} \\] Matrix-Vektor-Multiplikation mit symmetrischer Matrix <p>Gegeben sei die symmetrische Matrix</p> \\[ A = \\begin{pmatrix} 4 &amp; 2 &amp; -1 \\\\ 2 &amp; 3 &amp; 0 \\\\ -1 &amp; 0 &amp; 5 \\end{pmatrix} \\] <p>und der Vektor </p> \\[ x = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} \\] <p>Multipliziere einmal vollst\u00e4ndig das Produkt \\( x^t A x \\) aus. </p> <p>Bilde dann die partiellen Ableitungen \\( \\frac{\\partial x^t A x}{\\partial x_1} \\), \\( \\frac{\\partial x^t A x}{\\partial x_2} \\) und \\( \\frac{\\partial x^t A x}{\\partial x_3} \\).</p> <p>Sie l\u00e4sst sich </p> \\[ \\frac{\\partial x^t A x}{\\partial x} =  \\begin{pmatrix} \\frac{\\partial x^t A x}{\\partial x_1} \\\\ \\frac{\\partial x^t A x}{\\partial x_2} \\\\ \\frac{\\partial x^t A x}{\\partial x_3}  \\end{pmatrix} \\] <p>k\u00fcrzer ausdr\u00fccken?</p> Tipp <p>Die Matrix ist symmetrisch, sodass sich die Terme beim Multiplizieren vereinfacht addieren lassen.</p> L\u00f6sung <p>Wir berechnen zun\u00e4chst \\( x^t A x \\):</p> \\[ x^t A x = \\begin{pmatrix} x_1 &amp; x_2 &amp; x_3 \\end{pmatrix} \\begin{pmatrix} 4 &amp; 2 &amp; -1 \\\\ 2 &amp; 3 &amp; 0 \\\\ -1 &amp; 0 &amp; 5 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} \\] <p>Zun\u00e4chst berechnen wir das Zwischenprodukt \\( A x \\):</p> \\[ A x = \\begin{pmatrix} 4x_1 + 2x_2 - x_3 \\\\ 2x_1 + 3x_2 + 0x_3 \\\\ -x_1 + 0x_2 + 5x_3 \\end{pmatrix} \\] <p>Jetzt folgt die Multiplikation mit \\( x^t \\):</p> \\[ x^t A x = x_1 (4x_1 + 2x_2 - x_3) + x_2 (2x_1 + 3x_2 + 0x_3) + x_3 (-x_1 + 0x_2 + 5x_3) \\] \\[ = 4x_1^2 + 2x_1x_2 - x_1x_3 + 2x_1x_2 + 3x_2^2 + 0x_2x_3 - x_1x_3 + 0x_2x_3 + 5x_3^2 \\] \\[ = 4x_1^2 + 4x_1x_2 - 2x_1x_3 + 3x_2^2 + 5x_3^2 \\] <p>Nun berechnen wir die partiellen Ableitungen:</p> \\[ \\frac{\\partial x^t A x}{\\partial x_1} = 8x_1 + 4x_2 - 2x_3 \\] \\[ \\frac{\\partial x^t A x}{\\partial x_2} = 4x_1 + 6x_2 \\] \\[ \\frac{\\partial x^t A x}{\\partial x_3} = -2x_1 + 10x_3 \\] <p>Insgesamt k\u00f6nnen wir die Ableitungen als Gradientenvektor notieren:</p> \\[ \\frac{\\partial x^t A x}{\\partial x} = \\begin{pmatrix} 8x_1 + 4x_2 - 2x_3 \\\\ 4x_1 + 6x_2 \\\\ -2x_1 + 10x_3 \\end{pmatrix} = 2Ax \\] Video <p> </p> <p>Ableitung von \\(x^t A x\\)</p> <p>Es seien \\(x\\in \\mathbb{R}^D\\) und \\(A\\in \\mathbb{R}^{D\\times D}\\) eine Matrix. Dann ist</p> \\[ x^t A x = x^t \\begin{pmatrix} \\sum a_{1i} x_i \\\\ \\vdots\\\\ \\sum a_{Di} x_i \\end{pmatrix}         = \\sum_i \\sum_j a_{ij}x_i x_j \\] <p>Und die partiellen Ableitungen hat dann die Form:</p> \\[  \\begin{equation}    \\frac{\\partial (x^t A x)_{ij}}{\\partial x_k}      = \\frac{\\partial \\sum_i \\sum_j a_{ij}x_i x_j}{\\partial x_k} =     \\begin{cases}      0 &amp; \\text{f\u00fcr } j \\neq k \\neq i \\\\      a_{ik}x_i &amp; \\text{f\u00fcr } j = k \\neq i \\\\      a_{kj}x_j &amp; \\text{f\u00fcr } j \\neq k = i \\\\      2a_{kk}x_k &amp; \\text{f\u00fcr } j = k = i     \\end{cases} \\end{equation} \\] <p>Ableitung von \\(x^t A x\\) bei symetrischen Matrizen</p> <p>Es seien \\(x\\in \\mathbb{R}^D\\) und \\(A\\in \\mathbb{R}^{D\\times D}\\) eine symetrische Matrix. Dann ist</p> \\[ \\frac{\\partial x^t A x}{\\partial x} = 2Ax \\] <p>Wir k\u00f6nnen nun also Lagrange ausf\u00fchren. Wir leiten </p> \\[ L(v, \\lambda) = v^t S v + \\lambda (1 - v^t v) \\] <p>nach \\(v\\) ab und setzen den Term gleich \\(0\\): </p> \\[ \\frac{\\partial L(v,\\lambda)}{\\partial v} = 2Sv - 2\\lambda v = 0 \\] <p>Wir k\u00f6nnen den Term umstellen zu:</p> \\[\\begin{align}                 &amp;2Sv - 2\\lambda v = 0 \\\\ \\Leftrightarrow &amp;Sv - \\lambda v = 0 \\\\ \\Leftrightarrow &amp;Sv = \\lambda v \\\\ \\Leftrightarrow &amp;v^t S v = \\lambda v^t v \\overset{|v|=1}{=} \\lambda \\end{align}\\] <p>Wir sehen also in der letzten Zeile, dass unsere urspr\u00fcngliche Funktion \\(v^t S v\\) gleich \\(\\lambda\\) ist. Das hei\u00dft um \\(v^t S v\\) zu maximieren, hei\u00dft \\(\\lambda\\) zu maximieren.</p> <p>Aber \\(\\lambda\\) ist doch blo\u00df irgendeine beliebige, reelle Zahl, oder? Kann sie nicht beliebig gro\u00df werden? Nein, denn die vorletzte Gleichung \\(Sv = \\lambda v\\) bedeutet, dass \\(\\lambda\\) zus\u00e4tzlich noch ein Eigenwert sein muss. Und von diesen gibt es nur eine endliche Anzahl, wie wir im folgenden Kapitel sehen werden. </p> <p>Das hei\u00dft, wir k\u00f6nnen unseren gesuchten Varianz-maximierenden Projektionsvektor \\(v\\) finden, indem wir den gr\u00f6\u00dften Eigenwert \\(\\lambda\\) von \\(S\\) finden. What a crazy turn of events!</p>"},{"location":"content/PCA/4_eigenvektor/","title":"Eigenvektoren und Eigenwerte","text":"<p>Die n\u00e4chste zentrale Definition ist die von Eigenwerten und Eigenvektoren eines Endomorphismus eines Vektorraums.  </p> <p>Eigenwert</p> <p>Sei \\( f: V \\to V \\) ein Endomorphismus. Ein \\( \\lambda \\in K \\) hei\u00dft Eigenwert von \\( f \\), wenn es einen Vektor \\( v \\neq 0 \\) gibt mit \\( f(v) = \\lambda v \\). </p> <p>Ein solcher Vektor hei\u00dft dann ein Eigenvektor von \\( f \\) zum Eigenwert \\( \\lambda \\).</p> <p>Eigenvektor</p> <p>Ein Eigenvektor \\( v \\) bzgl. \\( f \\) ist ein Vektor, der nicht Null ist und der durch \\( f \\) um einen Faktor \\( \\lambda \\), den Eigenwert, gestreckt wird. Wir definieren:</p> \\[ E(f, \\lambda) = \\{ v \\in V \\mid f(v) = \\lambda v \\} \\] <p>f\u00fcr alle \\( \\lambda \\in K \\).</p> <p>Dies ist ein Untervektorraum von \\( V \\). Per Definition ist \\( \\lambda \\in K \\) ein Eigenwert von \\( f \\), wenn es einen Vektor \\( v \\neq 0 \\) gibt, der zu \\( E(f, \\lambda) \\) geh\u00f6rt. \\( E(f, \\lambda) = \\{ v \\in V \\mid f(v) = \\lambda v \\} \\) ist ein Untervektorraum von \\( V \\).</p> <p>Bild zu Eigenvektoren</p> <p></p>"},{"location":"content/PCA/4_eigenvektor/#berechnen-von-eigenwerten-und-eigenvektoren","title":"Berechnen von Eigenwerten und Eigenvektoren","text":"<p>Nach Definition muss \\( f(v) = \\lambda v \\) sein. Das bedeutet konkret (A ist eine Matrix):</p> \\[ A v = \\lambda v. \\] <p>Dies l\u00e4sst sich auch umschreiben, mit \\( E \\) der Einheitsmatrix:</p> \\[ A v = \\lambda E v \\] <p>Das l\u00e4sst sich dann umformen zu:</p> \\[ (A - \\lambda E)v = 0 \\] <p>Bestimmung von Eigenwerten</p> <p>Um nun den Eigenwert zu berechnen, l\u00f6st man diese Gleichung. Da \\( v \\neq 0 \\) vorausgesetzt wird, folgt, dass es nur dann l\u00f6sbar ist, wenn \\( (A - \\lambda E) \\) einen nicht trivialen Kern hat (also keinen Kern \\( \\neq 0 \\)). Das bedeutet wiederum, dass die Determinante 0 sein muss:</p> \\[ \\det(A - \\lambda E) = 0. \\] <p>Diese Determinante nennt man das charakteristische Polynom. Die Nullstellen dieses Polynoms sind die Eigenwerte.</p> <p>Bestimmung von Eigenvektoren</p> <p>Zur Bestimmung der Eigenvektoren setzt man den Eigenvektor in die Gleichung:</p> \\[ (A - \\lambda E)v = 0 \\] <p>ein, anstelle des \\( \\lambda \\), und erh\u00e4lt so ein Gleichungssystem, das man l\u00f6sen kann. Die L\u00f6sung dieses Gleichungssystems ist dann der Eigenvektor bzw. die Eigenvektoren.</p> <p>Beispiel: Bestimmung von Eigenwerten</p> <p>Wir bestimmen mal die Eigenwerte der folgenden Matrix:</p> \\[ A = \\begin{pmatrix} 0 &amp; 2 &amp; -1 \\\\ 2 &amp; -1 &amp; 1 \\\\ 2 &amp; -1 &amp; 3 \\end{pmatrix} \\] <p>Setzt man diese in die Gleichung \\( (A - \\lambda E) = 0 \\) ein, dann erhaltet ihr:</p> \\[ A - \\lambda E = \\begin{pmatrix} 0 - \\lambda &amp; 2 &amp; -1 \\\\ 2 &amp; -1 - \\lambda &amp; 1 \\\\ 2 &amp; -1 &amp; 3 - \\lambda \\end{pmatrix} =0 \\] <p>Dann berechnet ihr die Determinante dazu:</p> \\[ \\begin{aligned} \\det(A - \\lambda E) &amp;= (0 - \\lambda)(-1 - \\lambda)(3 - \\lambda) + 4 + 2 - (2\\lambda + 2 + \\lambda + 12 - 4\\lambda) \\\\ &amp;= -\\lambda^3 + 2\\lambda^2 + 4\\lambda - 8 \\\\ &amp;= -(\\lambda - 2)(\\lambda - 2)(\\lambda + 2) = 0 \\end{aligned} \\] <p>Die Nullstellen des Polynoms sind dann die Eigenwerte. Also in diesem Fall \\( \\lambda_{1,2} = 2 \\) und \\( \\lambda_3 = -2 \\).</p> <p>Beispiel: Bestimmung von Eigenvektoren</p> <p>Jetzt geht es weiter mit den Eigenvektoren. Dazu setzen wir die Eigenwerte f\u00fcr \\( \\lambda \\) ein, zuerst \\( \\lambda = 2 \\):</p> \\[ A - 2E = \\begin{pmatrix} 0 - 2 &amp; 2 &amp; -1 \\\\ 2 &amp; -1 - 2 &amp; 1 \\\\ 2 &amp; -1 &amp; 3 - 2 \\end{pmatrix} = \\begin{pmatrix} -2 &amp; 2 &amp; -1 \\\\ 2 &amp; -3 &amp; 1 \\\\ 2 &amp; -1 &amp; 1 \\end{pmatrix} \\] <p>Dann muss man das folgende Gleichungssystem l\u00f6sen: </p> \\[ (A - \\lambda E) \\cdot v = 0 \\] \\[ (A - 2 E) \\cdot v_{2} = 0 \\] \\[ \\begin{pmatrix} -2 &amp; 2 &amp; -1 \\\\ 2 &amp; -3 &amp; 1 \\\\ 2 &amp; -1 &amp; 1 \\end{pmatrix} \\cdot v_{2} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\] <p>Man erh\u00e4lt durch Umformung: (z.B. Gau\u00df-Jordan-Algorithmus)</p> \\[ \\begin{pmatrix} 1 &amp; 0 &amp; \\frac{1}{2} \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{pmatrix} \\cdot v_{2} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\] <p>Dadurch ergibt sich dieses Gleichungssystem:</p> \\[ 1 \\cdot v_{2,1} + \\frac{1}{2} \\cdot v_{2,3} = 0  \\Rightarrow v_{2,1} = - \\frac{1}{2} \\cdot v_{2,3} \\] \\[ 1 \\cdot v_{2,2} = 0 \\] <p>Als m\u00f6glicher Vektor l\u00e4sst sich so ablesen:</p> \\[ v_{2} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ -1 \\end{pmatrix} \\] <p>Die Eigenvektoren sind dann alle Vielfachen dieses Vektors!</p> <p>F\u00fcr den Eigenwert \\( -2 \\) macht ihr das genauso:</p> \\[ A - (-2)E = A + 2E = \\begin{pmatrix} 0 + 2 &amp; 2 &amp; -1 \\\\ 2 &amp; -1 + 2 &amp; 1 \\\\ 2 &amp; -1 &amp; 3 + 2 \\end{pmatrix}     = \\begin{pmatrix} 2 &amp; 2 &amp; -1 \\\\ 2 &amp; 1 &amp; 1 \\\\ 2 &amp; -1 &amp; 5 \\end{pmatrix} \\] \\[ \\begin{pmatrix} 2 &amp; 2 &amp; -1 \\\\ 2 &amp; 1 &amp; 1 \\\\ 2 &amp; -1 &amp; 5 \\end{pmatrix} \\cdot v_{-2} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\] <p>So erhaltet ihr den zweiten Eigenvektor, n\u00e4mlich alle Vielfachen des Vektors:</p> \\[ v_{-2} = \\begin{pmatrix} \\frac{3}{2} \\\\ -2 \\\\ -1 \\end{pmatrix} \\]"},{"location":"content/PCA/4_eigenvektor/#aufgaben-zu-eigenwerten-und-eigenvektoren","title":"Aufgaben zu Eigenwerten und Eigenvektoren","text":"1) Eigenvektoren berechnen (2x2 Matrizen) <p>Berechne die Eigenvektoren der folgenden Matrizen:</p> <p>a) </p> \\[\\begin{pmatrix} 2 &amp; 2 \\\\ 0 &amp; 1 \\\\ \\end{pmatrix}\\] <p>b)</p> \\[\\begin{pmatrix} 5 &amp; 8 \\\\ 1 &amp; 3 \\\\ \\end{pmatrix}\\] <p>c)</p> \\[\\begin{pmatrix} 3 &amp; 0 \\\\ 8 &amp; -1 \\\\ \\end{pmatrix}\\] <p>d)</p> \\[\\begin{pmatrix} 2 &amp; 3 \\\\ 0 &amp; 2 \\\\ \\end{pmatrix}\\] L\u00f6sung <p>a) \\((2-\\lambda)(1-\\lambda)=0\\)</p> <p>\\(\\lambda_{1}=1\\) und \\(\\lambda_{2}=2\\)</p> <p>\\(v_{1}= \\begin{pmatrix}  -2 \\\\  1 \\end{pmatrix}\\) und  \\(v_{2}= \\begin{pmatrix}   1 \\\\  0 \\end{pmatrix}\\)</p> <p>b) \\(\\lambda^2 - 8\\cdot \\lambda +7=0\\)</p> <p>\\(\\lambda_{1}=1\\) und \\(\\lambda_{2}=7\\)</p> <p>\\(v_{1}= \\begin{pmatrix}  -2 \\\\  1 \\end{pmatrix}\\) und  \\(v_{2}= \\begin{pmatrix}   4 \\\\  1 \\end{pmatrix}\\) </p> <p>c) \\((3-\\lambda)\\cdot (-1-\\lambda)=0\\)</p> <p>\\(\\lambda_{1}=3\\) und \\(\\lambda_{2}=-1\\)</p> <p>\\(v_{1}= \\begin{pmatrix}  \\frac{1}{2} \\\\  1 \\end{pmatrix}\\) und  \\(v_{2}= \\begin{pmatrix}   0 \\\\  1 \\end{pmatrix}\\) </p> <p>d) \\((2-\\lambda)^2=0\\)</p> <p>\\(\\lambda_{1}=2\\)</p> <p>\\(v_{1}= \\begin{pmatrix}  1 \\\\  0 \\end{pmatrix}\\)</p> 2) Eigenvektoren - Sonderf\u00e4lle (2x2 Matrizen) <p>Berechne die Eigenvektoren der folgenden Matrizen:</p> <p>a) Scherung (in x-Richtung)</p> \\[\\begin{pmatrix} 1 &amp; 3 \\\\ 0 &amp; 1 \\\\ \\end{pmatrix}\\] <p>b) Rotation (um 90\u00b0)</p> \\[\\begin{pmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\\\ \\end{pmatrix}\\] <p>c) Skalierung (um Faktor 2)</p> \\[\\begin{pmatrix} 2 &amp; 0 \\\\ 0 &amp; 2 \\\\ \\end{pmatrix}\\] L\u00f6sung <p>a) Scherung </p> <p>\\((1-\\lambda)^2=0\\)</p> <p>\\(\\lambda_{1}=1\\) </p> <p>\\(v_{1}= \\begin{pmatrix}  1 \\\\  0 \\end{pmatrix}\\) </p> <p>Die Eigenvektoren liegen alle in x-Richtung, da die Matrix in x-Richtung schert.</p> <p>b) Rotation</p> <p>\\(\\lambda^2 + 1 = 0\\)</p> <p>Keine reellen L\u00f6sungen f\u00fcr \\(\\lambda\\), also gibt es keinen Eigenvektor. (Die Richtung kann ja auch nicht gleich bleiben, da sie immmer um 90\u00b0 ge\u00e4ndert wird.)</p> <p>c) Skalierung</p> <p>\\((2-\\lambda)^2=0\\)</p> <p>\\(\\lambda_{1}=2\\)</p> \\[ \\begin{pmatrix} 0 &amp; 0 \\\\ 0 &amp; 0 \\\\ \\end{pmatrix} \\cdot v_{1} = \\begin{pmatrix} 0 \\\\ 0  \\end{pmatrix} \\] <p>Die Gleichung enth\u00e4lt alle 2-dimensionalen Vektoren als L\u00f6sung, d.h. es gibt unendlich viele Eigenvektoren und ihre Richtung ist beliebig. (Da die Richtung immer beibehalten wird, und lediglich die L\u00e4nge des Vektors wird verdoppelt.)</p> 3) Eigenvektoren berechnen (3x3 Matrizen) <p>Berechne die Eigenvektoren der folgenden Matrizen:</p> <p>a)</p> \\[\\begin{pmatrix} 1 &amp; 2 &amp; -1 \\\\ 0 &amp; 3 &amp; 0 \\\\ -1 &amp; 2 &amp; 1 \\end{pmatrix}\\] <p>b)</p> \\[\\begin{pmatrix} 2 &amp; 1 &amp; -2 \\\\ 1 &amp; 2 &amp; 2 \\\\ -2 &amp; 2 &amp; -1 \\end{pmatrix}\\] <p>c)</p> \\[\\begin{pmatrix} 2 &amp; -1 &amp; 2 \\\\ -1 &amp; 10 &amp; -2 \\\\ 2 &amp; -2 &amp; 5 \\end{pmatrix}\\] L\u00f6sung <p>a) \\(\\lambda \\cdot (\\lambda -2) \\cdot (3-\\lambda)=0\\)</p> <p>\\(\\lambda_{1}=0\\) und \\(\\lambda_{2}=2\\) und \\(\\lambda_{3}=3\\)</p> <p>\\(v_{1}= \\begin{pmatrix}  1 \\\\  0 \\\\ 1 \\end{pmatrix}\\) und  \\(v_{2}= \\begin{pmatrix}   -1 \\\\  0 \\\\ 1 \\end{pmatrix}\\) und  \\(v_{3}= \\begin{pmatrix}   2 \\\\  3 \\\\ 2 \\end{pmatrix}\\)</p> <p>b) \\((\\lambda -3) \\cdot (-\\lambda^2+9)=0\\)</p> <p>\\(\\lambda_{1}=-3\\) und \\(\\lambda_{2}=3\\) und \\(\\lambda_{3}=3\\)</p> <p>\\(v_{1}= \\begin{pmatrix}  \\frac{1}{2} \\\\  -\\frac{1}{2} \\\\ 1 \\end{pmatrix}\\) und  \\(v_{2}= \\begin{pmatrix}   1 \\\\  1 \\\\ 0 \\end{pmatrix}\\) und  \\(v_{3}= \\begin{pmatrix}   -2 \\\\  0 \\\\ 1 \\end{pmatrix}\\)</p> <p>c) \\((5- \\lambda) \\cdot (\\lambda^2 -12 \\cdot \\lambda + 11)=0\\)</p> <p>\\(\\lambda_{1}=5\\) und \\(\\lambda_{2}=1\\) und \\(\\lambda_{3}=11\\)</p> <p>\\(v_{1}= \\begin{pmatrix}  1 \\\\  1 \\\\ 2 \\end{pmatrix}\\) und  \\(v_{2}= \\begin{pmatrix}   -2 \\\\  0 \\\\ 1 \\end{pmatrix}\\) und  \\(v_{3}= \\begin{pmatrix}   -1 \\\\  5 \\\\ -2 \\end{pmatrix}\\)</p> Eigenwerte mit Python finden <p>Finde heraus, wie man mit <code>numpy</code> Eigenwerte und Vektoren bestimmen kann.</p> L\u00f6sung <p>Nutze z.B. <code>numpy.linalg.eig</code> f\u00fcr die Bestimmung der Eigenwerte und Vektoren.</p>"},{"location":"content/PCA/4_eigenvektor/#begriffe-und-charakteristisches-polynom","title":"Begriffe und charakteristisches Polynom","text":"<p>Kurze \u00dcbersicht/Wiederholung wichtiger Begriffe f\u00fcr die Herleitung</p> <p>Determinante: Gibt an, wie sich das Volumen bei der durch die Matrix beschriebenen linearen Abbildung \u00e4ndert.</p> <p>Eigenvektor: Der Eigenvektor einer Abbildung ist ein vom Nullvektor verschiedener Vektor, dessen Richtung durch die Abbildung nicht ver\u00e4ndert wird.</p> <p>Sonderfall: F\u00fcr reelle symmetrische Matrizen gilt: Die Eigenvektoren zu verschiedenen Eigenwerten sind zueinander orthogonal.</p> <p>Eigenwert: Der Eigenwert ist der Skalierungsfaktor eines Eigenvektors.</p> <p>injektiv: Eine Funktion ist injektiv, wenn es zu jedem Element y der Zielmenge Y h\u00f6chstens ein Element x der Ausgangs- oder Definitionsmenge X gibt, das darauf abgebildet wird.</p> <p>invertierbar:  Eine Funktion ist genau dann invertierbar , wenn sie bijektiv (also gleichzeitig injektiv und surjektiv) ist. Eine quadratische Matrix ist genau dann invertierbar, wenn ihre Determinante ungleich null ist.</p> <p>Herleitung: Eigenwerte \\(\\lambda\\) erf\u00fcllen \\( \\det(\\lambda E - A) = 0 \\)</p> <p>Das charakteristische Polynom ( \\( \\det(\\lambda E - A) \\) ) spielt eine wichtige Rolle bei der Bestimmung der Eigenwerte einer Matrix, denn die Eigenwerte sind genau die Nullstellen des charakteristischen Polynoms. Auch wenn man zum expliziten Berechnen des charakteristischen Polynoms immer eine Basis und damit eine Darstellungsmatrix ausw\u00e4hlt, h\u00e4ngen das Polynom wie auch die Determinante nicht von dieser Wahl ab.</p> <p>Um zu zeigen, dass die Eigenwerte gerade die Nullstellen des charakteristischen Polynoms sind, geht man folgenderma\u00dfen vor:</p> <p>Es sei \\(\\lambda \\in \\mathbb{K}\\) und \\(A\\) eine \\(n \\times n\\)-Matrix \u00fcber \\(\\mathbb{K}\\). Dann gelten die folgenden \u00c4quivalenzen:</p> \\[ \\lambda \\text{ ist ein Eigenwert von } A  \\] \\[ \\iff \\exists x \\in \\mathbb{K}^n, x \\neq 0 \\text{ mit } A x = \\lambda x \\] \\[ \\iff \\exists x \\in \\mathbb{K}^n, x \\neq 0 \\text{ mit } (\\lambda E - A) x = 0 \\] \\[ \\iff \\text{Der Kern von } \\lambda E - A \\text{ besteht nicht nur aus dem Nullvektor, d.h. } \\ker(\\lambda E - A) \\neq \\{ 0 \\} \\] \\[ \\text{(D.h., andere Vektoren als der Nullvektor werden auf den Nullvektor abgebildet.)} \\] \\[ \\iff \\text{Die durch } \\lambda E - A \\text{ induzierte lineare Abbildung ist nicht injektiv} \\] \\[progra \\iff \\lambda E - A \\text{ ist nicht invertierbar} \\] \\[ \\iff \\det(\\lambda E - A) = 0 \\] \\[ \\iff \\lambda \\text{ ist Nullstelle des charakteristischen Polynoms von } A \\]"},{"location":"content/PCA/4_eigenvektor/#orthogonale-eigenvektoren","title":"Orthogonale Eigenvektoren","text":"<p>Herleitung: Eigenvektoren von symmetrischen Matrizen sind orthogonal</p> <p>F\u00fcr zwei verschiedene Eigenvektoren \\(v\\) und \\(w\\) mit den Eigenwerten \\(\\lambda\\) und \\(\\mu\\) gilt:</p> \\[ A v = \\lambda v, \\quad A w = \\mu w. \\] <p>Wenn \\(A\\) symmetrisch (d.h. \\(A = A^t\\)) ist, folgt:</p> \\[ \\begin{array}{rrl} &amp; A^t &amp;=&amp; A \\\\ \\Leftrightarrow &amp; v^t A^t &amp;=&amp; v^t A \\\\ \\Leftrightarrow &amp; v^t A^t w &amp;=&amp; v^t A w \\\\ \\Leftrightarrow &amp; (A v)^t w &amp;=&amp; v^t A w \\\\ \\Leftrightarrow &amp; (\\lambda v)^t w &amp;=&amp; v^t \\mu w \\\\ \\Leftrightarrow &amp; \\lambda v^t w &amp;=&amp;  \\mu v^t w \\\\ \\Leftrightarrow &amp; \\lambda v^t w - \\mu v^t w &amp;=&amp; 0 \\\\ \\Leftrightarrow &amp; (\\lambda- \\mu) v^t w  &amp;=&amp; 0 \\\\ \\Leftrightarrow &amp; v^t w  &amp;=&amp; 0 \\\\ \\Leftrightarrow &amp; v &amp;\\perp&amp; w \\end{array} \\] <p>Da \\(\\lambda \\neq \\mu\\), muss \\(v^t w = 0\\) sein. Das ist dann \\(v \\perp w\\). </p> 4) Eigenvektoren von symmetrischen Matrizen berechnen <p>(i) Berechne die Eigenvektoren der folgenden Matrizen.</p> <p>(ii) Verifiziere rechnerisch, dass die Eigenvektoren orthogonal zueinander stehen.</p> <p>a)</p> \\[\\begin{pmatrix} 7 &amp; 4 \\\\ 4 &amp; 1 \\\\ \\end{pmatrix}\\] <p>b)</p> \\[\\begin{pmatrix} 2 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 2 \\end{pmatrix}\\] L\u00f6sung <p>a) \\(\\lambda_{1}=-1\\) und \\(\\lambda_{2}=9\\)</p> <p>\\(v_{1}= \\begin{pmatrix}  -\\frac{1}{2} \\\\  1 \\end{pmatrix}\\) und  \\(v_{2}= \\begin{pmatrix}   2 \\\\  1 \\end{pmatrix}\\)</p> <p>b) \\(\\lambda_{1}=0\\) und \\(\\lambda_{2}=2\\) und \\(\\lambda_{3}=3\\)</p> <p>\\(v_{1}= \\begin{pmatrix}  1 \\\\  -2 \\\\ 1 \\end{pmatrix}\\) und  \\(v_{2}= \\begin{pmatrix}   -1 \\\\  0 \\\\ 1 \\end{pmatrix}\\) und  \\(v_{3}= \\begin{pmatrix}   1 \\\\  1 \\\\ 1 \\end{pmatrix}\\)</p> <p></p> 5) Eigenvektoren von allgemeinen Matrizen auf Orthogonalit\u00e4t untersuchen. <p>Untersuche die Eigenvektoren aus Aufgabe 3 auf Orthogonalit\u00e4t. (W\u00e4hle dazu deine eigenen Vektoren oder die aus der L\u00f6sung.)</p>"},{"location":"content/PCA/4_eigenvektor/#mitschriften","title":"Mitschriften","text":"<p>Eigenwerte berechnen (Aufgabe 1)</p> <p></p> <p>Eigenvektoren berechnen (Aufgabe 1a)</p> <p></p> <p>Eigenvektoren - Sonderf\u00e4lle (Aufabe 2)</p> <p></p>"},{"location":"content/PCA/5_pca/","title":"PCA","text":"<p>Wir haben nun alle theoretsichen Bausteine f\u00fcr PCA zusammengestellt.</p> <p>Zusammenfassung der Herleitung von PCA</p> <p>Gegeben sei ein Datensatz \\(X \\in \\mathbb{R}^{N\\times D}\\) mit \\(D\\) Merkmalen und \\(N\\) Auspr\u00e4gungen.</p> <p>\\(X\\) soll auf \\(D\\) neue Merkmale \\(pc_1, \\cdots , pc_D\\) \u00fcbertragen werden, wobei diese neuen Merkmale nach ihrer Varianz geordnet sind sind, d.h. \\(var(pc_1) \\geq var(pc_2) \\geq \\cdots \\geq var(pc_D)\\).</p> <p>Dazu werden die Daten zuerste in die Mitte des Koordinatensystems mit der Rechnung \\(Y := X - \\bar{X}\\) verschoben.</p> <p>Dann wird die Kovarianzmatrix \\(S\\) von \\(Y\\) berechnet.</p> <p>Berechne dann die Eigenwerte von \\(S\\) und sortiere diese der Gr\u00f6\u00dfe nach, sodass \\(\\lambda_1 \\geq \\cdots \\geq \\lambda_d\\) ist (\\(d \\leq D\\)).</p> <p>Bereche zu jedem Eigenwert die normierten Eigenvektoren \\((v_1, \\cdots, v_d)\\).</p> <p>Da \\(S\\) symetrisch ist, stehen die Eigenvektoren senkrecht aufeinander, sodass man \\(Y\\) einfach auf diese projizieren kann und so die neuen Merkmale erh\u00e4lt.</p> Vollst\u00e4ndiges PCA ausrechnen <p>Gegeben ist eine Matrix \\( X \\) mit drei Merkmalen und f\u00fcnf Beobachtungen:</p> \\[ X = \\begin{pmatrix} 2 &amp; 7 &amp; 2 \\\\ 4 &amp; 8 &amp; 6 \\\\ 2 &amp; 6 &amp; 1 \\\\ 7 &amp; 6 &amp; 3 \\\\ 1 &amp; 5 &amp; 9 \\\\ 1 &amp; 4 &amp; 6 \\\\ 2 &amp; 6 &amp; 8 \\\\ 1 &amp; 8 &amp; 3 \\\\ 4 &amp; 6 &amp; 7 \\\\ 6 &amp; 4 &amp; 5 \\end{pmatrix} \\] <p>Dabei entspricht jede Spalte einem Merkmal und jede Zeile einer Beobachtung.</p> <p>F\u00fchren Sie mit diesem Datensatz PCA komplett durch.</p> PCA implementieren <p>Implementieren Sie eine Klasse <code>CustomPCA</code>, die von <code>BaseEstimator</code> und <code>TransformerMixin</code> ableitet. (Erkl\u00e4re warum!)</p> <p>Diese Klasse muss die Methoden <code>fit(self, X, y=None)</code> <code>transform(self, X)</code>, <code>fit_transform(self, X, y=None)</code> implementieren (die <code>y</code> Parameter werden nicht ben\u00f6tigt, m\u00fcssen aber als Parameter vorhanden sein.</p> <p>Weiterhin soll <code>init(self, n_components)</code> in der Klasse vorliegen, in der \u00fcber <code>n_components</code> festgelegt wird, wie viele Dimensionen die Ausgabe haben soll.</p> <p>\ud83d\ude80Bonus: Erstelle eine Beispiel, bei der du einen echten Datensatz einl\u00e4dst und mit <code>CustomPCA</code> transformierst. Baue Sie dabei gerne in eine Pipeline ein.</p> Tipp <p>Legen Sie ein Feld <code>components_</code> an, in dem die Eigenvektoren gespeichert werden, sodass die  <code>transform</code> Funktion wie folgt implementiert werden werden kann:</p> <pre><code>def transform(self, X):\n    return np.dot(X - np.mean(X, axis=0), self.components_.T)\n</code></pre> L\u00f6sung <pre><code>## Daniel ####\n\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport unittest\n\nX = np.matrix([[2,4,2,7,1,1,2,1,4,6],[7,8,6,6,5,4,6,8,6,4], [2,6,1,3,9,6,8,3,7,5]]).T\nprint(\"Original Data Matrix:\\n\", X)\n\n# BaseEstimator stellet grundlegende Funktionalit\u00e4ten wie Setzen und \u00dcberpr\u00fcfen von Parametern bereit,\n# w\u00e4hrend transformer Mixin die Methoden fit und tansform bereitstellt. So kann eigene Klasse mit weiteren sklearn Funktionen zusammenarbeiten.\nclass CustomPCA(BaseEstimator, TransformerMixin):\n    def __init__(self, n_components):\n        super().__init__()\n        self.components_ = None\n        self.n_components = n_components\n\n    def fit(self, X, y=None):\n        # Daten in die Mitte des Koordinatensystems verschieben\n        Y = X - np.mean(X, axis=0)\n        # Kovarianzmatrix berechnen\n        covariance_matrix = np.cov(Y, rowvar=False)\n        # Eigenvwerte und Eigenvektoren ermitteln\n        eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix)\n        # Eigenwerte in absteigender Reihenfolge sortieren\n        sorted_indices = np.argsort(eigenvalues)[::-1]\n        self.components_ = eigenvectors[:, sorted_indices[:self.n_components]]\n        return self\n\n    def transform(self, X):\n        return np.dot(X - np.mean(X, axis=0), self.components_)\n\n\npca = CustomPCA(n_components=2)\n\ntransformed_X = pca.fit_transform(X)\nprint(\"Transformierte Matrix:\\n\", transformed_X)\n\n# Unit test for CustomPCA class\nclass TestCustomPCA(unittest.TestCase):\n    def setUp(self):\n        self.X = np.matrix([[2,4,2,7,1,1,2,1,4,6],[7,8,6,6,5,4,6,8,6,4], [2,6,1,3,9,6,8,3,7,5]]).T\n        self.pca = CustomPCA(n_components=2)\n\n    def test_fit_transform_shape(self):\n        transformed_X = self.pca.fit_transform(self.X)\n        self.assertEqual(transformed_X.shape[1], 2)\n\n    def test_fit_components_shape(self):\n        self.pca.fit(self.X)\n        self.assertEqual(self.pca.components_.shape[1], 2)\n\n    def test_fit_transform_values(self):\n        transformed_X = self.pca.fit_transform(self.X)\n        expected_transformed_X_shape = (self.X.shape[0], 2)\n        self.assertEqual(transformed_X.shape[0], expected_transformed_X_shape[0])\n        self.assertEqual(transformed_X.shape[1], expected_transformed_X_shape[1])\n\n    def test_fit_eigenvalues_order(self):\n        self.pca.fit(self.X)\n        covariance_matrix = np.cov(self.X - np.mean(self.X, axis=0), rowvar=False)\n        eigenvalues = np.linalg.eigh(covariance_matrix)[0]\n        sorted_eigenvalues = np.sort(eigenvalues)[::-1]\n        computed_eigenvalues = np.diag(np.dot(self.pca.components_.T,\n                                              np.dot(covariance_matrix,\n                                                     self.pca.components_)))\n        for i in range(2):\n            self.assertAlmostEqual(computed_eigenvalues[i], sorted_eigenvalues[i])\n\n\n # ------------------ Fabian\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\nclass CustomPCA(BaseEstimator, TransformerMixin):\n    def __init__(self, n_components) -&gt; None:\n        super().__init__()\n        self.n_components = n_components\n\n    def fit(self, X, y=None):\n        cov_mat = cov_matrix(X)\n        eigenwerte, eigenvektoren = np.linalg.eig(cov_mat)\n\n    sorted_indices = np.argsort(eigenwerte)[::-1]\n    eigenvektoren = eigenvektoren[:, sorted_indices]\n\n        self.components_ = eigenvektoren[:, :self.n_components]\n        return self\n\n\n    def transform(self, X):\n        return np.dot(X, self.components_)\n\n\n\ndef normalise(data):\n    mean = np.mean(data, axis=0)\n    return data - mean\n\ndef covariance(a, b):\n    if (l := len(a)) != len(b):\n        raise ValueError('Merkmale sind unterschiedlich lang!')\n    mean_a = np.mean(a)\n    mean_b = np.mean(b)\n    return sum([(a - mean_a) * (b - mean_b) for a, b in zip(a,b)])  / l\n\n\ndef cov_matrix(data_frame: pd.DataFrame):\n    cols = data_frame.columns\n    numeric_cols = [col for col in cols if df[col].dtype == 'int64'] # und alle anderen dtype die numerisch sind\n    matrix = np.array([])\n    for col1 in numeric_cols:\n        for col2 in numeric_cols:\n            matrix = np.append(matrix, (covariance(df[col1], df[col2])))\n    return matrix.reshape(len(numeric_cols), len(numeric_cols))\n\n\ndf = pd.DataFrame({'x1': [2, 4, 2, 7, 1, 1, 2, 1, 4, 6],\n                   'x2': [7, 8, 6, 6, 5, 4, 6, 8, 6, 4],\n                   'x3': [2, 6, 1, 3, 9, 6, 8, 3, 7, 5]})\n\nX_normalised = normalise(df)\n\npca = CustomPCA(n_components=2)\n\npca.fit_transform(X_normalised)\n\n\n# Tests gibts nicht\n\n-------------------- Noah\n\nimport numpy as np\n\n# Gegebene Matrix x\nx = np.array([[1, 1, 0],\n              [2, 3, 1],\n              [4, 4, -1],\n              [1, 4, 0]])\n\nx_mean = np.mean(x, axis=0)\n\nx_centered = x-x_mean\n\ncov_matrix = (x_centered.T @ x_centered) / len(x_centered)\n\neigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\nsorted_indicies = np.argsort(eigenvalues)[::-1]\neigenvalues = eigenvalues[sorted_indicies]\neigenvectors = eigenvectors[:, sorted_indicies]\n\npca_matrix = x_centered @ eigenvectors\nprint(pca_matrix)\n</code></pre> sklearn's Implementierung untersuchen <p>Untersuche die Implementierung von PCA von sklearn. Wie ist diese aufgebaut? Wie werden die Eigenvektoren bestimmt?</p> Tipp <p>Es gibt einen Button <code>[source]</code> auf der verlinkten Seite, mit der du zum Repository von sklearn gelangst.</p>"}]}